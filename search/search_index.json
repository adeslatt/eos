{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Kids First & INCLUDE Elements of Style in Workflow Creation Gabriella Miller Kids First Pediatric Research Program ( Kids First ) and the INvestigation of Co-occuring Conditions Across the Lifespan to Understand Down syndromE Project ( INCLUDE ) are joining forces to offer a 5-day training program designed specifically for researchers. Learn how to use cloud platforms and workflows to ask scientific questions with ease and confidence. Our expert instructors will guide you through the processes that assure reproducibility, repurposability regardless of the computational environment. Achieve greater success with a modular, testable approach to work. In 2018, the NIH launched INCLUDE Project to improve the health and quality of life for people with Down syndrome. The INCLUDE Data Coordinating Center (DCC) was created to give Down Syndrome researchers and the community access to data and analysis tools including a free-to-use resource called the INCLUDE Data Hub . The Data Hub brings information and resources from many sources to gether in one place and allows scientists to study conditions affecting individuals with Down syndrome fastr than ever before. Kids First is a trans-NIH Common Fund program whose goal is to help researchers uncover new insights into the biology of childhood cancer and structural birth defects, including the discovery of shared genetic pathways between these disorders. To achieve this goal, the program has developed the Kids First Data Resource Center, a cloud-based platform which publicly shares genetic and clinical data from 30,000 childhood cancer and structural birth defect patients and families. Through the Kids First Data Resource Portal researchers have access to analysis tools and collaborative research. Both of these platforms are gateways that provide democratizing access to the vast and flexible world of cloud computing, powered by CAVATICA and Velsera (formerly Seven Bridges) Before you attend the course, you need to set up GitHub , Zenodo and ORCID accounts and have basic knowledge of the terminal window, shell environment and the commands for navigation within that environment Course Overview:","title":"Home"},{"location":"#kids-first-include-elements-of-style-in-workflow-creation","text":"Gabriella Miller Kids First Pediatric Research Program ( Kids First ) and the INvestigation of Co-occuring Conditions Across the Lifespan to Understand Down syndromE Project ( INCLUDE ) are joining forces to offer a 5-day training program designed specifically for researchers. Learn how to use cloud platforms and workflows to ask scientific questions with ease and confidence. Our expert instructors will guide you through the processes that assure reproducibility, repurposability regardless of the computational environment. Achieve greater success with a modular, testable approach to work. In 2018, the NIH launched INCLUDE Project to improve the health and quality of life for people with Down syndrome. The INCLUDE Data Coordinating Center (DCC) was created to give Down Syndrome researchers and the community access to data and analysis tools including a free-to-use resource called the INCLUDE Data Hub . The Data Hub brings information and resources from many sources to gether in one place and allows scientists to study conditions affecting individuals with Down syndrome fastr than ever before. Kids First is a trans-NIH Common Fund program whose goal is to help researchers uncover new insights into the biology of childhood cancer and structural birth defects, including the discovery of shared genetic pathways between these disorders. To achieve this goal, the program has developed the Kids First Data Resource Center, a cloud-based platform which publicly shares genetic and clinical data from 30,000 childhood cancer and structural birth defect patients and families. Through the Kids First Data Resource Portal researchers have access to analysis tools and collaborative research. Both of these platforms are gateways that provide democratizing access to the vast and flexible world of cloud computing, powered by CAVATICA and Velsera (formerly Seven Bridges) Before you attend the course, you need to set up GitHub , Zenodo and ORCID accounts and have basic knowledge of the terminal window, shell environment and the commands for navigation within that environment","title":"Kids First &amp; INCLUDE Elements of Style in Workflow Creation"},{"location":"#course-overview","text":"","title":"Course Overview:"},{"location":"about/attribution/","text":"Acknowledgements CAVATICA is a joint development between Seven Bridges and the Children's Hospital of Philadelphia Seven Bridges supports multiple workflow languages in its application development, including CWL, Nextflow and soon WDL Nextflow workflow information and guidance was gratefully received from Phil Palmer from his classes at the Jackson Laboratory while he was at Lifebit and @cgpu Christina Chatzipantsiou has been my infallible guide. @cgpu Christina Chatzipantsiou taught with me the Dry Bench Skills for the Researchers and Elements of Style class at the ISCB Academy. Common Workflow script guidance was gratefully received from Miguel Brown at Children's Hospital of Philadelphia.","title":"Attribution"},{"location":"about/attribution/#acknowledgements","text":"CAVATICA is a joint development between Seven Bridges and the Children's Hospital of Philadelphia Seven Bridges supports multiple workflow languages in its application development, including CWL, Nextflow and soon WDL Nextflow workflow information and guidance was gratefully received from Phil Palmer from his classes at the Jackson Laboratory while he was at Lifebit and @cgpu Christina Chatzipantsiou has been my infallible guide. @cgpu Christina Chatzipantsiou taught with me the Dry Bench Skills for the Researchers and Elements of Style class at the ISCB Academy. Common Workflow script guidance was gratefully received from Miguel Brown at Children's Hospital of Philadelphia.","title":"Acknowledgements"},{"location":"about/background/","text":"About Over a period of 5 days, only two hours a day, the learner learned elements of style in the construction and containerization of small single-function processes that facilitate repurposable workflow creation and execution. This hands-on-tutorial was given through a webinar using the Kids First Data Resource Center . This repository was used in the course and contains self-learnings to facilitate work. In this repository, contains how these processes may be kept up-to-date and alert the creator to the functional state of these processes (working or failing) by using a feature found within GitHub called GitHub Actions. This hands-on-course will use a small example to provide the structure, philosophy and approach to achieving this desirable outcome. This course seeks to help to demystify and make accessible powerful methods one can use to achieve platform independence and platform interoperability. Using a simple example to demonstrate these techniques, we will break down and walk the learner through each of the construction steps. The learners will be introduced to Conda, Docker, GitHub and the standard workflow language, Nextflow. If time permits, we will also show how these containerized processes can also be represented in a second standard workflow language implementation (e.g. Common Workflow Language or WDL). By the end of the course, the learner will understand these Elements of Style and will know how Conda, Docker, GitHub, Zenodo, and Nextflow enable repurposable research. Moreover, these steps will be on GitHub for the Learner to return to and reproduce themselves after the end of the course. In taking this course, the Learner will also be shown the power of JupyterLab notebooks to facilitate literate programming. Through their participation in the class, learners will learn and understand FAIR (findability, accessibility, interoperability and reusability) best practices. We ask all participants to get a GitHub, Zenodo and ORCID accounts prior to the course. We ask for minimal background knowledge of the command line, simple commands in the shell environment, we enable a bit of self-learning from the repository to facilitate the acquisition of this knowledge. ) It is an exciting time in scalable, platform agnostic methods and computational analysis where data live. I have a goal of making this work as accessible as possible to all researchers. This content was created by myself, Anne Deslattes Mays, PhD, and I am solely responsible for the correctness or errors contained within. Feel free to reach out -- my GitHub id is adeslatt . Feedback from many students over the years have gone into improving the navigation and content. I will expand the content to serve other more complext examples.","title":"Background"},{"location":"about/background/#about","text":"Over a period of 5 days, only two hours a day, the learner learned elements of style in the construction and containerization of small single-function processes that facilitate repurposable workflow creation and execution. This hands-on-tutorial was given through a webinar using the Kids First Data Resource Center . This repository was used in the course and contains self-learnings to facilitate work. In this repository, contains how these processes may be kept up-to-date and alert the creator to the functional state of these processes (working or failing) by using a feature found within GitHub called GitHub Actions. This hands-on-course will use a small example to provide the structure, philosophy and approach to achieving this desirable outcome. This course seeks to help to demystify and make accessible powerful methods one can use to achieve platform independence and platform interoperability. Using a simple example to demonstrate these techniques, we will break down and walk the learner through each of the construction steps. The learners will be introduced to Conda, Docker, GitHub and the standard workflow language, Nextflow. If time permits, we will also show how these containerized processes can also be represented in a second standard workflow language implementation (e.g. Common Workflow Language or WDL). By the end of the course, the learner will understand these Elements of Style and will know how Conda, Docker, GitHub, Zenodo, and Nextflow enable repurposable research. Moreover, these steps will be on GitHub for the Learner to return to and reproduce themselves after the end of the course. In taking this course, the Learner will also be shown the power of JupyterLab notebooks to facilitate literate programming. Through their participation in the class, learners will learn and understand FAIR (findability, accessibility, interoperability and reusability) best practices. We ask all participants to get a GitHub, Zenodo and ORCID accounts prior to the course. We ask for minimal background knowledge of the command line, simple commands in the shell environment, we enable a bit of self-learning from the repository to facilitate the acquisition of this knowledge. ) It is an exciting time in scalable, platform agnostic methods and computational analysis where data live. I have a goal of making this work as accessible as possible to all researchers. This content was created by myself, Anne Deslattes Mays, PhD, and I am solely responsible for the correctness or errors contained within. Feel free to reach out -- my GitHub id is adeslatt . Feedback from many students over the years have gone into improving the navigation and content. I will expand the content to serve other more complext examples.","title":"About"},{"location":"about/how-this-site-was-made/","text":"How this site was made This site was made and is maintained using MkDocs . It was pretty straight forward to convert the existing GitHub repository structure used for teaching the Elements of Style course. The tricky part was understanding the deployment aspect. I did this by understanding the two different functions of this repository. Location for teaching and using MarkDown for the information Website for navigation of the content. Both of the different functions require their own repositories. The first repository is the content repository where the information is maintained, added to and altered. The second repository is the GitHub pages repository where the html information is generated and deployed. Because this repository is auto-generated, it is important not to edit there, however it still needs to exist. This was a difficult concept for me to wrap my head around. Kids First & INCLUDE Elements of Style Content Kids First & INCLUDE Elements of Style WebSite","title":"Making this site"},{"location":"about/how-this-site-was-made/#how-this-site-was-made","text":"This site was made and is maintained using MkDocs . It was pretty straight forward to convert the existing GitHub repository structure used for teaching the Elements of Style course. The tricky part was understanding the deployment aspect. I did this by understanding the two different functions of this repository. Location for teaching and using MarkDown for the information Website for navigation of the content. Both of the different functions require their own repositories. The first repository is the content repository where the information is maintained, added to and altered. The second repository is the GitHub pages repository where the html information is generated and deployed. Because this repository is auto-generated, it is important not to edit there, however it still needs to exist. This was a difficult concept for me to wrap my head around.","title":"How this site was made"},{"location":"about/how-this-site-was-made/#kids-first-include-elements-of-style-content","text":"","title":"Kids First &amp; INCLUDE Elements of Style Content"},{"location":"about/how-this-site-was-made/#kids-first-include-elements-of-style-website","text":"","title":"Kids First &amp; INCLUDE Elements of Style WebSite"},{"location":"about/license/","text":"Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright [yyyy] [name of copyright owner] Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"about/other-topics-of-interest/","text":"Background Information and other Topics of Interest Anaconda Package Jupytext CAVATICA Data Studio JupyterLab Startup INCLUDE DataHub Login with ORCID Shell Google Cloud","title":"Other topics of interest"},{"location":"about/other-topics-of-interest/#background-information-and-other-topics-of-interest","text":"Anaconda Package Jupytext CAVATICA Data Studio JupyterLab Startup INCLUDE DataHub Login with ORCID Shell Google Cloud","title":"Background Information and other Topics of Interest"},{"location":"classes/day-1-reasoning/A-Few-Simple-Rules-Shortened/","text":"A Few Simple Rules - Shortened Edition Disclaimer The views expressed in this course represent the views of Anne Deslattes Mays, PhD and do not represent the views of NICHD, NIH or the United States Government. In what follows - is my development of a practice that enables workflow and platform independence facilitating repurposability. Learning from those who have walked the journey Elements of Programming Style B. W. Kernighan and P. J. Plauger, The Elements of Programming Style 2nd Edition, McGraw Hill, New York, 1978. ISBN 0-07-034207-5 Quotes by P. J. Plauger The year was 1919, the first World War was at its close and a student, E. B. White, took a course, English 8, taught by William Strunk Jr. The course featured a required textbook, a slim volume called The Elements of Style . The durability of this slim book informed the development of the book, The Elements of Programming Style by Kernigan and Plauger, whose lessons we adapt here in this course. Showing again the durability of the approach of beginning with philosophy as one approaches their work and use of programs and structure to achieve their work. So with this nod to E.B. White, William Strunk, Jr, Brian Kernigan and Plauger, we begin with our own Lessons and Pithy Phrases. What is a Pithy Phrase: A pithy phrase or statement is brief but full of substance and meaning Lessons Translated to the Workflow/Containerized Process (Truncated Pithy Phrases) Top 10 Lessons Number Lesson Relates Too Full Lessons Number 1. Write clearly \u2013 don't be too clever. Everything 1. 2. Write first in easy-to-understand pseudo language Everything 13. 3. Modularize. Use procedures and functions. Containerizing with Dockerfile 14. 4. Write and test in small pieces. Use Dockerfiles, GitHub Actions to test 17. 5. Test inputs and outputs Test on command line workflows are input, process, output 19. 6. Use debugging compilers. Compiling with workflows 29. 7. Make it right before you make it faster. Everything 30. 8. Make sure special cases are truly special. Everythng 35. 9. Keep it simple to make it faster. Everything 36. 10. Document your process in GitHub use workflow languages, containers and GitHub 45. A Few Simple Rules - Full List A Few Simple Rules Return to today's Agenda","title":"A Few Simple Rules Shortened"},{"location":"classes/day-1-reasoning/A-Few-Simple-Rules-Shortened/#a-few-simple-rules-shortened-edition","text":"","title":"A Few Simple Rules - Shortened Edition"},{"location":"classes/day-1-reasoning/A-Few-Simple-Rules-Shortened/#disclaimer","text":"The views expressed in this course represent the views of Anne Deslattes Mays, PhD and do not represent the views of NICHD, NIH or the United States Government. In what follows - is my development of a practice that enables workflow and platform independence facilitating repurposability.","title":"Disclaimer"},{"location":"classes/day-1-reasoning/A-Few-Simple-Rules-Shortened/#learning-from-those-who-have-walked-the-journey","text":"Elements of Programming Style B. W. Kernighan and P. J. Plauger, The Elements of Programming Style 2nd Edition, McGraw Hill, New York, 1978. ISBN 0-07-034207-5 Quotes by P. J. Plauger The year was 1919, the first World War was at its close and a student, E. B. White, took a course, English 8, taught by William Strunk Jr. The course featured a required textbook, a slim volume called The Elements of Style . The durability of this slim book informed the development of the book, The Elements of Programming Style by Kernigan and Plauger, whose lessons we adapt here in this course. Showing again the durability of the approach of beginning with philosophy as one approaches their work and use of programs and structure to achieve their work. So with this nod to E.B. White, William Strunk, Jr, Brian Kernigan and Plauger, we begin with our own Lessons and Pithy Phrases. What is a Pithy Phrase: A pithy phrase or statement is brief but full of substance and meaning","title":"Learning from those who have walked the journey"},{"location":"classes/day-1-reasoning/A-Few-Simple-Rules-Shortened/#lessons-translated-to-the-workflowcontainerized-process-truncated-pithy-phrases","text":"Top 10 Lessons Number Lesson Relates Too Full Lessons Number 1. Write clearly \u2013 don't be too clever. Everything 1. 2. Write first in easy-to-understand pseudo language Everything 13. 3. Modularize. Use procedures and functions. Containerizing with Dockerfile 14. 4. Write and test in small pieces. Use Dockerfiles, GitHub Actions to test 17. 5. Test inputs and outputs Test on command line workflows are input, process, output 19. 6. Use debugging compilers. Compiling with workflows 29. 7. Make it right before you make it faster. Everything 30. 8. Make sure special cases are truly special. Everythng 35. 9. Keep it simple to make it faster. Everything 36. 10. Document your process in GitHub use workflow languages, containers and GitHub 45.","title":"Lessons Translated to the Workflow/Containerized Process (Truncated Pithy Phrases)"},{"location":"classes/day-1-reasoning/A-Few-Simple-Rules-Shortened/#a-few-simple-rules-full-list","text":"A Few Simple Rules","title":"A Few Simple Rules - Full List"},{"location":"classes/day-1-reasoning/A-Few-Simple-Rules-Shortened/#return-to-todays-agenda","text":"","title":"Return to today's Agenda"},{"location":"classes/day-1-reasoning/A-Few-Simple-Rules/","text":"A Few Simple Rules Disclaimer The views expressed in this course represent the views of Anne Deslattes Mays, PhD and do not represent the views of NICHD, NIH or the United States Government. In what follows - is my development of a practice that enables workflow and platform independence facilitating reproducibility. Learning from those who have walked the journey Elements of Programming Style B. W. Kernighan and P. J. Plauger, The Elements of Programming Style 2nd Edition, McGraw Hill, New York, 1978. ISBN 0-07-034207-5 Quotes by P. J. Plauger The year was 1919, the first World War was at its close and a student, E. B. White, took a course, English 8, taught by William Strunk Jr. The course featured a required textbook, a slim volume called The Elements of Style . The durability of this slim book informed the development of the book, The Elements of Programming Style by Kernigan and Plauger, whose lessons we adapt here in this course. Showing again the durability of the approach of beginning with philosophy as one approaches their work and use of programs and structure to achieve their work. So with this nod to E.B. White, William Strunk, Jr, Brian Kernigan and Plauger, we begin with our own Lessons and Pithy Phrases. Lessons Translated to the Workflow/Containerized Process (Truncated Pithy Phrases) Where I have made modifications to these Pithy Phrases to the map to what we are teaching here will be in italized and emphasized Its lessons are summarized at the end of each section in pithy maxims, such as \"Let the machine do the dirty work\": Write clearly \u2013 don't be too clever. timeless Say what you mean, simply and directly. timeless Use containerized processes (in a way similar to library functions) whenever feasible. Avoid too many temporary variables. Write clearly \u2013 don't sacrifice clarity for efficiency. timeless Let the machine do the dirty work. timeless Replace repetitive expressions by calls to common functions. timeless - when you start to see yourself do this - replace with a single function Parenthesize to avoid ambiguity. 9 Choose variable names that won't be confused. timeless Avoid unnecessary branches. If a logical expression is hard to understand, try transforming it. timeless Choose a data representation that makes the program simple. Write first in easy-to-understand pseudo language; then translate into whatever language you have to use. timeless Modularize. Use procedures and functions. and containerize - use GitHub Actions to build, test and keep up-to-date Avoid gotos completely if you can keep the program readable. Don't patch bad code \u2013 rewrite it. Write and test a big program in small pieces. timeless this can be done by the use of these tested, dockerized processes Use recursive procedures for recursively-defined data structures. Test input for plausibility and validity. timeless - make sure you understand the source of your data Make sure input doesn't violate the limits of the program. Terminate input by end-of-file marker, not by count. Identify bad input; recover if possible. Make input easy to prepare and output self-explanatory. Use uniform input formats. Make input easy to proofread. timeless Use self-identifying input. Allow defaults. Echo both on output. Make sure all variables are initialized before use. Don't stop at one bug. Use debugging compilers. timeless this is different with workflow languages - you can test each of the steps in the workflow verifying inputs, outputs and processes - dockerizing, testing Watch out for off-by-one errors. timeless Take care to branch the right way on equality. Be careful if a loop exits to the same place from the middle and the bottom. Make sure your code does \"nothing\" gracefully. Test programs at their boundary values. Check some answers by hand. timeless 10.0 times 0.1 is hardly ever 1.0. timeless always aim for simplicity 7/8 is zero while 7.0/8.0 is not zero. timeless but this would be better for an R or a Python class Don't compare floating point numbers solely for equality. timeless but this would be better for an R or a Python class Make it right before you make it faster. timeless for everything Make it fail-safe before you make it faster. timeless Make it clear before you make it faster. timeless Don't sacrifice clarity for small gains in efficiency. timeless Let your compiler do the simple optimizations. `again, for our world of platforms, let your platform help you - Platforms as a Service, such as CAVATICA by Seven Bridges and CloudOS by Lifebit Don't strain to re-use code; reorganize instead. timeless the more you perform a task, the simpler you see how to get it done, exploit that simplicity and rewrite Make sure special cases are truly special. timeless Keep it simple to make it faster. timeless Don't diddle code to make it faster \u2013 find a better algorithm. timeless find another Bioinformatics algorithm, collaborate, give attribution and expand your reach Instrument your programs. Measure before making efficiency changes. timeless - this means if you introduce changes - are they appropriate Make sure comments and code agree. timeless Don't just echo the code with comments \u2013 make every comment count. timeless Don't comment bad code \u2013 rewrite it. timeless Use variable names that mean something. Use statement labels that mean something. Format a program to help the reader understand it. timeless Document your data layouts. timeless Don't over-comment. timeless Other useful and informative points","title":"A Few Simple Rules"},{"location":"classes/day-1-reasoning/A-Few-Simple-Rules/#a-few-simple-rules","text":"","title":"A Few Simple Rules"},{"location":"classes/day-1-reasoning/A-Few-Simple-Rules/#disclaimer","text":"The views expressed in this course represent the views of Anne Deslattes Mays, PhD and do not represent the views of NICHD, NIH or the United States Government. In what follows - is my development of a practice that enables workflow and platform independence facilitating reproducibility.","title":"Disclaimer"},{"location":"classes/day-1-reasoning/A-Few-Simple-Rules/#learning-from-those-who-have-walked-the-journey","text":"Elements of Programming Style B. W. Kernighan and P. J. Plauger, The Elements of Programming Style 2nd Edition, McGraw Hill, New York, 1978. ISBN 0-07-034207-5 Quotes by P. J. Plauger The year was 1919, the first World War was at its close and a student, E. B. White, took a course, English 8, taught by William Strunk Jr. The course featured a required textbook, a slim volume called The Elements of Style . The durability of this slim book informed the development of the book, The Elements of Programming Style by Kernigan and Plauger, whose lessons we adapt here in this course. Showing again the durability of the approach of beginning with philosophy as one approaches their work and use of programs and structure to achieve their work. So with this nod to E.B. White, William Strunk, Jr, Brian Kernigan and Plauger, we begin with our own Lessons and Pithy Phrases.","title":"Learning from those who have walked the journey"},{"location":"classes/day-1-reasoning/A-Few-Simple-Rules/#lessons-translated-to-the-workflowcontainerized-process-truncated-pithy-phrases","text":"Where I have made modifications to these Pithy Phrases to the map to what we are teaching here will be in italized and emphasized Its lessons are summarized at the end of each section in pithy maxims, such as \"Let the machine do the dirty work\": Write clearly \u2013 don't be too clever. timeless Say what you mean, simply and directly. timeless Use containerized processes (in a way similar to library functions) whenever feasible. Avoid too many temporary variables. Write clearly \u2013 don't sacrifice clarity for efficiency. timeless Let the machine do the dirty work. timeless Replace repetitive expressions by calls to common functions. timeless - when you start to see yourself do this - replace with a single function Parenthesize to avoid ambiguity. 9 Choose variable names that won't be confused. timeless Avoid unnecessary branches. If a logical expression is hard to understand, try transforming it. timeless Choose a data representation that makes the program simple. Write first in easy-to-understand pseudo language; then translate into whatever language you have to use. timeless Modularize. Use procedures and functions. and containerize - use GitHub Actions to build, test and keep up-to-date Avoid gotos completely if you can keep the program readable. Don't patch bad code \u2013 rewrite it. Write and test a big program in small pieces. timeless this can be done by the use of these tested, dockerized processes Use recursive procedures for recursively-defined data structures. Test input for plausibility and validity. timeless - make sure you understand the source of your data Make sure input doesn't violate the limits of the program. Terminate input by end-of-file marker, not by count. Identify bad input; recover if possible. Make input easy to prepare and output self-explanatory. Use uniform input formats. Make input easy to proofread. timeless Use self-identifying input. Allow defaults. Echo both on output. Make sure all variables are initialized before use. Don't stop at one bug. Use debugging compilers. timeless this is different with workflow languages - you can test each of the steps in the workflow verifying inputs, outputs and processes - dockerizing, testing Watch out for off-by-one errors. timeless Take care to branch the right way on equality. Be careful if a loop exits to the same place from the middle and the bottom. Make sure your code does \"nothing\" gracefully. Test programs at their boundary values. Check some answers by hand. timeless 10.0 times 0.1 is hardly ever 1.0. timeless always aim for simplicity 7/8 is zero while 7.0/8.0 is not zero. timeless but this would be better for an R or a Python class Don't compare floating point numbers solely for equality. timeless but this would be better for an R or a Python class Make it right before you make it faster. timeless for everything Make it fail-safe before you make it faster. timeless Make it clear before you make it faster. timeless Don't sacrifice clarity for small gains in efficiency. timeless Let your compiler do the simple optimizations. `again, for our world of platforms, let your platform help you - Platforms as a Service, such as CAVATICA by Seven Bridges and CloudOS by Lifebit Don't strain to re-use code; reorganize instead. timeless the more you perform a task, the simpler you see how to get it done, exploit that simplicity and rewrite Make sure special cases are truly special. timeless Keep it simple to make it faster. timeless Don't diddle code to make it faster \u2013 find a better algorithm. timeless find another Bioinformatics algorithm, collaborate, give attribution and expand your reach Instrument your programs. Measure before making efficiency changes. timeless - this means if you introduce changes - are they appropriate Make sure comments and code agree. timeless Don't just echo the code with comments \u2013 make every comment count. timeless Don't comment bad code \u2013 rewrite it. timeless Use variable names that mean something. Use statement labels that mean something. Format a program to help the reader understand it. timeless Document your data layouts. timeless Don't over-comment. timeless","title":"Lessons Translated to the Workflow/Containerized Process (Truncated Pithy Phrases)"},{"location":"classes/day-1-reasoning/A-Few-Simple-Rules/#other-useful-and-informative-points","text":"","title":"Other useful and informative points"},{"location":"classes/day-1-reasoning/creating-a-cavatica-account/","text":"Creating a CAVATICA Account Please create your account on CAVATICA Navigate to CAVATICA First screen you will see: You can either select Create an account or Log in with eRA Commons . If you select Create an account , you will see: Or if you select Log in with eRACommons you will see: See further documentation: CAVATICA Account Login Creation Documentation","title":"Creating a CAVATICA Account"},{"location":"classes/day-1-reasoning/creating-a-cavatica-account/#creating-a-cavatica-account","text":"Please create your account on CAVATICA Navigate to CAVATICA First screen you will see: You can either select Create an account or Log in with eRA Commons . If you select Create an account , you will see: Or if you select Log in with eRACommons you will see: See further documentation: CAVATICA Account Login Creation Documentation","title":"Creating a CAVATICA Account"},{"location":"classes/day-1-reasoning/day-1-reasoning/","text":"Agenda for the Day 1: Reasoning Time (UTC) Programme 11.00 - 11:30 Welcome Address and Presentation 11.30 - 11.45 A few simple rules for easier workflow maintenance and reuse 11:45 - 12:00 Lets Dive In, Create an Account, Start a JupyterLab Notebook 12.00 - 12.10 Short Break 12.10 - 12.25 Introduction to the Command-line 12.25 - 12.50 Example Volcano Plot on CAVATICA 12.50 - 13:00 Wrap up and overview of next days topics Additional Resources: NIH Kids First Cloud Credit Program Overview NIH INCLUDE Cloud Credit Program Overview How to apply for Kids First Cloud Credits How to apply for INCLUDE Cloud Credits Joint Kids First and INCLUDE with Velsera (formerly Seven Bridges) Monthly User Support Office Hours - every third Thursday of the month JupyterLab Notebook Conversions The Jupyter Text - do you like working in R Studio with R-markdown? You can convert easily JupyterLab notebooks with R kernels to R Script or R markdown. Explore the concept here: - Jupytext Supported Languages - Jupytext Command Line","title":"Day 1 - Reasoning"},{"location":"classes/day-1-reasoning/day-1-reasoning/#agenda-for-the-day-1-reasoning","text":"Time (UTC) Programme 11.00 - 11:30 Welcome Address and Presentation 11.30 - 11.45 A few simple rules for easier workflow maintenance and reuse 11:45 - 12:00 Lets Dive In, Create an Account, Start a JupyterLab Notebook 12.00 - 12.10 Short Break 12.10 - 12.25 Introduction to the Command-line 12.25 - 12.50 Example Volcano Plot on CAVATICA 12.50 - 13:00 Wrap up and overview of next days topics","title":"Agenda for the Day 1: Reasoning"},{"location":"classes/day-1-reasoning/day-1-reasoning/#additional-resources","text":"NIH Kids First Cloud Credit Program Overview NIH INCLUDE Cloud Credit Program Overview How to apply for Kids First Cloud Credits How to apply for INCLUDE Cloud Credits Joint Kids First and INCLUDE with Velsera (formerly Seven Bridges) Monthly User Support Office Hours - every third Thursday of the month","title":"Additional Resources:"},{"location":"classes/day-1-reasoning/day-1-reasoning/#jupyterlab-notebook-conversions","text":"The Jupyter Text - do you like working in R Studio with R-markdown? You can convert easily JupyterLab notebooks with R kernels to R Script or R markdown. Explore the concept here: - Jupytext Supported Languages - Jupytext Command Line","title":"JupyterLab Notebook Conversions"},{"location":"classes/day-1-reasoning/lets-dive-in/","text":"Let's Dive In So we can be ready with a running notebook, lets get a machine up and running - once up - it will stay up for 30 minutes -- challenging us to keep to our agenda! Lets see how we do! FAST Track Create an Account with CAVATICA If you have not already done so, please go ahead and create an account on CAVATICA . Any difficulties reach out to David in the Zoom Chat. We have an hour after each class this week, where we will stay to assist. Log in straight to CAVATICA Step-by-Step Instructions to log into CAVATICA Step-by-Step Instructions to create a project and start a JupyterLab Notebook While things start to cook -- let me review the Agenda and show a brief presentation If not yet registered If you have not already registered with Kids First, INCLUDE and CAVATICA, please stay after today and we will get you caught up. Registering with Kids First, INCLUDE and other accounts Return to Day 1 Return to today's agenda","title":"Lets dive in"},{"location":"classes/day-1-reasoning/lets-dive-in/#lets-dive-in","text":"So we can be ready with a running notebook, lets get a machine up and running - once up - it will stay up for 30 minutes -- challenging us to keep to our agenda! Lets see how we do!","title":"Let's Dive In"},{"location":"classes/day-1-reasoning/lets-dive-in/#fast-track-create-an-account-with-cavatica","text":"If you have not already done so, please go ahead and create an account on CAVATICA . Any difficulties reach out to David in the Zoom Chat. We have an hour after each class this week, where we will stay to assist.","title":"FAST Track Create an Account with CAVATICA"},{"location":"classes/day-1-reasoning/lets-dive-in/#log-in-straight-to-cavatica","text":"Step-by-Step Instructions to log into CAVATICA Step-by-Step Instructions to create a project and start a JupyterLab Notebook While things start to cook -- let me review the Agenda and show a brief presentation","title":"Log in straight to CAVATICA"},{"location":"classes/day-1-reasoning/lets-dive-in/#if-not-yet-registered","text":"If you have not already registered with Kids First, INCLUDE and CAVATICA, please stay after today and we will get you caught up. Registering with Kids First, INCLUDE and other accounts","title":"If not yet registered"},{"location":"classes/day-1-reasoning/lets-dive-in/#return-to-day-1","text":"Return to today's agenda","title":"Return to Day 1"},{"location":"classes/day-1-reasoning/logging-into-cavatica-step-by-step/","text":"Logging into CAVATICA Tutorial Login to CAVATICA Step-by-Step Step 1 - Navigate to Cavatica Navigate in your BROWSER (Chrome prefered) https://cavatica.sbgenomics.com/home The CAVATICA Initial Login Window looks like this: Logging in with your eRACommons ID will allow CAVATICA to provide you access to data for which you are authorized. You can Login with the eRA Commons after you have created an account on Cavatica. The account will automatically linked to the eRA Commons, making it possible to Log in with the eRA Commons. See the CAVATICA platform documentation for details You can read about the connection between dbGaP and eRACommons Step 2 - Authorize CAVATICA The next window you will see is the Gen3 login window - where you authorize NIH and RAS to retrieve the controlled-access datasets to which you have access. Click Yes, I authorize The next window can vary depending upon if this is your first time logging in or not. An example of your next window is the Dashboard view Next Step - Create a Project and Start a JupyterLab Notebook Create a Project and Start a JupyterLab Notebook","title":"Logging into CAVATICA Tutorial"},{"location":"classes/day-1-reasoning/logging-into-cavatica-step-by-step/#logging-into-cavatica-tutorial","text":"","title":"Logging into CAVATICA Tutorial"},{"location":"classes/day-1-reasoning/logging-into-cavatica-step-by-step/#login-to-cavatica-step-by-step","text":"","title":"Login to CAVATICA Step-by-Step"},{"location":"classes/day-1-reasoning/logging-into-cavatica-step-by-step/#step-1-navigate-to-cavatica","text":"Navigate in your BROWSER (Chrome prefered) https://cavatica.sbgenomics.com/home The CAVATICA Initial Login Window looks like this: Logging in with your eRACommons ID will allow CAVATICA to provide you access to data for which you are authorized. You can Login with the eRA Commons after you have created an account on Cavatica. The account will automatically linked to the eRA Commons, making it possible to Log in with the eRA Commons. See the CAVATICA platform documentation for details You can read about the connection between dbGaP and eRACommons","title":"Step 1 - Navigate to Cavatica"},{"location":"classes/day-1-reasoning/logging-into-cavatica-step-by-step/#step-2-authorize-cavatica","text":"The next window you will see is the Gen3 login window - where you authorize NIH and RAS to retrieve the controlled-access datasets to which you have access. Click Yes, I authorize The next window can vary depending upon if this is your first time logging in or not. An example of your next window is the Dashboard view","title":"Step 2 - Authorize CAVATICA"},{"location":"classes/day-1-reasoning/logging-into-cavatica-step-by-step/#next-step-create-a-project-and-start-a-jupyterlab-notebook","text":"Create a Project and Start a JupyterLab Notebook","title":"Next Step - Create a Project and Start a JupyterLab Notebook"},{"location":"classes/day-1-reasoning/running-a-jupyterlab-notebook/","text":"To begin, your screen should look like this: This is a JupyterLab environment. Project Jupyter is a non-profit, open-source project, born out of the IPython Project in 2014 as it evolved to support interactive data science and scientific computing across all programming languages. Jupyter will always be 100% open-source software, free for all to use and released under the liberal terms of the modified BSD license. I encourage you to explore Project Jupyter and understand the directions they are going in and how they support open, transparent, re-purposeable and literate programming. The JupyterLab Notebook environment supports at this point 40 languages, in kernels, providing a flexible way to achieve not only the ends of what we are trying to do with analysis, but in a well documented, clear and literate manner. Now that we are at the terminal window, we need to bring our lesson in hand -- and to do that, we are going to begin with forking our GitHub repository. Clone a GitHub Repository We are going to clone a GitHub repository - to do so navigate in your browser to Example Volcano Plot Notebook Copy the Link to Clone the repository Next, navigate to the \ud83d\udfe6 Code button on the right and select HTTPS and copy the link. Clone the repository in the Jupyterlab terminal window Return to your CAVATICA Window and go back to your JupyterLab notebook. And select the terminal window Now we want to clone the repository fork we made: At the prompt type: git clone https://github.com/NIH-NICHD/exampleVolcanoPlotNotebook.git Now let's use our cd command to change into the appropriate directory. cd exampleVolcanoPlotNotebook Recap In this lesson: * We logged into CAVATICA * We started an interactive analysis with JupyterLab * We authenticated with GitHub * We executed and saw some Command Line Functions * We generated a Volcano Plot in a JupyterLab notebook running an R kernel Additional resources: CAVATICA documentation for the JupyterLab interface: https://docs.cavatica.org/docs/editor-quick-reference Official documentation from project JupyterLab: https://jupyterlab.readthedocs.io/en/stable/","title":"Running a jupyterlab notebook"},{"location":"classes/day-1-reasoning/running-a-jupyterlab-notebook/#clone-a-github-repository","text":"We are going to clone a GitHub repository - to do so navigate in your browser to Example Volcano Plot Notebook","title":"Clone a GitHub Repository"},{"location":"classes/day-1-reasoning/running-a-jupyterlab-notebook/#copy-the-link-to-clone-the-repository","text":"Next, navigate to the \ud83d\udfe6 Code button on the right and select HTTPS and copy the link.","title":"Copy the Link to Clone the repository"},{"location":"classes/day-1-reasoning/running-a-jupyterlab-notebook/#clone-the-repository-in-the-jupyterlab-terminal-window","text":"Return to your CAVATICA Window and go back to your JupyterLab notebook. And select the terminal window Now we want to clone the repository fork we made: At the prompt type: git clone https://github.com/NIH-NICHD/exampleVolcanoPlotNotebook.git Now let's use our cd command to change into the appropriate directory. cd exampleVolcanoPlotNotebook","title":"Clone the repository in the Jupyterlab terminal window"},{"location":"classes/day-1-reasoning/running-a-jupyterlab-notebook/#recap","text":"In this lesson: * We logged into CAVATICA * We started an interactive analysis with JupyterLab * We authenticated with GitHub * We executed and saw some Command Line Functions * We generated a Volcano Plot in a JupyterLab notebook running an R kernel","title":"Recap"},{"location":"classes/day-1-reasoning/running-a-jupyterlab-notebook/#additional-resources","text":"CAVATICA documentation for the JupyterLab interface: https://docs.cavatica.org/docs/editor-quick-reference Official documentation from project JupyterLab: https://jupyterlab.readthedocs.io/en/stable/","title":"Additional resources:"},{"location":"classes/day-1-reasoning/starting-a-jupyter-lab-notebook/","text":"Starting A JupyterLab Notebook To start a notebook , we need a project . Please let David know your username in the Zoom Chat so you can be added to the elements-of-style-kids-first billing group. Create a new project We are at the login page, please select so we can create a new project. We need to select three pieces of important inforrmation. Enter a name, here I have used Kids-First-INCLUDE-eos - note you will see in the project URL how it uniquely is identified because it includes your user id . Select the appropriate Billing group. For this course, give David your userid and you will be added so you can select EoS-Include-KidsFirst . Select Allow network access \u2757Important\u2757 be sure that you select Allow network access Start a new Analysis Next, within this new project, we will start an analysis. To do this, select Data Studio and select to create a new analysis. Name the analysis, here I used Kids-First-INCLUDE-eos-analysis . Next select . Look over what you have entered When satisfied that it is correct, press Confirm it is starting Upon success, we will see it is starting, lets take a small break. After we will begin with the Command Line Return to Day 1 Agenda","title":"Starting a jupyter lab notebook"},{"location":"classes/day-1-reasoning/starting-a-jupyter-lab-notebook/#starting-a-jupyterlab-notebook","text":"To start a notebook , we need a project . Please let David know your username in the Zoom Chat so you can be added to the elements-of-style-kids-first billing group.","title":"Starting A JupyterLab Notebook"},{"location":"classes/day-1-reasoning/starting-a-jupyter-lab-notebook/#create-a-new-project","text":"We are at the login page, please select so we can create a new project. We need to select three pieces of important inforrmation. Enter a name, here I have used Kids-First-INCLUDE-eos - note you will see in the project URL how it uniquely is identified because it includes your user id . Select the appropriate Billing group. For this course, give David your userid and you will be added so you can select EoS-Include-KidsFirst . Select Allow network access \u2757Important\u2757 be sure that you select Allow network access","title":"Create a new project"},{"location":"classes/day-1-reasoning/starting-a-jupyter-lab-notebook/#start-a-new-analysis","text":"Next, within this new project, we will start an analysis. To do this, select Data Studio and select to create a new analysis. Name the analysis, here I used Kids-First-INCLUDE-eos-analysis . Next select . Look over what you have entered When satisfied that it is correct, press","title":"Start a new Analysis"},{"location":"classes/day-1-reasoning/starting-a-jupyter-lab-notebook/#confirm-it-is-starting","text":"Upon success, we will see it is starting, lets take a small break.","title":"Confirm it is starting"},{"location":"classes/day-1-reasoning/starting-a-jupyter-lab-notebook/#after-we-will-begin-with-the-command-line","text":"Return to Day 1 Agenda","title":"After   we will begin with the Command Line"},{"location":"classes/day-1-reasoning/using-the-command-line/","text":"Using the command line Because I want to make this course accessible to researchers/clinicians who may or may not have had previous command line experience, I recently asked chatGPT the following question: how would you explain \"what is a terminal window\" and \"what is a command line\" to someone naive to these terms? ChatGPT had the following response A terminal window is a type of computer program that allows you to interact with your computer using text commands instead of clicking on icons or menus with a mouse. It's a way to access the \"guts\" of your computer and execute specific instructions. A command line is the text interface in a terminal window where you can enter these instructions or commands. Essentially, it's a way to communicate with your computer using a language that it understands. Instead of pointing and clicking with a mouse, you type out specific commands to make things happen. For example, if you want to create a new folder, you could type \"mkdir foldername\" in the command line, and the computer would create a new folder with the name you specified. Or if you want to open a specific program, you could type its name followed by \".exe\" and press enter, and the program would launch. The command line can be a powerful tool for accomplishing tasks quickly and efficiently, but it does require some knowledge of specific commands and syntax. However, there are many resources available online for learning how to use the command line, and many people find it to be a valuable skill to have. However, there are many resources available online for learning how to use the command line, and many people find it to be a valuable skill to have. We begin by using a tool ExlainShell Utilising the ExplainShell interactive command explorer There is an excellent resource that provides an interactive command explainer that can be accessed at ExplainShell Feel free to use and paste the following commands from the tutorial to see an explanation of the command based on the official commands documentation. 1. List files in the current folder Directory is used interchangeably with the term folder. ls -l Inside ExplainShell you will see the following explanation: 2. Inspect your current folder location in the file system To see in which folder you are currently in, use the pwd command, short for print working directory . This will return the absolut path to the location you currently stand. NOTE: What is an absolute path? what is a relative path? See this resource for a thorough explanation Again feel free now to copy and paste this command in the ExplainShell pwd 3. Copy a file Now let's go to a terminal window inside of the JupyterLab notebook you have just started up. Select terminal Then you will see the command-line: Before we copy a file, let's use a command-line command echo and create a file with the content \"I am the header of your first README.md\". If you want to know about this command, look it up on ExplainShell . We are also using a command called a redirect -- this will use it to put content into a new file. echo 'I am the header of your first README.md' > README.md You can run ls -l to see that the file is there. As well as see the commands more and another command called less . Use ExplainShell to know about these commands, but they show the contents of the file as you might expect. Now let's create a copy: cp README.md duplicated_README.md Now let's inspect if the file was created, by listing files in the directory with the ls -l command. We are expecting to view the file with name duplicated_README.md in the listed files and folders ls -l 4. Deleting a file To delete (remove) a file we can use the command rm name_of_folder . Let's delete the duplicate README.md file named duplicated_README.md rm duplicated_README.md Now let's inspect if the file was created, by listing files in the directory with the ls -l command. We are expecting to not view the file with name duplicated_README.md as we just deleted it in the listed files and folders ls -l 5. Create a new directory (folder) To create a new folder if that folder doesn't exist, we can use the command mkdir name_of_folder . Let's create a folder named new_folder mkdir new_folder Now let's inspect if the folder was created, by listing files in the directory with the ls -l command. We are expecting to view the folder with name new_folder in the listed files and folders Let's inspect the folder exists with our trusted ls -l command ls -l 6. Delete a folder We can delete a folder using the rm -r command: rm -r new_folder Let's inspect that the folder is indeed deleted: ls -l 7. Downloading a file using a link We can retrieve a file using a public link. For example, we have uploaded in a public ZENODO record for this course an example file. The link for the file is: https://zenodo.org/record/4302133/files/deseq2_5k.csv Details of this file, which is a selected subset of the results of running DESeq on RNA-seq files, only those that were with an appropriate p-value (< 0.05) and above the 1.5 fold difference were put into the matrix. https://zenodo.org/record/4302133. You can see the version history, this file was created by a colleague at Lifebit, for the first time this course was offered in 2019 at the Jackson Laboratory. To retrieve this file, we are going to open the terminal window and run the command: cd /sbgenomics/workspace/Kids-First-Elements-of-Style-Workflow-Creation-Maintenance/classes/Running-a-JupyterLab-Notebook wget https://zenodo.org/record/4302133/files/deseq2_5k.csv So please copy this command and paste it in the terminal window and press return - in preparation for the next section of todays class. Reading data and plotting a volcano plot. Return to Day 1 Return to today's agenda","title":"Using the command line"},{"location":"classes/day-1-reasoning/using-the-command-line/#using-the-command-line","text":"Because I want to make this course accessible to researchers/clinicians who may or may not have had previous command line experience, I recently asked chatGPT the following question: how would you explain \"what is a terminal window\" and \"what is a command line\" to someone naive to these terms? ChatGPT had the following response A terminal window is a type of computer program that allows you to interact with your computer using text commands instead of clicking on icons or menus with a mouse. It's a way to access the \"guts\" of your computer and execute specific instructions. A command line is the text interface in a terminal window where you can enter these instructions or commands. Essentially, it's a way to communicate with your computer using a language that it understands. Instead of pointing and clicking with a mouse, you type out specific commands to make things happen. For example, if you want to create a new folder, you could type \"mkdir foldername\" in the command line, and the computer would create a new folder with the name you specified. Or if you want to open a specific program, you could type its name followed by \".exe\" and press enter, and the program would launch. The command line can be a powerful tool for accomplishing tasks quickly and efficiently, but it does require some knowledge of specific commands and syntax. However, there are many resources available online for learning how to use the command line, and many people find it to be a valuable skill to have. However, there are many resources available online for learning how to use the command line, and many people find it to be a valuable skill to have. We begin by using a tool ExlainShell","title":"Using the command line"},{"location":"classes/day-1-reasoning/using-the-command-line/#utilising-the-explainshell-interactive-command-explorer","text":"There is an excellent resource that provides an interactive command explainer that can be accessed at ExplainShell Feel free to use and paste the following commands from the tutorial to see an explanation of the command based on the official commands documentation.","title":"Utilising the ExplainShell interactive command explorer"},{"location":"classes/day-1-reasoning/using-the-command-line/#1-list-files-in-the-current-folder","text":"Directory is used interchangeably with the term folder. ls -l Inside ExplainShell you will see the following explanation:","title":"1. List files in the current folder"},{"location":"classes/day-1-reasoning/using-the-command-line/#2-inspect-your-current-folder-location-in-the-file-system","text":"To see in which folder you are currently in, use the pwd command, short for print working directory . This will return the absolut path to the location you currently stand. NOTE: What is an absolute path? what is a relative path? See this resource for a thorough explanation Again feel free now to copy and paste this command in the ExplainShell pwd","title":"2. Inspect your current folder location in the file system"},{"location":"classes/day-1-reasoning/using-the-command-line/#3-copy-a-file","text":"Now let's go to a terminal window inside of the JupyterLab notebook you have just started up. Select terminal Then you will see the command-line: Before we copy a file, let's use a command-line command echo and create a file with the content \"I am the header of your first README.md\". If you want to know about this command, look it up on ExplainShell . We are also using a command called a redirect -- this will use it to put content into a new file. echo 'I am the header of your first README.md' > README.md You can run ls -l to see that the file is there. As well as see the commands more and another command called less . Use ExplainShell to know about these commands, but they show the contents of the file as you might expect. Now let's create a copy: cp README.md duplicated_README.md Now let's inspect if the file was created, by listing files in the directory with the ls -l command. We are expecting to view the file with name duplicated_README.md in the listed files and folders ls -l","title":"3. Copy a file"},{"location":"classes/day-1-reasoning/using-the-command-line/#4-deleting-a-file","text":"To delete (remove) a file we can use the command rm name_of_folder . Let's delete the duplicate README.md file named duplicated_README.md rm duplicated_README.md Now let's inspect if the file was created, by listing files in the directory with the ls -l command. We are expecting to not view the file with name duplicated_README.md as we just deleted it in the listed files and folders ls -l","title":"4. Deleting a file"},{"location":"classes/day-1-reasoning/using-the-command-line/#5-create-a-new-directory-folder","text":"To create a new folder if that folder doesn't exist, we can use the command mkdir name_of_folder . Let's create a folder named new_folder mkdir new_folder Now let's inspect if the folder was created, by listing files in the directory with the ls -l command. We are expecting to view the folder with name new_folder in the listed files and folders Let's inspect the folder exists with our trusted ls -l command ls -l","title":"5. Create a new directory (folder)"},{"location":"classes/day-1-reasoning/using-the-command-line/#6-delete-a-folder","text":"We can delete a folder using the rm -r command: rm -r new_folder Let's inspect that the folder is indeed deleted: ls -l","title":"6. Delete a folder"},{"location":"classes/day-1-reasoning/using-the-command-line/#7-downloading-a-file-using-a-link","text":"We can retrieve a file using a public link. For example, we have uploaded in a public ZENODO record for this course an example file. The link for the file is: https://zenodo.org/record/4302133/files/deseq2_5k.csv Details of this file, which is a selected subset of the results of running DESeq on RNA-seq files, only those that were with an appropriate p-value (< 0.05) and above the 1.5 fold difference were put into the matrix. https://zenodo.org/record/4302133. You can see the version history, this file was created by a colleague at Lifebit, for the first time this course was offered in 2019 at the Jackson Laboratory. To retrieve this file, we are going to open the terminal window and run the command: cd /sbgenomics/workspace/Kids-First-Elements-of-Style-Workflow-Creation-Maintenance/classes/Running-a-JupyterLab-Notebook wget https://zenodo.org/record/4302133/files/deseq2_5k.csv So please copy this command and paste it in the terminal window and press return - in preparation for the next section of todays class. Reading data and plotting a volcano plot.","title":"7. Downloading a file using a link"},{"location":"classes/day-1-reasoning/using-the-command-line/#return-to-day-1","text":"Return to today's agenda","title":"Return to Day 1"},{"location":"classes/day-2-code-versioning/day-2-code-versioning/","text":"Agenda for the Day 2: Code Versioning Time (UTC) Programme 11.00 - 11.10 Workspace set up and agenda for the day 11.10 - 11.30 Why Git and GitHub? Motivation and set up in the JupyterLab workspace 11.30 - 11.50 GitHub Auth Login: How to authenticate from Jupyter Terminal 12.00 - 12.10 Short break 12.10 - 12.30 Git Routine 1 : Reusing an available repository with fork and how to keep in sync with parent project 12.30 - 12:50: Git Routine 2 : Extend your current code and use Git, GitHub to keep track of changes and contribute 12.50 - 13:00 Wrap up and overview of next days topics Additional resources: GitHub Hello World Happy Git and GitHub for the useR by Jenny Bryan GitHub Documentation","title":"Day 2 - Code versioning"},{"location":"classes/day-2-code-versioning/day-2-code-versioning/#agenda-for-the-day-2-code-versioning","text":"Time (UTC) Programme 11.00 - 11.10 Workspace set up and agenda for the day 11.10 - 11.30 Why Git and GitHub? Motivation and set up in the JupyterLab workspace 11.30 - 11.50 GitHub Auth Login: How to authenticate from Jupyter Terminal 12.00 - 12.10 Short break 12.10 - 12.30 Git Routine 1 : Reusing an available repository with fork and how to keep in sync with parent project 12.30 - 12:50: Git Routine 2 : Extend your current code and use Git, GitHub to keep track of changes and contribute 12.50 - 13:00 Wrap up and overview of next days topics","title":"Agenda for the Day 2: Code Versioning"},{"location":"classes/day-2-code-versioning/day-2-code-versioning/#additional-resources","text":"GitHub Hello World Happy Git and GitHub for the useR by Jenny Bryan GitHub Documentation","title":"Additional resources:"},{"location":"classes/day-2-code-versioning/github-authenticate/","text":"GitHub Authentication To make a contribution to a GitHub repository, you need to authenticate. You can do this with the tool we installed gh . I didn't mention where I found this? Yesterday I told you about Anaconda Package Library . There you can find an active contributor environment and the latest tools that can be used from the command line, in your containers and in your workflows. It is as I mentioned an excellent place to educate and everyday working with these pacakges is easier and easier. Here we see that gh is the GitHub CLI where CLI stands for Command Line Interface , the application we now have been using for the last two days. Generating A GitHub Authentication Token Before we authenticate with GitHub we need to generate a token that we will provide to GitHub to tell that application who we are. This is a Personal Access Token your private secret key to prove who you are. Step 1 - Navigate to Settings, located just under your profile in the upper right hand corner: Step 2 - Navigate to the bottom to < > Developer Settings on the bottom left hand corner: Step 3 - Select Personal access tokens third option from the top on the left side: Step 4 - Select Generate new token on the upper right corner - put a name in the note I used eos Step 5 - Select all of the options and select Generate token Step 6 - Copy the token because as the note mentions - it will not be available again - but you can regenerate tokens now - so don't worry. Now we can authenticate with GitHub Once we have our token, we can now authenticate. Step 1 - Type gh auth login at the command line in the Terminal window Step 2 - Use arrow keeps and select GitHub.com (should be the default) Step 3 - Select HTTPS for your _preferred protocol for git operations Step 4 - Confirm you wish to authenticate with your GitHub Credentials Step 5 - Use arrow keys to select Paste an authentication token Summary of all the steps Return to Agenda","title":"Github authenticate"},{"location":"classes/day-2-code-versioning/github-authenticate/#github-authentication","text":"To make a contribution to a GitHub repository, you need to authenticate. You can do this with the tool we installed gh . I didn't mention where I found this? Yesterday I told you about Anaconda Package Library . There you can find an active contributor environment and the latest tools that can be used from the command line, in your containers and in your workflows. It is as I mentioned an excellent place to educate and everyday working with these pacakges is easier and easier. Here we see that gh is the GitHub CLI where CLI stands for Command Line Interface , the application we now have been using for the last two days.","title":"GitHub Authentication"},{"location":"classes/day-2-code-versioning/github-authenticate/#generating-a-github-authentication-token","text":"Before we authenticate with GitHub we need to generate a token that we will provide to GitHub to tell that application who we are. This is a Personal Access Token your private secret key to prove who you are. Step 1 - Navigate to Settings, located just under your profile in the upper right hand corner: Step 2 - Navigate to the bottom to < > Developer Settings on the bottom left hand corner: Step 3 - Select Personal access tokens third option from the top on the left side: Step 4 - Select Generate new token on the upper right corner - put a name in the note I used eos Step 5 - Select all of the options and select Generate token Step 6 - Copy the token because as the note mentions - it will not be available again - but you can regenerate tokens now - so don't worry.","title":"Generating A GitHub Authentication Token"},{"location":"classes/day-2-code-versioning/github-authenticate/#now-we-can-authenticate-with-github","text":"Once we have our token, we can now authenticate. Step 1 - Type gh auth login at the command line in the Terminal window Step 2 - Use arrow keeps and select GitHub.com (should be the default) Step 3 - Select HTTPS for your _preferred protocol for git operations Step 4 - Confirm you wish to authenticate with your GitHub Credentials Step 5 - Use arrow keys to select Paste an authentication token Summary of all the steps","title":"Now we can authenticate with GitHub"},{"location":"classes/day-2-code-versioning/github-authenticate/#return-to-agenda","text":"","title":"Return to Agenda"},{"location":"classes/day-2-code-versioning/keeping-your-repository-fork-in-sync/","text":"Keeping your Repository Fork in Sync Let's now fork the repository which held the volcano plot yesterday. Fork the https://github.com/NIH-NICHD/exampleVolcanoPlotNotebook repository now. Let's fork the volcano plot repository Step 1 Navigate in your browser window to https://github.com/NIH-NICHD/exampleVolcanoPlotNotebook Step 2 Create a Fork In the browser on the right of the screen you see, Edit Pins Watch and then Fork select Fork Select where you want to fork the repository, your own GitHub repository. You will see the progress window that shows the Forking is in progress Upon success, you will see the following screen Clone your repository exampleVolcanoPlot.png\" These words Clone Fork Pull are the language of Git and GitHub. But they mean as follows: Clone - make a working copy of someone elses repository. Fork - make a copy of someone else's repository, but this one I will make edits to and try to make a contribution. Pull - pull a version of the repository Push - push my changes back to the repository. Synchronizing Your Fork Navigate to your repository within a new Chrome browser tab on GitHub - https://github/[ insert your GitHub user name here ]/exampleVolcanoPlotNotebook. In my case, you will see the NIH-NICHD repository commits behind the adeslatt/exampleVolcanoPlotNotebook So I navigate to the button below code and press the I then get presented the screen: And I press the . And we have synchronize success. But your repositories since they were a fork of the out-of-date repository -- need now to be updated, so let's go ahead and spend a few moments getting your repositories in sync. Your repository is now up-to-date with the master adeslatt/exampleVolcanoPlotNotebook Return to Day 2 Return to Day 2","title":"Keeping your repository fork in sync"},{"location":"classes/day-2-code-versioning/keeping-your-repository-fork-in-sync/#keeping-your-repository-fork-in-sync","text":"Let's now fork the repository which held the volcano plot yesterday. Fork the https://github.com/NIH-NICHD/exampleVolcanoPlotNotebook repository now.","title":"Keeping your Repository Fork in Sync"},{"location":"classes/day-2-code-versioning/keeping-your-repository-fork-in-sync/#lets-fork-the-volcano-plot-repository","text":"Step 1 Navigate in your browser window to https://github.com/NIH-NICHD/exampleVolcanoPlotNotebook Step 2 Create a Fork In the browser on the right of the screen you see, Edit Pins Watch and then Fork select Fork Select where you want to fork the repository, your own GitHub repository. You will see the progress window that shows the Forking is in progress Upon success, you will see the following screen","title":"Let's fork the volcano plot repository"},{"location":"classes/day-2-code-versioning/keeping-your-repository-fork-in-sync/#clone-your-repository-examplevolcanoplotpng","text":"These words Clone Fork Pull are the language of Git and GitHub. But they mean as follows: Clone - make a working copy of someone elses repository. Fork - make a copy of someone else's repository, but this one I will make edits to and try to make a contribution. Pull - pull a version of the repository Push - push my changes back to the repository.","title":"Clone your repository exampleVolcanoPlot.png\""},{"location":"classes/day-2-code-versioning/keeping-your-repository-fork-in-sync/#synchronizing-your-fork","text":"Navigate to your repository within a new Chrome browser tab on GitHub - https://github/[ insert your GitHub user name here ]/exampleVolcanoPlotNotebook. In my case, you will see the NIH-NICHD repository commits behind the adeslatt/exampleVolcanoPlotNotebook So I navigate to the button below code and press the I then get presented the screen: And I press the . And we have synchronize success. But your repositories since they were a fork of the out-of-date repository -- need now to be updated, so let's go ahead and spend a few moments getting your repositories in sync. Your repository is now up-to-date with the master adeslatt/exampleVolcanoPlotNotebook","title":"Synchronizing Your Fork"},{"location":"classes/day-2-code-versioning/keeping-your-repository-fork-in-sync/#return-to-day-2","text":"Return to Day 2","title":"Return to Day 2"},{"location":"classes/day-2-code-versioning/the-add-push-git-routine/","text":"Git Routine 2: Extend your current code and use Git, GitHub to Keep track of Changes and collaborate In this exercise, we will use the personal forks as the repository where each of us will contribute. To do that we are going to make a clone of your version of the repository. Use the command line tool mkdir to make a directory with your user name. My user name is adeslatt mkdir adeslatt Move into that directory using the command line tool cd to change directory into your user name cd adeslatt In this case now I am in an empty directory. Now I am going to clone my forked version of the NIH-NICHD exampleVolcanoPlot repository. git clone https://github.com/Wellstein-lab/exampleVolcanoPlotNotebook.git Now we need to move into the exampleVolcanoPlotNotebook directory. cd exampleVolcanoPlotNotebook Type as the following command to see where the upstream location of our file is. A very handy command I use daily git status The git status command helps us inspect to see if there are any changes to our local copy of our repository. git status You should see something like: On branch main Your branch is up to date with 'origin/main'. nothing to commit, working tree clean Edit a file We can edit a file inside your folder README.md Let's use nano or the notebook interface to edit tje README.md file. First lets navigate to the directory for this class. cd <my username>/exampleVolcanoPlotNotebook Next lets edit a file README.md You can use Nano or you can double click the file on the left and add some lines to the readme. Learn about Zenodo here: https://zenodo.org When ready to save the file by using these shortcut keys. in Nano or just hit x and it will ask you to save. CTRL + O This will declare your willingness to save the file. Click ENTER to verify that you want to save the file. Now Close the file and exit from nano with the the shortcut: CTRL + X Awesome, your file has just been saved! You can inspect using the head command, eg head README.md Inspect the changes with git status git status which returns something like On branch main Your branch is up to date with 'origin/main'. Untracked files: (use \"git add <file>...\" to include in what will be committed) README.md nothing added to commit but untracked files present (use \"git add\" to track) We now need to add the modified file to the branch git add README.md We also need to write a message denoting what change was done and finalize the addition. git commit -m \"added useful commands to the README.md\" Now we need to execute the last step -- which is to push the changes to GitHub git push Now we can see what we have done with git status git status And we see we are now up to date. Contributing to the main branch Now we have the changes in our repository. We are out of sync potentially with the main repository. Lets inspect. In the chrome browser, navigate to your repository on GitHub and inspect our local changes. First, we synchronize with the main branch, by pressing the sync button. Next, we make a pull request, this allows us to offer our changes to the main repository. As the repository owner, I can inspect and accept or not accept all the proposed changes. Conclusion of todays class Recap _ You learned about Git and GitHub _ You Fork ed a repository _ You synchronized your Fork to the main repository - You made a contribution with a Pull Request You have entered the world of documenting your work using GitHub -- Congratulations!! Return to Agenda","title":"Git Routine 2:  Extend your current code and use Git, GitHub to Keep track of Changes and collaborate"},{"location":"classes/day-2-code-versioning/the-add-push-git-routine/#git-routine-2-extend-your-current-code-and-use-git-github-to-keep-track-of-changes-and-collaborate","text":"In this exercise, we will use the personal forks as the repository where each of us will contribute. To do that we are going to make a clone of your version of the repository. Use the command line tool mkdir to make a directory with your user name. My user name is adeslatt mkdir adeslatt Move into that directory using the command line tool cd to change directory into your user name cd adeslatt In this case now I am in an empty directory. Now I am going to clone my forked version of the NIH-NICHD exampleVolcanoPlot repository. git clone https://github.com/Wellstein-lab/exampleVolcanoPlotNotebook.git Now we need to move into the exampleVolcanoPlotNotebook directory. cd exampleVolcanoPlotNotebook Type as the following command to see where the upstream location of our file is.","title":"Git Routine 2:  Extend your current code and use Git, GitHub to Keep track of Changes and collaborate"},{"location":"classes/day-2-code-versioning/the-add-push-git-routine/#a-very-handy-command-i-use-daily-git-status","text":"The git status command helps us inspect to see if there are any changes to our local copy of our repository. git status You should see something like: On branch main Your branch is up to date with 'origin/main'. nothing to commit, working tree clean","title":"A very handy command I use daily git status"},{"location":"classes/day-2-code-versioning/the-add-push-git-routine/#edit-a-file","text":"We can edit a file inside your folder README.md Let's use nano or the notebook interface to edit tje README.md file. First lets navigate to the directory for this class. cd <my username>/exampleVolcanoPlotNotebook Next lets edit a file README.md You can use Nano or you can double click the file on the left and add some lines to the readme. Learn about Zenodo here: https://zenodo.org When ready to save the file by using these shortcut keys. in Nano or just hit x and it will ask you to save. CTRL + O This will declare your willingness to save the file. Click ENTER to verify that you want to save the file. Now Close the file and exit from nano with the the shortcut: CTRL + X Awesome, your file has just been saved! You can inspect using the head command, eg head README.md","title":"Edit a file"},{"location":"classes/day-2-code-versioning/the-add-push-git-routine/#inspect-the-changes-with-git-status","text":"git status which returns something like On branch main Your branch is up to date with 'origin/main'. Untracked files: (use \"git add <file>...\" to include in what will be committed) README.md nothing added to commit but untracked files present (use \"git add\" to track) We now need to add the modified file to the branch git add README.md We also need to write a message denoting what change was done and finalize the addition. git commit -m \"added useful commands to the README.md\" Now we need to execute the last step -- which is to push the changes to GitHub git push Now we can see what we have done with git status git status And we see we are now up to date.","title":"Inspect the changes with git status"},{"location":"classes/day-2-code-versioning/the-add-push-git-routine/#contributing-to-the-main-branch","text":"Now we have the changes in our repository. We are out of sync potentially with the main repository. Lets inspect. In the chrome browser, navigate to your repository on GitHub and inspect our local changes. First, we synchronize with the main branch, by pressing the sync button. Next, we make a pull request, this allows us to offer our changes to the main repository. As the repository owner, I can inspect and accept or not accept all the proposed changes.","title":"Contributing to the main branch"},{"location":"classes/day-2-code-versioning/the-add-push-git-routine/#conclusion-of-todays-class","text":"Recap _ You learned about Git and GitHub _ You Fork ed a repository _ You synchronized your Fork to the main repository - You made a contribution with a Pull Request You have entered the world of documenting your work using GitHub -- Congratulations!!","title":"Conclusion of todays class"},{"location":"classes/day-2-code-versioning/the-add-push-git-routine/#return-to-agenda","text":"","title":"Return to Agenda"},{"location":"classes/day-2-code-versioning/why-git-and-setup/","text":"Why Git and GitHub? Motivation and set up in the JupyterLab workspace Why Git? Git is a technology or a system used for version control of code Frees up our code from our local machine (no risk of losing files from your laptop or your jupyterlab notebook on an ephemeral image!) Ensures that we are tracking the changes of our code as a project progresses Allows us to revisit older versions of our work (R codes, notebooks, even this markdown used to make this course - text files, etc). Allows us to keep different versions of our work Why GitHub? Git Hub is a \"code hosting platform for version control and collaboration\" Large community of open source project to browse Enables collaboration of many contributors in the same project Serves as a code vault for making our reasearch code findable and our methods transparent Setting up in the terminal In the JupyterLab notebook we started on Cavatica, looking like this: Please open the Terminal window. We are going back to the command line. in zoom to know all is well. Using the nano file editor Before we start working with Git in the command line, we will learn one more helpful skill. How to use a code editor. A code editors allows us to edit files in an environment without a graphical user interface. For us, it will be needed in some Git related tasks, so we want to be prepared. Edit a file using nano in only 4 steps: Your screen likely looks similar to this: 1. Type nano hello.txt This will create and open a new file hello.txt . If the file already exists, it will open that existing file. If the file is not yet in the directory, the screen looks like this: 2. Write inside the open file named hello.txt As you typically would type in any Document editor (eg. Word, Google Docs, etc). You can type for example, Hello Git! 3. When ready to save type the shortcut: CTRL + O This will declare your willingness to save the file. Click ENTER to verify that you want to save the file. Now you will see it in the directory on the left as well showing that the file has now been saved. 4. Close the file and exit from nano with the the shortcut: CTRL + X Awesome, your file has just been saved! You can inspect using another command line function, head . In the terminal window at the command line, type: head hello.txt We will probably need soon nano in one of our Git tasks. Creating an environment to install packages Not exactly required as the base environment within CAVATICA JupyterLab is already there. But as a habit I begin with creating an conda environment so that I can keep track of what I add. When we get to containerizing, you will see how using conda will help keep all of the details about what you need to install safely as minimal as you can. We will cover that more on the containerization less, which is tomorrow. conda create -n eos -y Once the environment is created, you activate it as instructed conda activate eos Then I install two additional things that I use on a regular basis, GitHub CLI and emacs conda install -c conda-forge gh -y (Optionally install emacs) conda install -c conda-forge emacs -y More on Anaconda and packages later. Configuring our GitHub user information To be able to use Git and GitHub from the command line we need to configure the information related to our GitHub user. Let's follow the commands below to set in the workspace the required information. Set user name Replace <my github user name> with your actual GitHub email. git config --global user.name <my github user name> Set user email Replace <my email associated with my github user name> with your actual GitHub email. git config --global user.email <my email associated with my github user name> (Optional) Set preferred file editor eg nano or emacs git config --global core.editor nano Or if you like emacs , or vim , change it to your desired editor . git config --global core.editor emacs Return to Agenda","title":"Why Git and GitHub?"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#why-git-and-github","text":"Motivation and set up in the JupyterLab workspace","title":"Why Git and GitHub?"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#why-git","text":"Git is a technology or a system used for version control of code Frees up our code from our local machine (no risk of losing files from your laptop or your jupyterlab notebook on an ephemeral image!) Ensures that we are tracking the changes of our code as a project progresses Allows us to revisit older versions of our work (R codes, notebooks, even this markdown used to make this course - text files, etc). Allows us to keep different versions of our work","title":"Why Git?"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#why-github","text":"Git Hub is a \"code hosting platform for version control and collaboration\" Large community of open source project to browse Enables collaboration of many contributors in the same project Serves as a code vault for making our reasearch code findable and our methods transparent","title":"Why GitHub?"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#setting-up-in-the-terminal","text":"In the JupyterLab notebook we started on Cavatica, looking like this: Please open the Terminal window. We are going back to the command line. in zoom to know all is well.","title":"Setting up in the terminal"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#using-the-nano-file-editor","text":"Before we start working with Git in the command line, we will learn one more helpful skill. How to use a code editor. A code editors allows us to edit files in an environment without a graphical user interface. For us, it will be needed in some Git related tasks, so we want to be prepared. Edit a file using nano in only 4 steps: Your screen likely looks similar to this:","title":"Using the nano file editor"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#1-type-nano-hellotxt","text":"This will create and open a new file hello.txt . If the file already exists, it will open that existing file. If the file is not yet in the directory, the screen looks like this:","title":"1. Type nano hello.txt"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#2-write-inside-the-open-file-named-hellotxt","text":"As you typically would type in any Document editor (eg. Word, Google Docs, etc). You can type for example, Hello Git!","title":"2. Write inside the open file named hello.txt"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#3-when-ready-to-save-type-the-shortcut","text":"CTRL + O This will declare your willingness to save the file. Click ENTER to verify that you want to save the file. Now you will see it in the directory on the left as well showing that the file has now been saved.","title":"3. When ready to save type the shortcut:"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#4-close-the-file-and-exit-from-nano-with-the-the-shortcut","text":"CTRL + X Awesome, your file has just been saved! You can inspect using another command line function, head . In the terminal window at the command line, type: head hello.txt We will probably need soon nano in one of our Git tasks.","title":"4. Close the file and exit from nano with the the shortcut:"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#creating-an-environment-to-install-packages","text":"Not exactly required as the base environment within CAVATICA JupyterLab is already there. But as a habit I begin with creating an conda environment so that I can keep track of what I add. When we get to containerizing, you will see how using conda will help keep all of the details about what you need to install safely as minimal as you can. We will cover that more on the containerization less, which is tomorrow. conda create -n eos -y Once the environment is created, you activate it as instructed conda activate eos Then I install two additional things that I use on a regular basis, GitHub CLI and emacs conda install -c conda-forge gh -y","title":"Creating an environment to install packages"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#optionally-install-emacs","text":"conda install -c conda-forge emacs -y More on Anaconda and packages later.","title":"(Optionally install emacs)"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#configuring-our-github-user-information","text":"To be able to use Git and GitHub from the command line we need to configure the information related to our GitHub user. Let's follow the commands below to set in the workspace the required information.","title":"Configuring our GitHub user information"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#set-user-name","text":"Replace <my github user name> with your actual GitHub email. git config --global user.name <my github user name>","title":"Set user name"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#set-user-email","text":"Replace <my email associated with my github user name> with your actual GitHub email. git config --global user.email <my email associated with my github user name>","title":"Set user email"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#optional-set-preferred-file-editor-eg-nano-or-emacs","text":"git config --global core.editor nano Or if you like emacs , or vim , change it to your desired editor . git config --global core.editor emacs","title":"(Optional) Set preferred file editor eg nano or emacs"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#return-to-agenda","text":"","title":"Return to Agenda"},{"location":"classes/day-2-code-versioning/workspace-setup/","text":"Workspace Set-up Today, we will start either with the JupyterLab Notebook you started yesterday or if you have not already registered and started a JupyterLab Notebook, please go ahead and do so. For the class, we recommend you use the Chrome Browswer. And as you click through the lesson, open each link in a new tab. Already had a notebook yesterday Navigate to CAVATICA and go ahead and log in . You will land at a Dashboard, on the left you have your projects, and on your right your analyses. Select Data Studio You will see all your past analyses. Note that you do not pay for your analyses when they are stopped. You also see documentation regarding the environment you had set up, the cost and the duration spent on the session. You will also see Files and Settings . You Do pay for storage. The Cloud Cost Overview the Kids First DRC helps you calculate costs. Files Lets look at Files. In general, adopting a habit of getting in to do your work, developing your analysis and your workflows in small pieces with small files before you execute over 100s if not 1000s of files, will save you time and money. Be judicious in what files you need to keep. This is a habit that will save you. This is why we use GitHub, we deposit our original measurement data in appropriate public storage sites. We use repositories such as Zenodo for Data Object Identifiers and GitHub to track our Notebooks, which are essentially our scientific electronic notebooks, and our workflows. Settings Here you have the opportunity to change the size of your machine. If you scroll down, you will see the size and the price of the machine. One thing to note. We are using a dedicated instance when we are doing interactive analysis. That is why the price is $0.34/hour. When we run a workflow, we typically use spot instances that are usually 1/8th the cost or so. This then allows us to Fire and Forget . The beauty of spot instances is not only are they cheaper, but they help enforce that they are ephemeral, that is from a security, cost standpoint they are ideal. They exist, their images loaded onto them and then they are gone. This is why we always run workflows that are in GitHub and/or an App that is persistent. The memory of what was run is held in the repository. So when you publish, or you collaborate, or when you ship your analyses or your workflow to a collaborator, you can share the repository, making configuration adjustments for platform differences within the repository. More on that later. Start your notebook. Go ahead and hit and lets get started. Give us a in the Zoom chat if all is going ok. You will soon see the JupyterLab Launch pad - which as we know will stay alive for about 30 minutes. Lets go back to the lesson and understand a bit about Why Git and GitHub . Starting From Scratch If you were not with us yesterday, please follow these directions to start a notebook. If you need help, we will help you at the coffee break or reach out to David in the Chat. Continue with our Lesson for Day 2 Return to Agenda","title":"Workspace Set-up"},{"location":"classes/day-2-code-versioning/workspace-setup/#workspace-set-up","text":"Today, we will start either with the JupyterLab Notebook you started yesterday or if you have not already registered and started a JupyterLab Notebook, please go ahead and do so. For the class, we recommend you use the Chrome Browswer. And as you click through the lesson, open each link in a new tab.","title":"Workspace Set-up"},{"location":"classes/day-2-code-versioning/workspace-setup/#already-had-a-notebook-yesterday","text":"Navigate to CAVATICA and go ahead and log in . You will land at a Dashboard, on the left you have your projects, and on your right your analyses.","title":"Already had a notebook yesterday"},{"location":"classes/day-2-code-versioning/workspace-setup/#select-data-studio","text":"You will see all your past analyses. Note that you do not pay for your analyses when they are stopped. You also see documentation regarding the environment you had set up, the cost and the duration spent on the session. You will also see Files and Settings . You Do pay for storage. The Cloud Cost Overview the Kids First DRC helps you calculate costs.","title":"Select Data Studio"},{"location":"classes/day-2-code-versioning/workspace-setup/#files","text":"Lets look at Files. In general, adopting a habit of getting in to do your work, developing your analysis and your workflows in small pieces with small files before you execute over 100s if not 1000s of files, will save you time and money. Be judicious in what files you need to keep. This is a habit that will save you. This is why we use GitHub, we deposit our original measurement data in appropriate public storage sites. We use repositories such as Zenodo for Data Object Identifiers and GitHub to track our Notebooks, which are essentially our scientific electronic notebooks, and our workflows.","title":"Files"},{"location":"classes/day-2-code-versioning/workspace-setup/#settings","text":"Here you have the opportunity to change the size of your machine. If you scroll down, you will see the size and the price of the machine. One thing to note. We are using a dedicated instance when we are doing interactive analysis. That is why the price is $0.34/hour. When we run a workflow, we typically use spot instances that are usually 1/8th the cost or so. This then allows us to Fire and Forget . The beauty of spot instances is not only are they cheaper, but they help enforce that they are ephemeral, that is from a security, cost standpoint they are ideal. They exist, their images loaded onto them and then they are gone. This is why we always run workflows that are in GitHub and/or an App that is persistent. The memory of what was run is held in the repository. So when you publish, or you collaborate, or when you ship your analyses or your workflow to a collaborator, you can share the repository, making configuration adjustments for platform differences within the repository. More on that later.","title":"Settings"},{"location":"classes/day-2-code-versioning/workspace-setup/#start-your-notebook","text":"Go ahead and hit and lets get started. Give us a in the Zoom chat if all is going ok. You will soon see the JupyterLab Launch pad - which as we know will stay alive for about 30 minutes. Lets go back to the lesson and understand a bit about Why Git and GitHub .","title":"Start your notebook."},{"location":"classes/day-2-code-versioning/workspace-setup/#starting-from-scratch","text":"If you were not with us yesterday, please follow these directions to start a notebook. If you need help, we will help you at the coffee break or reach out to David in the Chat.","title":"Starting From Scratch"},{"location":"classes/day-2-code-versioning/workspace-setup/#continue-with-our-lesson-for-day-2","text":"Return to Agenda","title":"Continue with our Lesson for Day 2"},{"location":"classes/day-3-containerization/building-dockerfiles/","text":"Building Dockerfiles We have now learned that we can control our environment with conda . If you haven't done that lesson, it would be best for you to go through that lesson before beginning with this step. The Conda Lesson will show you how to create a clean environment. Now we will show how to build an Docker image Docker from a Dockerfile and we will use this in our workflow in three ways: in a Nextflow workflow script, in a common workflow script and as an Application within CAVATICA. What is a Docker image and a Dockerfile Docker images are created by using a so called Dockerfile i.e. a simple text file containing a list of commands to be executed to assemble and configure the image with the software packages required. In this step, you will create a Docker image containing the FastQC & MultiQC tools. Warning: the Docker build process automatically copies all files that are located in the current directory to the Docker daemon in order to create the image. This can take a lot of time when big/many files exist. For this reason, it's important to always work in a directory containing only the files you really need to include in your Docker image. Alternatively, you can use the .dockerignore file to select the path to exclude from the build. Navigate to the Google Shell Window We will use both the editor and the shell terminal window to accomplish these next tasks. If you do not already have this activated, activate your environment we just made. Copy the lines below or type in your own terminal window. conda activate eos Creating your first Dockerfile Then make the clean directory for us to work in. Copy the lines here below into your own window. mkdir fastqc-docker Now navigate into that directory, again you can type this or simply copy the lines below in your google shell terminal window. cd fastqc-docker Now we are going to create our first Dockerfile touch is another handy bash command for creating an empty file. touch Dockerfile The docker application requires the exact filename Dockerfile when creating the image. In the google shell editor window this time, open the file named Dockerfile Copy the contents of the window below and paste it into your window. # Full contents of Dockerfile FROM continuumio/miniconda3 LABEL description=\"Base docker image with conda and util libraries\" ARG ENV_NAME=\"fastqc\" # Install the conda environment COPY environment.yml / RUN conda env create --quiet --name ${ENV_NAME} --file /environment.yml && conda clean -a # Add conda installation dir to PATH (instead of doing 'conda activate') ENV PATH /opt/conda/envs/${ENV_NAME}/bin:$PATH After pasting in the window, your file should look like this: The docker application understands the commands FROM , LABEL , ARG ENV_NAME , COPY , RUN and ENV PATH . It also knows that # are comments and are for the human reader that is reading this file to understand what is happening within the file itself. The base image we are using for the Dockerfile is from continuumio the maker of our miniconda3 which we installed earlier. It is because of that environment that we do not need to install conda , it is part of the image. We are using the function COPY to bring it into the image. We are adding to this base image our new function fastqc . You will see later that you can COPY your own custom scripts for use within the container. This is the process of containerization . Notice as well we are prompted to look at our quotas, not an infinite resource but pretty good availability for a free asset. Now we will inspect our environment.yml which is being used by conda to install our desired application. Creating your environment.yml File Notice that this Docker file uses a file called environment.yml . Back in the terminal window again: Type at the prompt touch environment.yml Return to the Google Shell editor and open the file named environment.yml and copy the contents of the window below into that file. What we are doing is using conda to control our environment. And this file is used within our Dockerfile to create the controlled environment and install the function, fastqc . The channels listed are exactly the same channels we would have used if we wanted to make the fastqc application available within our environment to use on the command line. What we are doing is then reproducing this command line environment within our soon to be Docker image. name: fastqc channels: - bioconda - defaults dependencies: - fastqc Building our Dockerfile image for fastqc Build the Docker image by using the following command: There are three steps with building a Docker image: build tag push Here below we are executing the build command with a tag, indicated by the -t . The . says to build in the current directory. This is the normal way to build your Docker image because as we noted before, the docker application will use all that is within your current directory to make its image. docker build -t fastqc . Here the google shell asks to authorize it to complete the task. When it completes, we can verify that the image has been created listing all available images with the following command: docker images Test this image from the command line By now you see what we have done. You could install fastqc using the command conda install -c bioconda fastqc -y We see what is happening is using the Anaconda channel bioconda we are installing fastqc for our local use within our own file system on this small virtual instance made free for us from Google . Let's do that real quick to show what I mean. Go ahead and copy the line above on the command line and install fastqc within our eos environment. As expected it has been installed within the binary directory, the directory that contains executables and shortened to the unix convention to be just bin of our controlled environment eos . This would be fine if we wanted to do local work on some fastqc files. But it would not be available for the workflow that will be spun up on ephemeral machines where we would have to build our entire environment from scratch. But I have made the argument, that what is more sustainable is if we containerize at the process level, our work will be more durable, more portable, and more repurposable. That is it will be available for me to use for future work. So we built a container image. Now how could we use that container image? To test this tool from the command line Set up an environment variable capturing your current command line: PWD=$(pwd) To make our local directory available to the image, which is a self-contained environment, we need to do something called mounting and use your current directory and call the tool now encapsulated within the environment. I use this command to do that. docker run -it -v $PWD:$PWD -w $PWD fastqc fastqc -h You can read more about what is happening in detail with Docker containers and images . You don't need to know every detail to use them. Add this code to your GitHub repository It is a best practice with GitHub to always add a README.md . Let's add this file and then from the command line use the GitHub command line tools to push this to our GitHub repository. touch README.md Open with the Code Editor (the Markdown Editor is experimental). To do so, hover over the README.md file and click Open With... and select Code Editor . Copy the contents here that explain what we have in this repository # fastqc-docker Build a Container for fastqc from bioconda Steps to build this docker container. 1. Look up on [anaconda](https://anaconda.org/) the tool you wish to install 2. create an `environment.yml` file either manually or automatically 3. Use the template `Dockerfile` modifying if necessary (in our case we have no custom files for the `src` directory so we do not use that) 4. Build the Docker Image 5. Set up GitHub Actions To build your image from the command line: * Can do this on [Google shell](https://shell.cloud.google.com) - docker is installed and available \\```bash docker build -t fastqc . \\``` To test this tool from the command line Set up an environment variable capturing your current command line: \\```bash PWD=$(pwd) \\``` Then mount and use your current directory and call the tool now encapsulated within the environment. \\```bash docker run -it -v $PWD:$PWD -w $PWD fastqc fastqc -h \\``` Now edit the file (I added the \\ so Markdown would ignore the data but it means we need to edit them out for the README.md to properly render on the repo. Please remove all the \\ 's the file will look like this: Use GitHub Command line tool gh to authenticate before we push create our repository. The Google shell comes with the gh already installed. So we just begin with authentication. gh auth login As we did yesterday, authenticate with your Personal Authentication Token (eos) ad376@cloudshell:~/fastqc-docker$ gh auth login ? What account do you want to log into? GitHub.com ? What is your preferred protocol for Git operations? HTTPS ? Authenticate Git with your GitHub credentials? Yes ? How would you like to authenticate GitHub CLI? [Use arrows to move, type to filter] Login with a web browser > Paste an authentication token Now we can from the command line create this repository. Let's go ahead and tell git who we are. git config --global user.email \"adeslat@scitechcon.org\" git config --global user.name \"adeslatt\" Following GitHub 's updated instructions on how to create a new repository from the command line We now type git init -b main Which will return Initialized empty Git repository in /home/ad376/fastqc-docker/.git/ And then we type git add . && git commit -m \"initial commit\" which returns something like this. [main (root-commit) d1e421f] initial commit 3 files changed, 33 insertions(+) create mode 100644 Dockerfile create mode 100644 README.md create mode 100644 environment.yml We then use the gh repo create command to create the reposistory. gh repo create Which then prompts us to what we need to do -- important is that what we want to do is Push an existing local repository to GitHub . The tool's remainder defaults are acceptable, because we staged ourselves with the name of the directory. (eos) ad376@cloudshell:~/fastqc-docker$ gh repo create ? What would you like to do? Push an existing local repository to GitHub ? Path to local repository . ? Repository name fastqc-docker ? Description a container for the bioconda command fastqc ? Visibility Public \u2713 Created repository adeslatt/fastqc-docker on GitHub ? Add a remote? Yes ? What should the new remote be called? origin \u2713 Added remote https://github.com/adeslatt/fastqc-docker.git ? Would you like to push commits from the current branch to \"origin\"? Yes \u2713 Pushed commits to https://github.com/adeslatt/fastqc-docker.git Building the multiqc Docker image Navigate to the top of your home directory cd ~ Clone the multiqc-docker repository. git clone https://github.com/adeslatt/multiqc-docker.git Creating Your Second Dockerfile Navigate now to your home directory on the google shell. Remember that .. is the parent directory. cd .. mkdir multiqc-docker Now navigate into that directory, again you can type this or simply copy the lines below in your google shell terminal window. cd multiqc-docker Now we are going to create our first Dockerfile touch is another handy bash command for creating an empty file. touch Dockerfile The docker application requires the exact filename Dockerfile when creating the image. Again in the google shell editor window open the file named Dockerfile Copy the contents of the window below and paste it into your window. # Full contents of Dockerfile FROM continuumio/miniconda3 LABEL description=\"Base docker image with conda and util libraries\" ARG ENV_NAME=\"multiqc\" # Install the conda environment COPY environment.yml / RUN conda env create --quiet --name ${ENV_NAME} --file /environment.yml && conda clean -a # Add conda installation dir to PATH (instead of doing 'conda activate') ENV PATH /opt/conda/envs/${ENV_NAME}/bin:$PATH Creating your multiqc environment.yml File Back in the terminal window again: Type at the prompt touch environment.yml What we are doing is then reproducing this command line environment within our soon to be Docker image. name: multiqc channels: - bioconda - defaults dependencies: - multiqc And build the image docker build -t multiqc . Inspect what images you have now available to you locally You can see what you have built -- and see that we have tag ged our files in a certain way docker images Add this multiqc code to your GitHub repository It is a best practice with GitHub to always add a README.md . Let's add this file and then from the command line use the GitHub command line tools to push this to our GitHub repository. touch README.md Open with the Code Editor (the Markdown Editor is experimental). To do so, hover over the README.md file and click Open With... and select Code Editor . # multiqc-docker Build a Container for multiqc from bioconda Steps to build this docker container. 1. Look up on [anaconda](https://anaconda.org/) the tool you wish to install 2. create an `environment.yml` file either manually or automatically 3. Use the template `Dockerfile` modifying if necessary (in our case we have no custom files for the `src` directory so we do not use that) 4. Build the Docker Image 5. Set up GitHub Actions To build your image from the command line: * Can do this on [Google shell](https://shell.cloud.google.com) - docker is installed and available \\```bash docker build -t multiqc . \\``` To test this tool from the command line Set up an environment variable capturing your current command line: \\```bash PWD=$(pwd) \\``` Then mount and use your current directory and call the tool now encapsulated within the environment. \\```bash docker run -it -v $PWD:$PWD -w $PWD multiqc multiqc -h \\``` Now edit the file (I added the \\ so Markdown would ignore the data but it means we need to edit them out for the README.md to properly render on the repo. Please remove all the \\ 's the file will look like this: After pasting in the window, your file should look like this: We now type git init -b main Which will return Initialized empty Git repository in /home/ad376/fastqc-docker/.git/ And then we type git add . && git commit -m \"initial commit\" which returns something like this. [main (root-commit) d1e421f] initial commit 3 files changed, 33 insertions(+) create mode 100644 Dockerfile create mode 100644 README.md create mode 100644 environment.yml We then use the gh repo create command to create the reposistory. gh repo create Which then prompts us to what we need to do -- important is that what we want to do is Push an existing local repository to GitHub . The tool's remainder defaults are acceptable, because we staged ourselves with the name of the directory. (eos) ad376@cloudshell:~/multiqc-docker$ gh repo create ? What would you like to do? Push an existing local repository to GitHub ? Path to local repository . ? Repository name multiqc-docker ? Description a container for the bioconda command multiqc ? Visibility Public \u2713 Created repository adeslatt/fastqc-docker on GitHub ? Add a remote? Yes ? What should the new remote be called? origin \u2713 Added remote https://github.com/adeslatt/multiqc-docker.git ? Would you like to push commits from the current branch to \"origin\"? Yes \u2713 Pushed commits to https://github.com/adeslatt/multiqc-docker.git Upon success your repository will be in place on your GitHub. Recap We have created a Dockerfile using the Google Shell Editor We have created the environment.yml file also with the Google Shell Editor We have used the new command placed our desired function fastqc within the environment.yml file for the ultimate Docker container. We built the container for the application fastqc in the fastqc-docker subdirectory. We used GitHub command line tool gh to take our local repositories and put both up on GitHub. We inspected our images. Tomorrow, we will make our first workflow and show how to use these containers within our private repository on CAVATICA Return to the Agenda","title":"Building dockerfiles"},{"location":"classes/day-3-containerization/building-dockerfiles/#building-dockerfiles","text":"We have now learned that we can control our environment with conda . If you haven't done that lesson, it would be best for you to go through that lesson before beginning with this step. The Conda Lesson will show you how to create a clean environment. Now we will show how to build an Docker image Docker from a Dockerfile and we will use this in our workflow in three ways: in a Nextflow workflow script, in a common workflow script and as an Application within CAVATICA.","title":"Building Dockerfiles"},{"location":"classes/day-3-containerization/building-dockerfiles/#what-is-a-docker-image-and-a-dockerfile","text":"Docker images are created by using a so called Dockerfile i.e. a simple text file containing a list of commands to be executed to assemble and configure the image with the software packages required. In this step, you will create a Docker image containing the FastQC & MultiQC tools. Warning: the Docker build process automatically copies all files that are located in the current directory to the Docker daemon in order to create the image. This can take a lot of time when big/many files exist. For this reason, it's important to always work in a directory containing only the files you really need to include in your Docker image. Alternatively, you can use the .dockerignore file to select the path to exclude from the build.","title":"What is a Docker image and a Dockerfile"},{"location":"classes/day-3-containerization/building-dockerfiles/#navigate-to-the-google-shell-window","text":"We will use both the editor and the shell terminal window to accomplish these next tasks. If you do not already have this activated, activate your environment we just made. Copy the lines below or type in your own terminal window. conda activate eos","title":"Navigate to the Google Shell Window"},{"location":"classes/day-3-containerization/building-dockerfiles/#creating-your-first-dockerfile","text":"Then make the clean directory for us to work in. Copy the lines here below into your own window. mkdir fastqc-docker Now navigate into that directory, again you can type this or simply copy the lines below in your google shell terminal window. cd fastqc-docker Now we are going to create our first Dockerfile touch is another handy bash command for creating an empty file. touch Dockerfile The docker application requires the exact filename Dockerfile when creating the image. In the google shell editor window this time, open the file named Dockerfile Copy the contents of the window below and paste it into your window. # Full contents of Dockerfile FROM continuumio/miniconda3 LABEL description=\"Base docker image with conda and util libraries\" ARG ENV_NAME=\"fastqc\" # Install the conda environment COPY environment.yml / RUN conda env create --quiet --name ${ENV_NAME} --file /environment.yml && conda clean -a # Add conda installation dir to PATH (instead of doing 'conda activate') ENV PATH /opt/conda/envs/${ENV_NAME}/bin:$PATH After pasting in the window, your file should look like this: The docker application understands the commands FROM , LABEL , ARG ENV_NAME , COPY , RUN and ENV PATH . It also knows that # are comments and are for the human reader that is reading this file to understand what is happening within the file itself. The base image we are using for the Dockerfile is from continuumio the maker of our miniconda3 which we installed earlier. It is because of that environment that we do not need to install conda , it is part of the image. We are using the function COPY to bring it into the image. We are adding to this base image our new function fastqc . You will see later that you can COPY your own custom scripts for use within the container. This is the process of containerization . Notice as well we are prompted to look at our quotas, not an infinite resource but pretty good availability for a free asset. Now we will inspect our environment.yml which is being used by conda to install our desired application.","title":"Creating your first Dockerfile"},{"location":"classes/day-3-containerization/building-dockerfiles/#creating-your-environmentyml-file","text":"Notice that this Docker file uses a file called environment.yml . Back in the terminal window again: Type at the prompt touch environment.yml Return to the Google Shell editor and open the file named environment.yml and copy the contents of the window below into that file. What we are doing is using conda to control our environment. And this file is used within our Dockerfile to create the controlled environment and install the function, fastqc . The channels listed are exactly the same channels we would have used if we wanted to make the fastqc application available within our environment to use on the command line. What we are doing is then reproducing this command line environment within our soon to be Docker image. name: fastqc channels: - bioconda - defaults dependencies: - fastqc","title":"Creating your environment.yml File"},{"location":"classes/day-3-containerization/building-dockerfiles/#building-our-dockerfile-image-for-fastqc","text":"Build the Docker image by using the following command: There are three steps with building a Docker image: build tag push Here below we are executing the build command with a tag, indicated by the -t . The . says to build in the current directory. This is the normal way to build your Docker image because as we noted before, the docker application will use all that is within your current directory to make its image. docker build -t fastqc . Here the google shell asks to authorize it to complete the task. When it completes, we can verify that the image has been created listing all available images with the following command: docker images","title":"Building our Dockerfile image for fastqc"},{"location":"classes/day-3-containerization/building-dockerfiles/#test-this-image-from-the-command-line","text":"By now you see what we have done. You could install fastqc using the command conda install -c bioconda fastqc -y We see what is happening is using the Anaconda channel bioconda we are installing fastqc for our local use within our own file system on this small virtual instance made free for us from Google . Let's do that real quick to show what I mean. Go ahead and copy the line above on the command line and install fastqc within our eos environment. As expected it has been installed within the binary directory, the directory that contains executables and shortened to the unix convention to be just bin of our controlled environment eos . This would be fine if we wanted to do local work on some fastqc files. But it would not be available for the workflow that will be spun up on ephemeral machines where we would have to build our entire environment from scratch. But I have made the argument, that what is more sustainable is if we containerize at the process level, our work will be more durable, more portable, and more repurposable. That is it will be available for me to use for future work. So we built a container image. Now how could we use that container image? To test this tool from the command line Set up an environment variable capturing your current command line: PWD=$(pwd) To make our local directory available to the image, which is a self-contained environment, we need to do something called mounting and use your current directory and call the tool now encapsulated within the environment. I use this command to do that. docker run -it -v $PWD:$PWD -w $PWD fastqc fastqc -h You can read more about what is happening in detail with Docker containers and images . You don't need to know every detail to use them.","title":"Test this image from the command line"},{"location":"classes/day-3-containerization/building-dockerfiles/#add-this-code-to-your-github-repository","text":"It is a best practice with GitHub to always add a README.md . Let's add this file and then from the command line use the GitHub command line tools to push this to our GitHub repository. touch README.md Open with the Code Editor (the Markdown Editor is experimental). To do so, hover over the README.md file and click Open With... and select Code Editor . Copy the contents here that explain what we have in this repository # fastqc-docker Build a Container for fastqc from bioconda Steps to build this docker container. 1. Look up on [anaconda](https://anaconda.org/) the tool you wish to install 2. create an `environment.yml` file either manually or automatically 3. Use the template `Dockerfile` modifying if necessary (in our case we have no custom files for the `src` directory so we do not use that) 4. Build the Docker Image 5. Set up GitHub Actions To build your image from the command line: * Can do this on [Google shell](https://shell.cloud.google.com) - docker is installed and available \\```bash docker build -t fastqc . \\``` To test this tool from the command line Set up an environment variable capturing your current command line: \\```bash PWD=$(pwd) \\``` Then mount and use your current directory and call the tool now encapsulated within the environment. \\```bash docker run -it -v $PWD:$PWD -w $PWD fastqc fastqc -h \\``` Now edit the file (I added the \\ so Markdown would ignore the data but it means we need to edit them out for the README.md to properly render on the repo. Please remove all the \\ 's the file will look like this:","title":"Add this code to your GitHub repository"},{"location":"classes/day-3-containerization/building-dockerfiles/#use-github-command-line-tool-gh-to-authenticate-before-we-push-create-our-repository","text":"The Google shell comes with the gh already installed. So we just begin with authentication. gh auth login As we did yesterday, authenticate with your Personal Authentication Token (eos) ad376@cloudshell:~/fastqc-docker$ gh auth login ? What account do you want to log into? GitHub.com ? What is your preferred protocol for Git operations? HTTPS ? Authenticate Git with your GitHub credentials? Yes ? How would you like to authenticate GitHub CLI? [Use arrows to move, type to filter] Login with a web browser > Paste an authentication token Now we can from the command line create this repository. Let's go ahead and tell git who we are. git config --global user.email \"adeslat@scitechcon.org\" git config --global user.name \"adeslatt\" Following GitHub 's updated instructions on how to create a new repository from the command line We now type git init -b main Which will return Initialized empty Git repository in /home/ad376/fastqc-docker/.git/ And then we type git add . && git commit -m \"initial commit\" which returns something like this. [main (root-commit) d1e421f] initial commit 3 files changed, 33 insertions(+) create mode 100644 Dockerfile create mode 100644 README.md create mode 100644 environment.yml We then use the gh repo create command to create the reposistory. gh repo create Which then prompts us to what we need to do -- important is that what we want to do is Push an existing local repository to GitHub . The tool's remainder defaults are acceptable, because we staged ourselves with the name of the directory. (eos) ad376@cloudshell:~/fastqc-docker$ gh repo create ? What would you like to do? Push an existing local repository to GitHub ? Path to local repository . ? Repository name fastqc-docker ? Description a container for the bioconda command fastqc ? Visibility Public \u2713 Created repository adeslatt/fastqc-docker on GitHub ? Add a remote? Yes ? What should the new remote be called? origin \u2713 Added remote https://github.com/adeslatt/fastqc-docker.git ? Would you like to push commits from the current branch to \"origin\"? Yes \u2713 Pushed commits to https://github.com/adeslatt/fastqc-docker.git","title":"Use GitHub Command line tool gh to authenticate before we push create our repository."},{"location":"classes/day-3-containerization/building-dockerfiles/#building-the-multiqc-docker-image","text":"Navigate to the top of your home directory cd ~ Clone the multiqc-docker repository. git clone https://github.com/adeslatt/multiqc-docker.git","title":"Building the multiqc Docker image"},{"location":"classes/day-3-containerization/building-dockerfiles/#creating-your-second-dockerfile","text":"Navigate now to your home directory on the google shell. Remember that .. is the parent directory. cd .. mkdir multiqc-docker Now navigate into that directory, again you can type this or simply copy the lines below in your google shell terminal window. cd multiqc-docker Now we are going to create our first Dockerfile touch is another handy bash command for creating an empty file. touch Dockerfile The docker application requires the exact filename Dockerfile when creating the image. Again in the google shell editor window open the file named Dockerfile Copy the contents of the window below and paste it into your window. # Full contents of Dockerfile FROM continuumio/miniconda3 LABEL description=\"Base docker image with conda and util libraries\" ARG ENV_NAME=\"multiqc\" # Install the conda environment COPY environment.yml / RUN conda env create --quiet --name ${ENV_NAME} --file /environment.yml && conda clean -a # Add conda installation dir to PATH (instead of doing 'conda activate') ENV PATH /opt/conda/envs/${ENV_NAME}/bin:$PATH","title":"Creating Your Second Dockerfile"},{"location":"classes/day-3-containerization/building-dockerfiles/#creating-your-multiqc-environmentyml-file","text":"Back in the terminal window again: Type at the prompt touch environment.yml What we are doing is then reproducing this command line environment within our soon to be Docker image. name: multiqc channels: - bioconda - defaults dependencies: - multiqc And build the image docker build -t multiqc .","title":"Creating your multiqc environment.yml File"},{"location":"classes/day-3-containerization/building-dockerfiles/#inspect-what-images-you-have-now-available-to-you-locally","text":"You can see what you have built -- and see that we have tag ged our files in a certain way docker images","title":"Inspect what images you have now available to you locally"},{"location":"classes/day-3-containerization/building-dockerfiles/#add-this-multiqc-code-to-your-github-repository","text":"It is a best practice with GitHub to always add a README.md . Let's add this file and then from the command line use the GitHub command line tools to push this to our GitHub repository. touch README.md Open with the Code Editor (the Markdown Editor is experimental). To do so, hover over the README.md file and click Open With... and select Code Editor . # multiqc-docker Build a Container for multiqc from bioconda Steps to build this docker container. 1. Look up on [anaconda](https://anaconda.org/) the tool you wish to install 2. create an `environment.yml` file either manually or automatically 3. Use the template `Dockerfile` modifying if necessary (in our case we have no custom files for the `src` directory so we do not use that) 4. Build the Docker Image 5. Set up GitHub Actions To build your image from the command line: * Can do this on [Google shell](https://shell.cloud.google.com) - docker is installed and available \\```bash docker build -t multiqc . \\``` To test this tool from the command line Set up an environment variable capturing your current command line: \\```bash PWD=$(pwd) \\``` Then mount and use your current directory and call the tool now encapsulated within the environment. \\```bash docker run -it -v $PWD:$PWD -w $PWD multiqc multiqc -h \\``` Now edit the file (I added the \\ so Markdown would ignore the data but it means we need to edit them out for the README.md to properly render on the repo. Please remove all the \\ 's the file will look like this: After pasting in the window, your file should look like this: We now type git init -b main Which will return Initialized empty Git repository in /home/ad376/fastqc-docker/.git/ And then we type git add . && git commit -m \"initial commit\" which returns something like this. [main (root-commit) d1e421f] initial commit 3 files changed, 33 insertions(+) create mode 100644 Dockerfile create mode 100644 README.md create mode 100644 environment.yml We then use the gh repo create command to create the reposistory. gh repo create Which then prompts us to what we need to do -- important is that what we want to do is Push an existing local repository to GitHub . The tool's remainder defaults are acceptable, because we staged ourselves with the name of the directory. (eos) ad376@cloudshell:~/multiqc-docker$ gh repo create ? What would you like to do? Push an existing local repository to GitHub ? Path to local repository . ? Repository name multiqc-docker ? Description a container for the bioconda command multiqc ? Visibility Public \u2713 Created repository adeslatt/fastqc-docker on GitHub ? Add a remote? Yes ? What should the new remote be called? origin \u2713 Added remote https://github.com/adeslatt/multiqc-docker.git ? Would you like to push commits from the current branch to \"origin\"? Yes \u2713 Pushed commits to https://github.com/adeslatt/multiqc-docker.git Upon success your repository will be in place on your GitHub.","title":"Add this multiqc code to your GitHub repository"},{"location":"classes/day-3-containerization/building-dockerfiles/#recap","text":"We have created a Dockerfile using the Google Shell Editor We have created the environment.yml file also with the Google Shell Editor We have used the new command placed our desired function fastqc within the environment.yml file for the ultimate Docker container. We built the container for the application fastqc in the fastqc-docker subdirectory. We used GitHub command line tool gh to take our local repositories and put both up on GitHub. We inspected our images. Tomorrow, we will make our first workflow and show how to use these containers within our private repository on CAVATICA Return to the Agenda","title":"Recap"},{"location":"classes/day-3-containerization/creating-a-conda-environment/","text":"Creating a Conda Environment What is conda Conda is an application that we use to control our environment and manage our package installations. We will work today within a terminal window and the first thing I like to do is to make a clean environment window. This ensures that all of the assumptions we made about our environment are clear and explicit. Without this you may inadvertantly introduce dependicies that may complicate your building of your container and/or stitching together your workflow. When we build our container, we begin many of times using the conda environment made by the group that builds conda as a basis. This then does give us the application conda within our environment for our use like docker. Today we will be using a terminal window within a free resource, Google Shell We will walk through those steps today with our simple example. Anaconda Search Packages I regularly type this in my browser window to find the packages I need to install. Anaconda environment is growing by leaps and bounds and there are more things that can be done there that I have yet to explore. In addition to well vetted packages that can be installed, this large community resource provides tutorials, data science instructions as well as whole environments where you can work, explore and learn. For example, today we already showed you how to run a jupyterlab notebook, but you want to know more. Their site offers instruction on Jupyterlab basics for you to explore. Logging into Google Shell For this part of our demo/hands on, we will be using google shell. Navigate please to Google Shell It will ask you to login, please follow the prompts and do so. Upon success you will see a window like this: We have an editor as well as a prompt. If you have items in your Google Shell, you may need to clean up a bit. There is not much space in this free ephemeral instance . Cleaning up your space If you have items in your directory, lets clean up! So we have enough room for the next few lessons. Let's see what is there. For a shell to list the contents of a directory, you type: ls -l You can use Explain Shell to explain the meaning of the command. Explain Shell ls -l . You can see hidden directories with the following command. ls -la Again, look at Explain Shell to explain the meaning of the command. Explain Shell ls -la Installing conda We need to get the installation package for conda. We get this package from continuum analytics, we will be installing Miniconda . Miniconda is the minimal installer and all we need. Navigating to that location you will see details here: Which installation do we need? Typing uname -a will tell us the information about our operating system and hardware basics. uname -a We see that we are using the Linux operating system, the first day I spun up this shell was on March 16 and we see that it has the architecture x86_64 . A handy 64-bit operating system. Scroll down the window and hover your cursor over the Miniconda Linux 64-bit (sorry it is implicitly that this is the right installation for the x86-64 architecture. Right click and select copy link address . This gives you the unique resource locator (url) for the installation script we will be downloading onto the machine we are talking to with our google cloud shell window. Now we will use the routine wget to copy this installation script. To make sure we can do this, let's make sure that we have the tool available. To do that, we type: which wget In my case, it returns the full path /usr/bin/wget . If you do not have wget , we can install it with Anaconda provided packages. Now to install Miniconda, so that we can use conda to install any and all packages we need we do as follows: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh If you haven't changed directories, you can see now that this package Miniconda3-latest-Linux-x86_64.sh is now in the top directory. As we learned earlier, we can use the bash shell commnad ls to inspect the contents of our current directory. ls -l provides the full path details of our directory. We check our current working directory with the bash shell comand pwd as we learned earlier. pwd For my execution, it returns /home/ad376 . Now running the command ls -l we see the downloaded file Miniconda3-latest-Linux-x86-64.sh . ls -l returns: /home/ad376 ad376@cloudshell:~$ ls -l total 73892 -rw-r--r-- 1 ad376 ad376 75660608 Feb 15 19:07 Miniconda3-latest-Linux-x86_64.sh -rw-r--r-- 1 ad376 ad376 913 Mar 31 12:34 README-cloudshell.txt ad376@cloudshell:~$ To execute this script now and install the conda application, we type: bash Miniconda3-latest-Linux-x86_64.sh This will return: Welcome to Miniconda3 py39_4.11.0 In order to continue the installation process, please review the license agreement. Please, press ENTER to continue >>> press ENTER to continue and accept the terms and defaults as instructed. I used SPACE to accelerate the passing of the license - when you have time you should read the details of this license as it is interesting. The last question is: Do you accept the license terms? [yes|no] [no] >>> yes It returns now: Miniconda3 will now be installed into this location: /home/ad376/miniconda3 - Press ENTER to confirm the location - Press CTRL-C to abort the installation - Or specify a different location below [/home/ad376/miniconda3] >>> Accept the default location to install. Next you will be asked Do you wish the installer to initialize Miniconda3 by running conda init? [yes|no] [no] >>> yes Please answer yes to this question. When finished we can see that the file is now in our execution path is the conda application. While conda now has been installed, to activate the use of conda we need to reboot our shell environment. The command exec -l bash will execute this restart of the bash shell. exec -l bash Now we can see the result: ad376@cloudshell:~$ exec -l bash Welcome to Cloud Shell! Type \"help\" to get started. To set your Cloud Platform project in this session use \u201cgcloud config set project [PROJECT_ID]\u201d (base) ad376@cloudshell:~$ And we can see now we have the conda application. which conda returns: /home/ad376/miniconda3/condabin/conda Creating a clean environment Now that we have conda we can create an environment that will keep our workspace clean. We can name it whatever we would like, since this is the Elements of Style course, I choose to name my environment eos for short. We create an environment as follows: conda create -n eos -y You can see now that we now have the option to activate the environment # # To activate this environment, use # # $ conda activate eos # # To deactivate an active environment, use # # $ conda deactivate Now that we have created a clean environment, we follow the directions and activate the environment, that is enter this new clean environment. conda activate eos We will see next to our name in the google shell, that it should look something like this: (base) ad376@cloudshell:~$ conda activate eos (eos) ad376@cloudshell:~$ The (eos) indicating that we are in that environment. You can list all your environments list all your environments with the following command: conda env list Recap We have learned in this lesson We started a bash shell environment using our browser and the shell We explored the open community-based resources offered through Anaconda We used wget available within the shell and got the installation package necessary to install conda We installed conda and We used numerous command line arguments We used conda to create a new clean environment. We now move on to build Docker images which we will use in a workflow. Return to the Agenda","title":"Creating a `Conda` Environment"},{"location":"classes/day-3-containerization/creating-a-conda-environment/#creating-a-conda-environment","text":"","title":"Creating a Conda Environment"},{"location":"classes/day-3-containerization/creating-a-conda-environment/#what-is-conda","text":"Conda is an application that we use to control our environment and manage our package installations. We will work today within a terminal window and the first thing I like to do is to make a clean environment window. This ensures that all of the assumptions we made about our environment are clear and explicit. Without this you may inadvertantly introduce dependicies that may complicate your building of your container and/or stitching together your workflow. When we build our container, we begin many of times using the conda environment made by the group that builds conda as a basis. This then does give us the application conda within our environment for our use like docker. Today we will be using a terminal window within a free resource, Google Shell We will walk through those steps today with our simple example.","title":"What is conda"},{"location":"classes/day-3-containerization/creating-a-conda-environment/#anaconda-search-packages","text":"I regularly type this in my browser window to find the packages I need to install. Anaconda environment is growing by leaps and bounds and there are more things that can be done there that I have yet to explore. In addition to well vetted packages that can be installed, this large community resource provides tutorials, data science instructions as well as whole environments where you can work, explore and learn. For example, today we already showed you how to run a jupyterlab notebook, but you want to know more. Their site offers instruction on Jupyterlab basics for you to explore.","title":"Anaconda Search Packages"},{"location":"classes/day-3-containerization/creating-a-conda-environment/#logging-into-google-shell","text":"For this part of our demo/hands on, we will be using google shell. Navigate please to Google Shell It will ask you to login, please follow the prompts and do so. Upon success you will see a window like this: We have an editor as well as a prompt. If you have items in your Google Shell, you may need to clean up a bit. There is not much space in this free ephemeral instance .","title":"Logging into Google Shell"},{"location":"classes/day-3-containerization/creating-a-conda-environment/#cleaning-up-your-space","text":"If you have items in your directory, lets clean up! So we have enough room for the next few lessons. Let's see what is there. For a shell to list the contents of a directory, you type: ls -l You can use Explain Shell to explain the meaning of the command. Explain Shell ls -l . You can see hidden directories with the following command. ls -la Again, look at Explain Shell to explain the meaning of the command. Explain Shell ls -la","title":"Cleaning up your space"},{"location":"classes/day-3-containerization/creating-a-conda-environment/#installing-conda","text":"We need to get the installation package for conda. We get this package from continuum analytics, we will be installing Miniconda . Miniconda is the minimal installer and all we need. Navigating to that location you will see details here: Which installation do we need? Typing uname -a will tell us the information about our operating system and hardware basics. uname -a We see that we are using the Linux operating system, the first day I spun up this shell was on March 16 and we see that it has the architecture x86_64 . A handy 64-bit operating system. Scroll down the window and hover your cursor over the Miniconda Linux 64-bit (sorry it is implicitly that this is the right installation for the x86-64 architecture. Right click and select copy link address . This gives you the unique resource locator (url) for the installation script we will be downloading onto the machine we are talking to with our google cloud shell window. Now we will use the routine wget to copy this installation script. To make sure we can do this, let's make sure that we have the tool available. To do that, we type: which wget In my case, it returns the full path /usr/bin/wget . If you do not have wget , we can install it with Anaconda provided packages. Now to install Miniconda, so that we can use conda to install any and all packages we need we do as follows: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh If you haven't changed directories, you can see now that this package Miniconda3-latest-Linux-x86_64.sh is now in the top directory. As we learned earlier, we can use the bash shell commnad ls to inspect the contents of our current directory. ls -l provides the full path details of our directory. We check our current working directory with the bash shell comand pwd as we learned earlier. pwd For my execution, it returns /home/ad376 . Now running the command ls -l we see the downloaded file Miniconda3-latest-Linux-x86-64.sh . ls -l returns: /home/ad376 ad376@cloudshell:~$ ls -l total 73892 -rw-r--r-- 1 ad376 ad376 75660608 Feb 15 19:07 Miniconda3-latest-Linux-x86_64.sh -rw-r--r-- 1 ad376 ad376 913 Mar 31 12:34 README-cloudshell.txt ad376@cloudshell:~$ To execute this script now and install the conda application, we type: bash Miniconda3-latest-Linux-x86_64.sh This will return: Welcome to Miniconda3 py39_4.11.0 In order to continue the installation process, please review the license agreement. Please, press ENTER to continue >>> press ENTER to continue and accept the terms and defaults as instructed. I used SPACE to accelerate the passing of the license - when you have time you should read the details of this license as it is interesting. The last question is: Do you accept the license terms? [yes|no] [no] >>> yes It returns now: Miniconda3 will now be installed into this location: /home/ad376/miniconda3 - Press ENTER to confirm the location - Press CTRL-C to abort the installation - Or specify a different location below [/home/ad376/miniconda3] >>> Accept the default location to install. Next you will be asked Do you wish the installer to initialize Miniconda3 by running conda init? [yes|no] [no] >>> yes Please answer yes to this question. When finished we can see that the file is now in our execution path is the conda application. While conda now has been installed, to activate the use of conda we need to reboot our shell environment. The command exec -l bash will execute this restart of the bash shell. exec -l bash Now we can see the result: ad376@cloudshell:~$ exec -l bash Welcome to Cloud Shell! Type \"help\" to get started. To set your Cloud Platform project in this session use \u201cgcloud config set project [PROJECT_ID]\u201d (base) ad376@cloudshell:~$ And we can see now we have the conda application. which conda returns: /home/ad376/miniconda3/condabin/conda","title":"Installing conda"},{"location":"classes/day-3-containerization/creating-a-conda-environment/#creating-a-clean-environment","text":"Now that we have conda we can create an environment that will keep our workspace clean. We can name it whatever we would like, since this is the Elements of Style course, I choose to name my environment eos for short. We create an environment as follows: conda create -n eos -y You can see now that we now have the option to activate the environment # # To activate this environment, use # # $ conda activate eos # # To deactivate an active environment, use # # $ conda deactivate Now that we have created a clean environment, we follow the directions and activate the environment, that is enter this new clean environment. conda activate eos We will see next to our name in the google shell, that it should look something like this: (base) ad376@cloudshell:~$ conda activate eos (eos) ad376@cloudshell:~$ The (eos) indicating that we are in that environment. You can list all your environments list all your environments with the following command: conda env list","title":"Creating a clean environment"},{"location":"classes/day-3-containerization/creating-a-conda-environment/#recap","text":"We have learned in this lesson We started a bash shell environment using our browser and the shell We explored the open community-based resources offered through Anaconda We used wget available within the shell and got the installation package necessary to install conda We installed conda and We used numerous command line arguments We used conda to create a new clean environment. We now move on to build Docker images which we will use in a workflow.","title":"Recap"},{"location":"classes/day-3-containerization/creating-a-conda-environment/#return-to-the-agenda","text":"","title":"Return to the Agenda"},{"location":"classes/day-3-containerization/day-3-containerization/","text":"Agenda for the Day 3: Containerization with Environment Control Time (UTC) Programme 11.00 - 11.30 Recap on the Volcano Plot and agenda for the day 11:30 - 12:00 Creating a conda environment 12.00 - 12.10 Short break 12:10 - 12:50 Building Dockerfiles 12.50 - 13:00 Wrap up and overview of next days topics Additional Resources Anaconda Gallery Anaconda Open Data Science Anaconda Open Source Anaconda Packages Search Conda Docker File Documentation Google Shell Who is Anaconda","title":"Day 3 - Containerization"},{"location":"classes/day-3-containerization/day-3-containerization/#agenda-for-the-day-3-containerization-with-environment-control","text":"Time (UTC) Programme 11.00 - 11.30 Recap on the Volcano Plot and agenda for the day 11:30 - 12:00 Creating a conda environment 12.00 - 12.10 Short break 12:10 - 12:50 Building Dockerfiles 12.50 - 13:00 Wrap up and overview of next days topics","title":"Agenda for the Day 3: Containerization with Environment Control"},{"location":"classes/day-3-containerization/day-3-containerization/#additional-resources","text":"Anaconda Gallery Anaconda Open Data Science Anaconda Open Source Anaconda Packages Search Conda Docker File Documentation Google Shell Who is Anaconda","title":"Additional Resources"},{"location":"classes/day-3-containerization/recap-workspace-setup/","text":"Let's Recap Day 1 Introduced The Elements of Style approach On-boarded to the Cavatica platform and started Data Studio which is a JupyterLab Notebook Inspected the exampleVolcanoPlot.ipynb which is a JupyterLab Notebook running an R Kernal Day 2 Introduced the concepts of Git , GitHub Learned about version control , attributions , and how GitHub enables a Lab Notebook concept, preparing us to better use the vast and amazing capabilities available to us in the managed platform environment, Cavatica with access to enormous power and storage via Elastic Compute and the growing resources available on the cloud. We Forked a repository We Synced a repository We made a pull request . Let's revisit Reading Data and Plotting in R There were two missing steps to successfully run the exampleVolcanoPlot . The Reading-data-and-plotting-in-R.ipynb , a JupyterLab Notebook running an R kernal needed three items: the data found in Zenodo at the DOI (Data Object Identifier) location https://doi.org/10.5281/zenodo.7510587 . to have run the command to install required dependencies on the machine where the notebook was running *rscript install_dependencies.R* . and finally to declare one of the libraries installed that was not yet declared: *library(rlang)* . Those changes were updated in the original repository https://github.com/adeslatt/exampleVolcanoPlotNotebook . And if you have synchronized with the repository you Forked your repository from https://github.com/NIH-NICHD/exampleVolcanoPlotNotebook , your repository is now up-to-date. Let's Read Data and Plot in R ! Go ahead and hit and lets get started. Give us a in the Zoom chat if all is going ok. You will soon see the JupyterLab Launch pad - which as we know will stay alive for about 30 minutes. Navigate to the right directory From the command line , navigate to the location where you git clone 'd your personal fork of the NIH-NICHD repository. cd adeslatt Now check the status of the repository with the command git status git status Uh-oh there is a problem! That's ok we were not quite in the right directory. You can look around where you are by using a command we learned yesterday ls -l ls -l Let's go down one more level to where the stuff we checked out of the repository actually is: cd exampleVolcanoPlotNotebook To check the status and to check the difference between what is in your directory and what is stored in your repository you use two commands git status and git diff git status followed by git diff Now in my case the repository had its own changes, but when I looked at the repository on the web site, they were identical. So I have decided I simply want to remove this directory with this work. I know it seems scary to do that, but with my changes in GitHub I have nothing to worry about. We are going to use another new command cd .. . In a terminal window there are shortcuts. Programmers like to use as few characters as possible to accomplish their work, including moving around in a terminal window. Over time you will also not only grow accustomed to these handy short cuts, but you will enjoy them as well. To look at your current directory you can use . . To look at the parent directory (what we call the directory above the one we are in, you can use .. . Go ahead and look around your current directory, by typing the unnecessary command cd . but I am using it to demonstrate a point (ha-ha). cd . ls -l But what we wanted to do is move up to the parent directoery. cd .. ls -l Ok now I want to remove the entire directory. Because I had local and changes at the master and really did not need to or desire to worry about how to clean it up. Easiest is to wipe the slate clean and get a fresh copy. We will use rm together with the options -f for force and -R for recursively delete all the subdirectories inside as well. You can go into ExplainShell to see this information for yourself. I also will get my clean repository by going to the code button and getting a copy by copying with the right protocol, * HTTPS* the link for git to use rm -f -R git clone https://github.com/adeslatt/exampleVolcanoPlotNotebook.git Now again open the notebook. Clear the outputs and restart the Kernel. Confirm And re-run! Now let's go back to our course. Return to the Agenda","title":"Recap workspace setup"},{"location":"classes/day-3-containerization/recap-workspace-setup/#lets-recap","text":"Day 1 Introduced The Elements of Style approach On-boarded to the Cavatica platform and started Data Studio which is a JupyterLab Notebook Inspected the exampleVolcanoPlot.ipynb which is a JupyterLab Notebook running an R Kernal Day 2 Introduced the concepts of Git , GitHub Learned about version control , attributions , and how GitHub enables a Lab Notebook concept, preparing us to better use the vast and amazing capabilities available to us in the managed platform environment, Cavatica with access to enormous power and storage via Elastic Compute and the growing resources available on the cloud. We Forked a repository We Synced a repository We made a pull request .","title":"Let's Recap"},{"location":"classes/day-3-containerization/recap-workspace-setup/#lets-revisit-reading-data-and-plotting-in-r","text":"There were two missing steps to successfully run the exampleVolcanoPlot . The Reading-data-and-plotting-in-R.ipynb , a JupyterLab Notebook running an R kernal needed three items: the data found in Zenodo at the DOI (Data Object Identifier) location https://doi.org/10.5281/zenodo.7510587 . to have run the command to install required dependencies on the machine where the notebook was running *rscript install_dependencies.R* . and finally to declare one of the libraries installed that was not yet declared: *library(rlang)* . Those changes were updated in the original repository https://github.com/adeslatt/exampleVolcanoPlotNotebook . And if you have synchronized with the repository you Forked your repository from https://github.com/NIH-NICHD/exampleVolcanoPlotNotebook , your repository is now up-to-date.","title":"Let's revisit Reading Data and Plotting in R"},{"location":"classes/day-3-containerization/recap-workspace-setup/#lets-read-data-and-plot-in-r","text":"Go ahead and hit and lets get started. Give us a in the Zoom chat if all is going ok. You will soon see the JupyterLab Launch pad - which as we know will stay alive for about 30 minutes.","title":"Let's Read Data and Plot in R !"},{"location":"classes/day-3-containerization/recap-workspace-setup/#navigate-to-the-right-directory","text":"From the command line , navigate to the location where you git clone 'd your personal fork of the NIH-NICHD repository. cd adeslatt Now check the status of the repository with the command git status git status Uh-oh there is a problem! That's ok we were not quite in the right directory. You can look around where you are by using a command we learned yesterday ls -l ls -l Let's go down one more level to where the stuff we checked out of the repository actually is: cd exampleVolcanoPlotNotebook To check the status and to check the difference between what is in your directory and what is stored in your repository you use two commands git status and git diff git status followed by git diff Now in my case the repository had its own changes, but when I looked at the repository on the web site, they were identical. So I have decided I simply want to remove this directory with this work. I know it seems scary to do that, but with my changes in GitHub I have nothing to worry about. We are going to use another new command cd .. . In a terminal window there are shortcuts. Programmers like to use as few characters as possible to accomplish their work, including moving around in a terminal window. Over time you will also not only grow accustomed to these handy short cuts, but you will enjoy them as well. To look at your current directory you can use . . To look at the parent directory (what we call the directory above the one we are in, you can use .. . Go ahead and look around your current directory, by typing the unnecessary command cd . but I am using it to demonstrate a point (ha-ha). cd . ls -l But what we wanted to do is move up to the parent directoery. cd .. ls -l Ok now I want to remove the entire directory. Because I had local and changes at the master and really did not need to or desire to worry about how to clean it up. Easiest is to wipe the slate clean and get a fresh copy. We will use rm together with the options -f for force and -R for recursively delete all the subdirectories inside as well. You can go into ExplainShell to see this information for yourself. I also will get my clean repository by going to the code button and getting a copy by copying with the right protocol, * HTTPS* the link for git to use rm -f -R git clone https://github.com/adeslatt/exampleVolcanoPlotNotebook.git Now again open the notebook. Clear the outputs and restart the Kernel. Confirm And re-run! Now let's go back to our course. Return to the Agenda","title":"Navigate to the right directory"},{"location":"classes/day-4-workflow-development/NextflowCommonWorkFlowLanguageSharedStructureSharedElements/","text":"Common Workflow Language and Nextflow Shared Elements What are we in the end doing here? We are trying to make our science as platform independent, workflow independent as possible while enabling open science. I have made the case (I hope) that by practing a few \"Elements of Style\" the researcher can come a long way to help achieve the lofty goals of reproducibility. Though seemingly ideal, they are within reach. By using containers at the level of logical single purpose. We can use the same containers across workflow languages. This achieves a fan in engineering organizational structure that facilitates not only reproducible science, but also cleaner more testable components that lend themselves to assembly into other workflows and functions. It readies the work for the next best component that can be stitched together in whatever workflow language supported by the platform that may be holding the data repository needed to be used to accomplish the specific scientific goal the researcher has in mind. Common Workflow Language The Common Workflow language has the same structure as nextflow: Inputs Outputs Scripts Containers Return to the Agenda Return to the Agenda","title":"NextflowCommonWorkFlowLanguageSharedStructureSharedElements"},{"location":"classes/day-4-workflow-development/NextflowCommonWorkFlowLanguageSharedStructureSharedElements/#common-workflow-language-and-nextflow-shared-elements","text":"What are we in the end doing here? We are trying to make our science as platform independent, workflow independent as possible while enabling open science. I have made the case (I hope) that by practing a few \"Elements of Style\" the researcher can come a long way to help achieve the lofty goals of reproducibility. Though seemingly ideal, they are within reach. By using containers at the level of logical single purpose. We can use the same containers across workflow languages. This achieves a fan in engineering organizational structure that facilitates not only reproducible science, but also cleaner more testable components that lend themselves to assembly into other workflows and functions. It readies the work for the next best component that can be stitched together in whatever workflow language supported by the platform that may be holding the data repository needed to be used to accomplish the specific scientific goal the researcher has in mind.","title":"Common Workflow Language and Nextflow Shared Elements"},{"location":"classes/day-4-workflow-development/NextflowCommonWorkFlowLanguageSharedStructureSharedElements/#common-workflow-language","text":"The Common Workflow language has the same structure as nextflow: Inputs Outputs Scripts","title":"Common Workflow Language"},{"location":"classes/day-4-workflow-development/NextflowCommonWorkFlowLanguageSharedStructureSharedElements/#containers","text":"","title":"Containers"},{"location":"classes/day-4-workflow-development/NextflowCommonWorkFlowLanguageSharedStructureSharedElements/#return-to-the-agenda","text":"Return to the Agenda","title":"Return to the Agenda"},{"location":"classes/day-4-workflow-development/authenticating-on-cavatica/","text":"Authenticating with CAVATICA You can authenticate in two ways, you can use a credential file that the CAVATICA application recognizes or you can do command line login. Both methods use the CAVATICA 's developer's authentication token. If you navigate to CAVATICA to your project , for me it was the elements-of-style project. Then navigate to the CAVATICA Docker registry under the Developer tab. Selecting the one of the containers of interest, fastqc , you can see more detail with the container: Further, you can see, that CAVATICA has conveniently placed the necessary commands to facilitate login , push and pull . Authenticate with command line login Let's copy the login command and paste it in our google shell window . docker login pgc-images.sbgenomics.com -u <USERNAME> -p <YOUR-AUTH-TOKEN> Replace the <USERNAME> with your username and the <YOUR-AUTH-TOKEN> with your authentication token . Authenticate with a CAVATICA credential file. Alternatively you create your credentials in a credential file. To do this at the root directory, you can type the command push ~ to navigate to the root directory. A fun pair of commands for navigation in a shell are pushd and popd . Wikipedia actually has a nice page to explain these two commands for faster navigation . Think of pushing your commands on a stack and popping them off to return where you are. Let's pushd to our root directory pushd ~/ What we have done is stored in memory where we started from and our next destination. We can see this by asking what we have on this kind of stack of directories with another command: dirs What this shows is: (eos) dirs ~ ~/Building-A-CWL-Script This returns our Home directory where we just navigated to and where we came from ~/adeslatt/Building-A-CWL-script . Home or the root directory (Home is the name used for the top of the directory or root because the directory structure is like a tree. And Home looks like ~ . Now make a directory for CAVATICA to find your credentials mkdir .sevenbridges You are creating a subdirectory that is a hidden directory by typing mkdir .sevenbridges , which you can now navigate to. cd .sevenbridges you can use touch to create your credentials file. [cavatica] api_endpoint = https://cavatica-api.sbgenomics.com/v2 auth_token = <paste your AUTHENTICATION TOKEN here> Now to get back to where we started we type popd Return to Agenda","title":"Authenticating on cavatica"},{"location":"classes/day-4-workflow-development/authenticating-on-cavatica/#authenticating-with-cavatica","text":"You can authenticate in two ways, you can use a credential file that the CAVATICA application recognizes or you can do command line login. Both methods use the CAVATICA 's developer's authentication token. If you navigate to CAVATICA to your project , for me it was the elements-of-style project. Then navigate to the CAVATICA Docker registry under the Developer tab. Selecting the one of the containers of interest, fastqc , you can see more detail with the container: Further, you can see, that CAVATICA has conveniently placed the necessary commands to facilitate login , push and pull .","title":"Authenticating with CAVATICA"},{"location":"classes/day-4-workflow-development/authenticating-on-cavatica/#authenticate-with-command-line-login","text":"Let's copy the login command and paste it in our google shell window . docker login pgc-images.sbgenomics.com -u <USERNAME> -p <YOUR-AUTH-TOKEN> Replace the <USERNAME> with your username and the <YOUR-AUTH-TOKEN> with your authentication token .","title":"Authenticate with command line login"},{"location":"classes/day-4-workflow-development/authenticating-on-cavatica/#authenticate-with-a-cavatica-credential-file","text":"Alternatively you create your credentials in a credential file. To do this at the root directory, you can type the command push ~ to navigate to the root directory. A fun pair of commands for navigation in a shell are pushd and popd . Wikipedia actually has a nice page to explain these two commands for faster navigation . Think of pushing your commands on a stack and popping them off to return where you are. Let's pushd to our root directory pushd ~/ What we have done is stored in memory where we started from and our next destination. We can see this by asking what we have on this kind of stack of directories with another command: dirs What this shows is: (eos) dirs ~ ~/Building-A-CWL-Script This returns our Home directory where we just navigated to and where we came from ~/adeslatt/Building-A-CWL-script . Home or the root directory (Home is the name used for the top of the directory or root because the directory structure is like a tree. And Home looks like ~ . Now make a directory for CAVATICA to find your credentials mkdir .sevenbridges You are creating a subdirectory that is a hidden directory by typing mkdir .sevenbridges , which you can now navigate to. cd .sevenbridges you can use touch to create your credentials file. [cavatica] api_endpoint = https://cavatica-api.sbgenomics.com/v2 auth_token = <paste your AUTHENTICATION TOKEN here> Now to get back to where we started we type popd Return to Agenda","title":"Authenticate with a CAVATICA credential file."},{"location":"classes/day-4-workflow-development/building-a-cwl-workflow/","text":"Building A Common Workflow Language (CWL) Workflow Here we now show how the same containers may be used in a CWL workflow. If you are starting here and haven't Forked this repository into your own * GitHub Repository, please Fork this repository If you have your personal GitHub Fork of this repository, then be sure to as changes may have been made. Now please git clone your personal fork of the repository for this lesson. git clone https://github.com/NIH-NICHD/Building-A-CWL-Script.git Showing the CWL workflow with the same containers Now Navigate to this class cd Building-A-CWL-Script Create and activate your environment, if you have not already done so. Create the eos environment. conda create -n eos -y Activate the eos environment. conda activate eos Looking at the directory we type the command: ls -l And now we see something like this total 28 drwxr-xr-x 2 adeslat adeslat 4096 Aug 30 18:57 cwl_tools -rw-r--r-- 1 adeslat adeslat 1234 Aug 30 18:57 fastqc_multiqc_wf.cwl -rw-r--r-- 1 adeslat adeslat 18656 Aug 30 18:57 README.md By convention, the bioinformaticians at Childrens Hospital of Philadelphia put their tools, their pieces for a workflow in a subdirectory they name cwl_tools . This is a good convention. We can see what is put in this directory by typing: ls -l cwl_tools We see that there are two files: -rw-r--r-- 1 ad376 ad376 1053 Mar 31 13:28 fastqc.cwl -rw-r--r-- 1 ad376 ad376 1004 Mar 31 13:28 multiqc.cwl We can inspect that file either by opening it in an editor or by typing at the terminal less cwl_tools/fastqc_cwl You can learn what the command less does by typing in the Explain shell less . Which just tells you it is the opposite of more , so lets type in the Explain shell more . It just allows us to inspect the contents of the file. To exit out of our less command press q To make the terminal window a bit larger, press the ... three triple . and pull the window larger. Type less cwl_tools/fastqc_cwl again, and pressing space bar Which we see now as: cwlVersion: v1.0 class: CommandLineTool id: fastqc requirements: - class: ShellCommandRequirement - class: DockerRequirement dockerPull: 'pgc-images.sbgenomics.com/deslattesmaysa2/fastqc:v1.0' - class: InlineJavascriptRequirement - class: ResourceRequirement ramMin: ${ return inputs.ram * 1024 } coresMin: $(inputs.cores) baseCommand: [mkdir] arguments: - position: 1 shellQuote: false valueFrom: >- $(inputs.outdir) - position: 2 shellQuote: false valueFrom: >- && fastqc inputs: input_reads: { type: 'File[]', inputBinding: {position: 99}, doc: \"Input fastq files\" } outdir: { type: 'string?', default: \"results\", inputBinding: { position: 2, prefix: \"--outdir\"} } noextract: { type: 'boolean?', default: true, inputBinding: { position: 2, prefix: \"--noextract\"} } cores: { type: 'int?', default: 2 , inputBinding: { position: 2, prefix: \"--threads\" } } ram: { type: 'int?', default: 2 } outputs: fastqc_results: type: Directory outputBinding: glob: $(inputs.outdir) To exit from the less command press q We see many of the same pieces that we saw with a Nextflow script. We see inputs: and we see outputs . We also see that there is a dockerPull: * command. dockerPull: 'pgc-images.sbgenomics.com/deslattesmaysa2/fastqc:v1.0' Which references the identical container that was used with Nextflow. Before we can pull from the docker image stored on CAVATICA, we have to authenticate. With the inputs: we see that we have input_reads: which asks us to input our reads, there is also a specification for an outdir . While I will not go into much more detail, I will emphasize that the cwl script is using the same containers as we did in the nextflow script. We can also inspect the multiqc.cwl file in that way or in the editor. Installing cwltool In the same way that we installed nextflow , we can install cwltool . To find the exact command, I typically google anaconda search packages and arrive here: Next, I get the information on how to install: To run the pipeline: conda install -c conda-forge cwltool -y We verify the installation with which cwltool Should tell us it is in the eos environment binary or executable subdirectory, bin , in my case that is here /home/adeslat/miniconda3/envs/eos/bin/cwltool Executing with cwltool Before we can execute, we need to let the tool know where the files are -- they are in fact on Zenodo wget https://zenodo.org/record/7025773/files/test.20k_reads_1.fastq.gz wget https://zenodo.org/record/7025773/files/test.20k_reads_2.fastq.gz And now we can execute, we do not need to tell the tool where the containers are because they are specified in the script. As before we can run the tools separately, before running them together. First we test out fastqc cwltool cwl_tools/fastqc.cwl --input_reads test.20k_reads_1.fastq.gz --input_reads test.20k_reads_2.fastq.gz As we watch, what is happening is the first thing is the docker image is pulled locally. If successful, the final line will read INFO Final process status is success We test out multiqc which uses the output of fastqc as its input. In this way, we are following the Elements of Style rules of testing in parts, before testing both together. We are testing the two different cwltool scripts independently. Let's proceed. Note that for executing the multiqc script, we are not providing as input the two fastq.gz files as we did to the fastqc.cwl script, but rather the output of this previous step is the input to multiqc , so we provide the results . Before proceeding, lets inspect the results subdirectory. Typing the command line command ls -l which lists in long format as we learned from Explain Shell ls -l We see that yes, the command was successful and we have the same output we saw from the output of the Nextflow script. -rw-r--r-- 1 adeslat adeslat 236403 Aug 30 16:46 test.20k_reads_1_fastqc.html -rw-r--r-- 1 adeslat adeslat 243006 Aug 30 16:46 test.20k_reads_1_fastqc.zip -rw-r--r-- 1 adeslat adeslat 246410 Aug 30 16:46 test.20k_reads_2_fastqc.html -rw-r--r-- 1 adeslat adeslat 263730 Aug 30 16:46 test.20k_reads_2_fastqc.zip You can look at the html formatted file in the Google shell editor window by right-clicking and selecting open with preview Now lets run multiqc cwltool cwl_tools/multiqc.cwl --fastqc_results results If we look in the directory we now see the output from the multiqc run. We can inspect the output file in the same manner we inspected the fastqc output. Navigate to the appropriate directory in the Cloud Editor window, right click on the file report.multiqc.html and select preview and review the output, which is the aggregated report of the quality of both of the fastq files. recap We saw how to: install cwltool run and test fastqc and multiqc separately using the CWL language but calling the same exact docker image as we did with the Nextflow language. Stitching together the two processes into a single workflow CWL does not use channels, but can read the output files from the first process as input to the second process, thereby permitting the stitching together of processes into a single workflow. When we go to the CAVATICA platform, we will see how we can do this within the visual editor. These steps help us to test the processes separately before putting them together into a single workflow, adhering to our Elements of Style approach! Below is the full cwl script for a workflow with the two steps fastqc followed by multiqc . The cwl file is in the directory and is called fastqc_multiqc_wf.cwl . Naming it with the _wf just helps us as humans to see that it is a workflow . Inspect the contents of this file, either in the editor or with the less command. cwlVersion: v1.0 class: Workflow id: fastqc_mutliqc_wf requirements: - class: MultipleInputFeatureRequirement - class: InlineJavascriptRequirement inputs: input_reads: { type: 'File[]', doc: \"Input fastq files\" } fastqc_outdir: { type: 'string?', default: \"results\" } noextract: { type: 'boolean?', default: true, doc: \"FastQC flag\" } flat: { type: 'boolean?', default: true, doc: \"MultiQC flag to use only static images\" } filename: { type: 'string?', default: \"report.multiqc\", doc: \"MultiQC output files prefix\" } cores: { type: 'int?', default: 2 } ram: { type: 'int?', default: 2 } outputs: fastqc_results: { type: Directory, outputSource: run_fastqc/fastqc_results } multiqc_zip: { type: File, outputSource: run_multiqc/multiqc_zip } multiqc_html: { type: File, outputSource: run_multiqc/multiqc_html } steps: fastqc: run: cwl_tools/fastqc.cwl in: input_reads: input_reads outdir: fastqc_outdir noextract: noextract cores: cores ram: ram out: [fastqc_results] multiqc: run: cwl_tools/multiqc.cwl in: fastqc_results: [fastqc/fastqc_results] flat: flat filename: filename cores: cores ram: ram out: [multiqc_zip, multiqc_html] The call to execute the combined workflow is then as follows: cwltool fastqc_multiqc_wf.cwl --input_reads test.20k_reads_1.fastq.gz --input_reads test.20k_reads_2.fastq.gz The steps: statement specifies which script to run. It is the exact script we have previously tested and run, cwl_tools/fastqc.cwl followed by cwl_tools/multiqc.cwl . To call this combined workflow, we call it with the input we gave the first process, cwl_tools/fastqc.cwl . We do not need to specify the input to the second process, as it requires nothing more than the output of the first workflow, cwl_tools/fastqc.cwl . But lets walk through some of the important steps and differences. Breaking down the CWL Script. One can see that there are two steps fastqc and multiqc . The steps are easily stated because the tools are further expanded in each of the separate tool definitions. steps: fastqc run: The first statement in this section contains the key word run: after this statement is the redirection to the subdirectory and full name of the already tested tool cwl_tools/fastqc.cwl . in: The next statement in: outlines where the input data files are coming from - and additional details. input_reads: input_reads: input_reads ```` redirects the machine and the reader to the default parameter definitions carried out at the beginning of the file. Going back to the top of the file we find ```bash inputs: input_reads: { type: 'File[]', doc: \"Input fastq files\" } So we see that (and input_reads states the user should specify these files on the command line) which we do in our call. Additionally the type: is File[] , indicates that the input accepts the files as an array ( [] indicates an array) means that the parameter or variable accepts multiple types, so we would provide as input using the --input_reads test.20k_reads_1.fastq.gz --input_reads test.20k_reads_2.fastq.gz outdir: fastqc_outdir the outdir: directive redirects us to fastqc_outdir which again we see defined at the top of the file, just after our input_reads . inputs: input_reads: { type: 'File[]', doc: \"Input fastq files\" } fastqc_outdir: { type: 'string?', default: \"results\" } here this does not provide documentation (but could), but tels us that the file type is a string and it provides a default name results which we have used in our use of the file. Theoretically we could specify another name, lets say we wanted to do so, lets call it --fastqc_outdir 2023mar17_results , to test our assumptions. cwltool fastqc_multiqc_wf.cwl --input_reads test.20k_reads_1.fastq.gz --input_reads test.20k_reads_2.fastq.gz --fastqc_outdir 2023mar17_results If we are correct, then we will now have a new directory fastqc_outdir , and we do! ~/Kids-First-Elements-of-Style-Workflow-Creation-Maintenance/classes/Building-A-CWL-Script$ ls -l total 5108 drwxr-xr-x 2 ad376 ad376 4096 Apr 4 17:47 2022mar31_results drwxr-xr-x 2 ad376 ad376 4096 Mar 31 16:28 cwl_tools -rw-r--r-- 1 ad376 ad376 1256 Apr 4 16:57 fastqc_multiqc_wf.cwl -rw-r--r-- 1 ad376 ad376 4496 Mar 31 16:28 README.md -rw-r--r-- 1 ad376 ad376 5398 Apr 4 17:47 report.multiqc_data.zip -rw-r--r-- 1 ad376 ad376 1463409 Apr 4 17:47 report.multiqc.html drwxr-xr-x 2 ad376 ad376 4096 Mar 31 17:12 results -rw-r--r-- 1 ad376 ad376 1956478 Mar 29 19:09 test.20k_reads_1.fastq.gz -rw-r--r-- 1 ad376 ad376 1770285 Mar 29 19:09 test.20k_reads_2.fastq.gz outdir: fastqc_outdir noextract: noextract cores: cores ram: ram steps: multiqc Multiqc takes as input the output of the fastqc step. The function step is called multiqc And the command to run is specified with the run: cwl_tools/multiqc.cwl . And the input is specified by the [stepname/out] directive which in our case is: [fastqc/fastqc_results] . As we see from this workflow snippet : multiqc: run: cwl_tools/multiqc.cwl in: fastqc_results: [fastqc/fastqc_results] recap In this last step, we executed the combined workflow which was done quite simply and executed successfully after debugging the separate steps. we saw that we did not need to specify other input than what was required by the first cwl_tools/fastqc.cwl workflow, because the second workflow, cwl_tools/multiqc.cwl required no additional input. we executed a workflow with the combined cwl_tools/fastqc.cwl and cwl_tools/multiqc.cwl . we broke down the workflow into its pieces and parts and understood how to change some of the default parameters. we followed the Elements of Style approach and used the same Docker image that was used in our Nextflow example. Return to today's (Day 4) Agenda Return to the Agenda","title":"Building A [Common Workflow Language (CWL) Workflow](https://www.commonwl.org/)"},{"location":"classes/day-4-workflow-development/building-a-cwl-workflow/#building-a-common-workflow-language-cwl-workflow","text":"Here we now show how the same containers may be used in a CWL workflow. If you are starting here and haven't Forked this repository into your own * GitHub Repository, please Fork this repository If you have your personal GitHub Fork of this repository, then be sure to as changes may have been made. Now please git clone your personal fork of the repository for this lesson. git clone https://github.com/NIH-NICHD/Building-A-CWL-Script.git","title":"Building A Common Workflow Language (CWL) Workflow"},{"location":"classes/day-4-workflow-development/building-a-cwl-workflow/#showing-the-cwl-workflow-with-the-same-containers","text":"Now Navigate to this class cd Building-A-CWL-Script Create and activate your environment, if you have not already done so. Create the eos environment. conda create -n eos -y Activate the eos environment. conda activate eos Looking at the directory we type the command: ls -l And now we see something like this total 28 drwxr-xr-x 2 adeslat adeslat 4096 Aug 30 18:57 cwl_tools -rw-r--r-- 1 adeslat adeslat 1234 Aug 30 18:57 fastqc_multiqc_wf.cwl -rw-r--r-- 1 adeslat adeslat 18656 Aug 30 18:57 README.md By convention, the bioinformaticians at Childrens Hospital of Philadelphia put their tools, their pieces for a workflow in a subdirectory they name cwl_tools . This is a good convention. We can see what is put in this directory by typing: ls -l cwl_tools We see that there are two files: -rw-r--r-- 1 ad376 ad376 1053 Mar 31 13:28 fastqc.cwl -rw-r--r-- 1 ad376 ad376 1004 Mar 31 13:28 multiqc.cwl We can inspect that file either by opening it in an editor or by typing at the terminal less cwl_tools/fastqc_cwl You can learn what the command less does by typing in the Explain shell less . Which just tells you it is the opposite of more , so lets type in the Explain shell more . It just allows us to inspect the contents of the file. To exit out of our less command press q To make the terminal window a bit larger, press the ... three triple . and pull the window larger. Type less cwl_tools/fastqc_cwl again, and pressing space bar Which we see now as: cwlVersion: v1.0 class: CommandLineTool id: fastqc requirements: - class: ShellCommandRequirement - class: DockerRequirement dockerPull: 'pgc-images.sbgenomics.com/deslattesmaysa2/fastqc:v1.0' - class: InlineJavascriptRequirement - class: ResourceRequirement ramMin: ${ return inputs.ram * 1024 } coresMin: $(inputs.cores) baseCommand: [mkdir] arguments: - position: 1 shellQuote: false valueFrom: >- $(inputs.outdir) - position: 2 shellQuote: false valueFrom: >- && fastqc inputs: input_reads: { type: 'File[]', inputBinding: {position: 99}, doc: \"Input fastq files\" } outdir: { type: 'string?', default: \"results\", inputBinding: { position: 2, prefix: \"--outdir\"} } noextract: { type: 'boolean?', default: true, inputBinding: { position: 2, prefix: \"--noextract\"} } cores: { type: 'int?', default: 2 , inputBinding: { position: 2, prefix: \"--threads\" } } ram: { type: 'int?', default: 2 } outputs: fastqc_results: type: Directory outputBinding: glob: $(inputs.outdir) To exit from the less command press q We see many of the same pieces that we saw with a Nextflow script. We see inputs: and we see outputs . We also see that there is a dockerPull: * command. dockerPull: 'pgc-images.sbgenomics.com/deslattesmaysa2/fastqc:v1.0' Which references the identical container that was used with Nextflow. Before we can pull from the docker image stored on CAVATICA, we have to authenticate. With the inputs: we see that we have input_reads: which asks us to input our reads, there is also a specification for an outdir . While I will not go into much more detail, I will emphasize that the cwl script is using the same containers as we did in the nextflow script. We can also inspect the multiqc.cwl file in that way or in the editor.","title":"Showing the CWL workflow with the same containers"},{"location":"classes/day-4-workflow-development/building-a-cwl-workflow/#installing-cwltool","text":"In the same way that we installed nextflow , we can install cwltool . To find the exact command, I typically google anaconda search packages and arrive here: Next, I get the information on how to install: To run the pipeline: conda install -c conda-forge cwltool -y We verify the installation with which cwltool Should tell us it is in the eos environment binary or executable subdirectory, bin , in my case that is here /home/adeslat/miniconda3/envs/eos/bin/cwltool","title":"Installing cwltool"},{"location":"classes/day-4-workflow-development/building-a-cwl-workflow/#executing-with-cwltool","text":"Before we can execute, we need to let the tool know where the files are -- they are in fact on Zenodo wget https://zenodo.org/record/7025773/files/test.20k_reads_1.fastq.gz wget https://zenodo.org/record/7025773/files/test.20k_reads_2.fastq.gz And now we can execute, we do not need to tell the tool where the containers are because they are specified in the script. As before we can run the tools separately, before running them together. First we test out fastqc cwltool cwl_tools/fastqc.cwl --input_reads test.20k_reads_1.fastq.gz --input_reads test.20k_reads_2.fastq.gz As we watch, what is happening is the first thing is the docker image is pulled locally. If successful, the final line will read INFO Final process status is success We test out multiqc which uses the output of fastqc as its input. In this way, we are following the Elements of Style rules of testing in parts, before testing both together. We are testing the two different cwltool scripts independently. Let's proceed. Note that for executing the multiqc script, we are not providing as input the two fastq.gz files as we did to the fastqc.cwl script, but rather the output of this previous step is the input to multiqc , so we provide the results . Before proceeding, lets inspect the results subdirectory. Typing the command line command ls -l which lists in long format as we learned from Explain Shell ls -l We see that yes, the command was successful and we have the same output we saw from the output of the Nextflow script. -rw-r--r-- 1 adeslat adeslat 236403 Aug 30 16:46 test.20k_reads_1_fastqc.html -rw-r--r-- 1 adeslat adeslat 243006 Aug 30 16:46 test.20k_reads_1_fastqc.zip -rw-r--r-- 1 adeslat adeslat 246410 Aug 30 16:46 test.20k_reads_2_fastqc.html -rw-r--r-- 1 adeslat adeslat 263730 Aug 30 16:46 test.20k_reads_2_fastqc.zip You can look at the html formatted file in the Google shell editor window by right-clicking and selecting open with preview Now lets run multiqc cwltool cwl_tools/multiqc.cwl --fastqc_results results If we look in the directory we now see the output from the multiqc run. We can inspect the output file in the same manner we inspected the fastqc output. Navigate to the appropriate directory in the Cloud Editor window, right click on the file report.multiqc.html and select preview and review the output, which is the aggregated report of the quality of both of the fastq files.","title":"Executing with cwltool"},{"location":"classes/day-4-workflow-development/building-a-cwl-workflow/#recap","text":"We saw how to: install cwltool run and test fastqc and multiqc separately using the CWL language but calling the same exact docker image as we did with the Nextflow language.","title":"recap"},{"location":"classes/day-4-workflow-development/building-a-cwl-workflow/#stitching-together-the-two-processes-into-a-single-workflow","text":"CWL does not use channels, but can read the output files from the first process as input to the second process, thereby permitting the stitching together of processes into a single workflow. When we go to the CAVATICA platform, we will see how we can do this within the visual editor. These steps help us to test the processes separately before putting them together into a single workflow, adhering to our Elements of Style approach! Below is the full cwl script for a workflow with the two steps fastqc followed by multiqc . The cwl file is in the directory and is called fastqc_multiqc_wf.cwl . Naming it with the _wf just helps us as humans to see that it is a workflow . Inspect the contents of this file, either in the editor or with the less command. cwlVersion: v1.0 class: Workflow id: fastqc_mutliqc_wf requirements: - class: MultipleInputFeatureRequirement - class: InlineJavascriptRequirement inputs: input_reads: { type: 'File[]', doc: \"Input fastq files\" } fastqc_outdir: { type: 'string?', default: \"results\" } noextract: { type: 'boolean?', default: true, doc: \"FastQC flag\" } flat: { type: 'boolean?', default: true, doc: \"MultiQC flag to use only static images\" } filename: { type: 'string?', default: \"report.multiqc\", doc: \"MultiQC output files prefix\" } cores: { type: 'int?', default: 2 } ram: { type: 'int?', default: 2 } outputs: fastqc_results: { type: Directory, outputSource: run_fastqc/fastqc_results } multiqc_zip: { type: File, outputSource: run_multiqc/multiqc_zip } multiqc_html: { type: File, outputSource: run_multiqc/multiqc_html } steps: fastqc: run: cwl_tools/fastqc.cwl in: input_reads: input_reads outdir: fastqc_outdir noextract: noextract cores: cores ram: ram out: [fastqc_results] multiqc: run: cwl_tools/multiqc.cwl in: fastqc_results: [fastqc/fastqc_results] flat: flat filename: filename cores: cores ram: ram out: [multiqc_zip, multiqc_html] The call to execute the combined workflow is then as follows: cwltool fastqc_multiqc_wf.cwl --input_reads test.20k_reads_1.fastq.gz --input_reads test.20k_reads_2.fastq.gz The steps: statement specifies which script to run. It is the exact script we have previously tested and run, cwl_tools/fastqc.cwl followed by cwl_tools/multiqc.cwl . To call this combined workflow, we call it with the input we gave the first process, cwl_tools/fastqc.cwl . We do not need to specify the input to the second process, as it requires nothing more than the output of the first workflow, cwl_tools/fastqc.cwl . But lets walk through some of the important steps and differences.","title":"Stitching together the two processes into a single workflow"},{"location":"classes/day-4-workflow-development/building-a-cwl-workflow/#breaking-down-the-cwl-script","text":"One can see that there are two steps fastqc and multiqc . The steps are easily stated because the tools are further expanded in each of the separate tool definitions.","title":"Breaking down the CWL Script."},{"location":"classes/day-4-workflow-development/building-a-cwl-workflow/#steps-fastqc","text":"","title":"steps: fastqc"},{"location":"classes/day-4-workflow-development/building-a-cwl-workflow/#run","text":"The first statement in this section contains the key word run: after this statement is the redirection to the subdirectory and full name of the already tested tool cwl_tools/fastqc.cwl .","title":"run:"},{"location":"classes/day-4-workflow-development/building-a-cwl-workflow/#in","text":"The next statement in: outlines where the input data files are coming from - and additional details.","title":"in:"},{"location":"classes/day-4-workflow-development/building-a-cwl-workflow/#input_reads","text":"input_reads: input_reads ```` redirects the machine and the reader to the default parameter definitions carried out at the beginning of the file. Going back to the top of the file we find ```bash inputs: input_reads: { type: 'File[]', doc: \"Input fastq files\" } So we see that (and input_reads states the user should specify these files on the command line) which we do in our call. Additionally the type: is File[] , indicates that the input accepts the files as an array ( [] indicates an array) means that the parameter or variable accepts multiple types, so we would provide as input using the --input_reads test.20k_reads_1.fastq.gz --input_reads test.20k_reads_2.fastq.gz","title":"input_reads:"},{"location":"classes/day-4-workflow-development/building-a-cwl-workflow/#outdir-fastqc_outdir","text":"the outdir: directive redirects us to fastqc_outdir which again we see defined at the top of the file, just after our input_reads . inputs: input_reads: { type: 'File[]', doc: \"Input fastq files\" } fastqc_outdir: { type: 'string?', default: \"results\" } here this does not provide documentation (but could), but tels us that the file type is a string and it provides a default name results which we have used in our use of the file. Theoretically we could specify another name, lets say we wanted to do so, lets call it --fastqc_outdir 2023mar17_results , to test our assumptions. cwltool fastqc_multiqc_wf.cwl --input_reads test.20k_reads_1.fastq.gz --input_reads test.20k_reads_2.fastq.gz --fastqc_outdir 2023mar17_results If we are correct, then we will now have a new directory fastqc_outdir , and we do! ~/Kids-First-Elements-of-Style-Workflow-Creation-Maintenance/classes/Building-A-CWL-Script$ ls -l total 5108 drwxr-xr-x 2 ad376 ad376 4096 Apr 4 17:47 2022mar31_results drwxr-xr-x 2 ad376 ad376 4096 Mar 31 16:28 cwl_tools -rw-r--r-- 1 ad376 ad376 1256 Apr 4 16:57 fastqc_multiqc_wf.cwl -rw-r--r-- 1 ad376 ad376 4496 Mar 31 16:28 README.md -rw-r--r-- 1 ad376 ad376 5398 Apr 4 17:47 report.multiqc_data.zip -rw-r--r-- 1 ad376 ad376 1463409 Apr 4 17:47 report.multiqc.html drwxr-xr-x 2 ad376 ad376 4096 Mar 31 17:12 results -rw-r--r-- 1 ad376 ad376 1956478 Mar 29 19:09 test.20k_reads_1.fastq.gz -rw-r--r-- 1 ad376 ad376 1770285 Mar 29 19:09 test.20k_reads_2.fastq.gz outdir: fastqc_outdir noextract: noextract cores: cores ram: ram","title":"outdir: fastqc_outdir"},{"location":"classes/day-4-workflow-development/building-a-cwl-workflow/#steps-multiqc","text":"Multiqc takes as input the output of the fastqc step. The function step is called multiqc And the command to run is specified with the run: cwl_tools/multiqc.cwl . And the input is specified by the [stepname/out] directive which in our case is: [fastqc/fastqc_results] . As we see from this workflow snippet : multiqc: run: cwl_tools/multiqc.cwl in: fastqc_results: [fastqc/fastqc_results]","title":"steps: multiqc"},{"location":"classes/day-4-workflow-development/building-a-cwl-workflow/#recap_1","text":"In this last step, we executed the combined workflow which was done quite simply and executed successfully after debugging the separate steps. we saw that we did not need to specify other input than what was required by the first cwl_tools/fastqc.cwl workflow, because the second workflow, cwl_tools/multiqc.cwl required no additional input. we executed a workflow with the combined cwl_tools/fastqc.cwl and cwl_tools/multiqc.cwl . we broke down the workflow into its pieces and parts and understood how to change some of the default parameters. we followed the Elements of Style approach and used the same Docker image that was used in our Nextflow example.","title":"recap"},{"location":"classes/day-4-workflow-development/building-a-cwl-workflow/#return-to-todays-day-4-agenda","text":"Return to the Agenda","title":"Return to today's (Day 4) Agenda"},{"location":"classes/day-4-workflow-development/building-a-nextflow-workflow/","text":"Building A Nextflow Workflow If you have arrived here from the front page and have not built your fastqc-docker and multiqc-docker containers, please proceed to the preamble to building a Nextflow Workflow Main outcome: During the first session you will build a FastQC & MultiQC pipeline to learn the basics of Nextflow including: Parameters Processes (inputs, outputs & scripts) Channels Operators Configuration In the Google Shell We are going to continue use Google shell Cloud to walk through the building of a Nextflow script and in the next session building the containers we used to do so. Inside the shell we have the following: Docker is installed conda has been installed -- which we did in the previous lesson. The standard workflow language for Nextflow nextflow can be installed The standard workflow language for the Common Workflow Language (CWL) cwltool can be installed What is Docker ? Just to recap a bit. Docker is the application we use to build our containerized images. It turns our code into an image that can be run interactively or be placed upon a virtual machine that we spin up on CAVATICA as part of a workflow. The logo that is associated with Docker is a whale but looks like a containership. Containers revolutionized the shipping industry by creating a uniform entity that could permits disparate items to be packaged in the same manner allowing devices that do not know what they contain to carry those items. Much in the same way that packets revolutionized communication with the internet. This is why we concern ourselves with containers. Install Nextflow Now we can install nextflow package. To get the installation command, we again search Anaconda for the package details. We see that this is a pretty popular package, there have been at the time of this writing, that this has been downloaded 162,780 times and that the last upload was 2 months and 6 days ago and we have the GitHub location of the software. Importantly, we now see how to install the nextflow application. conda install -c bioconda nextflow The package installer manages the items that need to be installed, telling us the following packages will be installed and asking if we would like to proceed. The following NEW packages will be INSTALLED: coreutils bioconda/linux-64::coreutils-8.25-1 java-jdk bioconda/linux-64::java-jdk-8.0.92-1 libgcc pkgs/main/linux-64::libgcc-7.2.0-h69d50b8_2 nextflow bioconda/linux-64::nextflow-0.24.2-0 The following packages will be UPDATED: ca-certificates conda-forge::ca-certificates-2021.10.~ --> pkgs/main::ca-certificates-2022.3.18-h06a4308_0 Proceed ([y]/n)? We say y and the package is installed. Confirming we type the following: which nextflow And because we have activated our environment eos , you will see that the nextflow application is installed in the bin which is short for binary location within eos (eos) adeslat@cloudshell:~$ which nextflow /home/adeslat/miniconda3/envs/eos/bin/nextflow a) Getting Course material. Let's use your forked version of the repository. First lets make sure it is up-to-date. To do that do the following. Follow the directions for keeping your repository in sync At this time do not worry about making a pull request. If your repository has commits ahead at this time, do not concern yourself. Then as we did yesterday, if you have not checked out your own version. Do so now but in your username subdirectory. After synchronizing, navigate to your subdirectory and clone the updated respository cd adeslatt And clone git clone https://github.com/NIH-NICHD/Building-A-Nextflow-Script.git b) Parameters (otherwise known as inputs) Now that we have Nextflow & Docker installed we're ready to run our first script and learn how we bring in data into a file. Let's open the file params_reads.nf You see the following content: // params_reads.nf params.reads = false println \"My reads: ${params.reads}\" The first line is a comment. Comments in Nextflow can begin with // . Every language has a different way of indicating that this is for the human to read and not the machine. This comment may seem silly, but when you have many files open, it is uesful to have an indication of where you are. The second line creates something for the computer. Think of it as an empty vessel. We can call it what we want, but in this case we are actually going to use something that our program, nextflow understands, params and create an arbitrary term reads Together they make what you could imagine is an element of a paragraph. You can define many things that are of a params type, but for now we are just creating one thing. Something we are calling reads . Finally we are setting this params.reads to false . The third line instructs the program nextflow to print out the value of this parameter upon execution of the workflow. This minimal workflow can now be executed by the nextflow application we have installed. We will do this and we will provide the workflow a value to params.reads . In this repository are two fastq files in a directory within the freshly checked out repository as follows. nextflow run params_reads.nf --reads testdata/test.20k_reads_1.fastq.gz The run returns the name of our file. c) Processes (inputs, outputs & scripts) Well, we have run our first nextflow workflow, congratulations!. Nextflow allows the execution of any command or user script by using a process definition. A process is defined by providing three main declarations: the process inputs , the process outputs and finally the command script . Let's look at the next workflow, fastqc.nf , we see the following: //fastqc.nf reads = file(params.reads) process fastqc { publishDir \"results\", mode: 'copy' input: file(reads) from reads output: file \"*_fastqc.{zip,html}\" into fastqc_results script: \"\"\" fastqc $reads \"\"\" } Here we created the variable reads which is a file from the command line input. We can then create the process fastqc including: - the directive publishDir to specify which folder to copy the output files to - the inputs where we declare a file reads from our variable reads - the output which is anything ending in _fastqc.zip or _fastqc.html which will go into a fastqc_results channel - the script where we are running the fastqc command on our reads variable We can then run our workflow with the following command: nextflow run fastqc.nf --reads testdata/test.20k_reads_1.fastq.gz -with-docker fastqc We are using a docker image I previously had pushed to the Seven Bridges Image repository -- in the next session I will walk through how that is done. By running Nextflow using the with-docker flag we can specify a Docker container to execute this command in. This is beneficial because it means we do not need to have fastqc installed locally on our laptop. We just need to specify a Docker container that has fastqc installed. d) Channels Channels are the preferred method of transferring data in Nextflow & can connect two processes or operators. Here we will use the method fromFilePairs to create a channel to load paired-end FASTQ data, rather than just a single FASTQ file. To do this we will replace the code from 1c with the following //fastqc.nf reads = Channel.fromFilePairs(params.reads, size: 2) process fastqc { tag \"$name\" publishDir \"results\", mode: 'copy' container 'pgc-images.sbgenomics.com/deslattesmaysa2/fastqc:v1.0' input: set val(name), file(reads) from reads output: file \"*_fastqc.{zip,html}\" into fastqc_results script: \"\"\" fastqc $reads \"\"\" } The reads variable is now equal to a channel which contains the reads prefix & paired-end FASTQ data. Therefore, the input declaration has also changed to reflect this by declaring the value name . This name can be used as a tag for when the pipeline is run. Also, as we are now declaring two inputs the set keyword has to be used. Finally, we can specify the container name within the processes as a directive. To run the pipeline: nextflow run fastqc.nf --reads \"testdata/test.20k_reads_{1,2}.fastq.gz\" -with-docker pgc-images.sbgenomics.com/adeslat/fastqc:v0.11.9 e) Operators Operators are methods that allow you to manipulate & connect channels. Here we will add a new process multiqc & use the .collect() operator Add the multiqc process after fastqc : //fastqc_multiqc_wf.nf reads = Channel.fromFilePairs(params.reads, size: 2) process fastqc { tag \"$name\" publishDir \"results\", mode: 'copy' container 'pgc-images.sbgenomics.com/deslattesmaysa2/fastqc:v1.0' input: set val(name), file(reads) from reads output: file \"*_fastqc.{zip,html}\" into fastqc_results script: \"\"\" fastqc $reads \"\"\" } process multiqc { publishDir \"results\", mode: 'copy' container 'pgc-images.sbgenomics.com/deslattesmaysa2/multiqc:v1.0' input: file ('fastqc/*') from fastqc_results.collect() output: file \"*multiqc_report.html\" into multiqc_report file \"*_data\" script: \"\"\" multiqc . -m fastqc \"\"\" } Here we have added another process multiqc . We have used the collect operator here so that if fastqc ran for more than two pairs of files multiqc would collect all of the files & run only once. The pipeline can be run with the following: nextflow run fastqc_multiqc_wf.nf --reads \"testdata/test.20k_reads_{1,2}.fastq.gz\" -with-docker pgc-images.sbgenomics.com/deslattesmaysa2/fastqc:v1.0 f) Configuration Configuration, such as parameters, containers & resources eg memory can be set in config files such as nextflow.config . For example our nextflow.config file might look like this: docker.enabled = true params.reads = false process { cpus = 1 memory = \"2.GB\" withName: fastqc { container = \"pgc-images.sbgenomics.com/adeslat/fastqc:v0.11.9\" } withName: multiqc { container = \"pgc-images.sbgenomics.com/adeslat/multiqc:v1.0dev0\" } } which are in fact the images as I stored them in the CAVATICA repository. Here we have enabled docker by default, initialised parameters, set resources & containers. It is best practice to keep these in the config file so that they can more easily be set or removed. Containers & params.reads can then be removed from main.nf . The pipeline can now be run with the following: nextflow run fastqc_multiqc_wf.nf --reads \"testdata/test.20k_reads_{1,2}.fastq.gz\" Proceed to the next lesson Return to the Agenda","title":"Building a nextflow workflow"},{"location":"classes/day-4-workflow-development/building-a-nextflow-workflow/#building-a-nextflow-workflow","text":"If you have arrived here from the front page and have not built your fastqc-docker and multiqc-docker containers, please proceed to the preamble to building a Nextflow Workflow Main outcome: During the first session you will build a FastQC & MultiQC pipeline to learn the basics of Nextflow including: Parameters Processes (inputs, outputs & scripts) Channels Operators Configuration","title":"Building A Nextflow Workflow"},{"location":"classes/day-4-workflow-development/building-a-nextflow-workflow/#in-the-google-shell","text":"We are going to continue use Google shell Cloud to walk through the building of a Nextflow script and in the next session building the containers we used to do so. Inside the shell we have the following: Docker is installed conda has been installed -- which we did in the previous lesson. The standard workflow language for Nextflow nextflow can be installed The standard workflow language for the Common Workflow Language (CWL) cwltool can be installed","title":"In the Google Shell"},{"location":"classes/day-4-workflow-development/building-a-nextflow-workflow/#what-is-docker","text":"Just to recap a bit. Docker is the application we use to build our containerized images. It turns our code into an image that can be run interactively or be placed upon a virtual machine that we spin up on CAVATICA as part of a workflow. The logo that is associated with Docker is a whale but looks like a containership. Containers revolutionized the shipping industry by creating a uniform entity that could permits disparate items to be packaged in the same manner allowing devices that do not know what they contain to carry those items. Much in the same way that packets revolutionized communication with the internet. This is why we concern ourselves with containers.","title":"What is Docker ?"},{"location":"classes/day-4-workflow-development/building-a-nextflow-workflow/#install-nextflow","text":"Now we can install nextflow package. To get the installation command, we again search Anaconda for the package details. We see that this is a pretty popular package, there have been at the time of this writing, that this has been downloaded 162,780 times and that the last upload was 2 months and 6 days ago and we have the GitHub location of the software. Importantly, we now see how to install the nextflow application. conda install -c bioconda nextflow The package installer manages the items that need to be installed, telling us the following packages will be installed and asking if we would like to proceed. The following NEW packages will be INSTALLED: coreutils bioconda/linux-64::coreutils-8.25-1 java-jdk bioconda/linux-64::java-jdk-8.0.92-1 libgcc pkgs/main/linux-64::libgcc-7.2.0-h69d50b8_2 nextflow bioconda/linux-64::nextflow-0.24.2-0 The following packages will be UPDATED: ca-certificates conda-forge::ca-certificates-2021.10.~ --> pkgs/main::ca-certificates-2022.3.18-h06a4308_0 Proceed ([y]/n)? We say y and the package is installed. Confirming we type the following: which nextflow And because we have activated our environment eos , you will see that the nextflow application is installed in the bin which is short for binary location within eos (eos) adeslat@cloudshell:~$ which nextflow /home/adeslat/miniconda3/envs/eos/bin/nextflow","title":"Install Nextflow"},{"location":"classes/day-4-workflow-development/building-a-nextflow-workflow/#a-getting-course-material","text":"Let's use your forked version of the repository. First lets make sure it is up-to-date. To do that do the following. Follow the directions for keeping your repository in sync At this time do not worry about making a pull request. If your repository has commits ahead at this time, do not concern yourself. Then as we did yesterday, if you have not checked out your own version. Do so now but in your username subdirectory. After synchronizing, navigate to your subdirectory and clone the updated respository cd adeslatt And clone git clone https://github.com/NIH-NICHD/Building-A-Nextflow-Script.git","title":"a) Getting Course material."},{"location":"classes/day-4-workflow-development/building-a-nextflow-workflow/#b-parameters-otherwise-known-as-inputs","text":"Now that we have Nextflow & Docker installed we're ready to run our first script and learn how we bring in data into a file. Let's open the file params_reads.nf You see the following content: // params_reads.nf params.reads = false println \"My reads: ${params.reads}\" The first line is a comment. Comments in Nextflow can begin with // . Every language has a different way of indicating that this is for the human to read and not the machine. This comment may seem silly, but when you have many files open, it is uesful to have an indication of where you are. The second line creates something for the computer. Think of it as an empty vessel. We can call it what we want, but in this case we are actually going to use something that our program, nextflow understands, params and create an arbitrary term reads Together they make what you could imagine is an element of a paragraph. You can define many things that are of a params type, but for now we are just creating one thing. Something we are calling reads . Finally we are setting this params.reads to false . The third line instructs the program nextflow to print out the value of this parameter upon execution of the workflow. This minimal workflow can now be executed by the nextflow application we have installed. We will do this and we will provide the workflow a value to params.reads . In this repository are two fastq files in a directory within the freshly checked out repository as follows. nextflow run params_reads.nf --reads testdata/test.20k_reads_1.fastq.gz The run returns the name of our file.","title":"b) Parameters (otherwise known as inputs)"},{"location":"classes/day-4-workflow-development/building-a-nextflow-workflow/#c-processes-inputs-outputs-scripts","text":"Well, we have run our first nextflow workflow, congratulations!. Nextflow allows the execution of any command or user script by using a process definition. A process is defined by providing three main declarations: the process inputs , the process outputs and finally the command script . Let's look at the next workflow, fastqc.nf , we see the following: //fastqc.nf reads = file(params.reads) process fastqc { publishDir \"results\", mode: 'copy' input: file(reads) from reads output: file \"*_fastqc.{zip,html}\" into fastqc_results script: \"\"\" fastqc $reads \"\"\" } Here we created the variable reads which is a file from the command line input. We can then create the process fastqc including: - the directive publishDir to specify which folder to copy the output files to - the inputs where we declare a file reads from our variable reads - the output which is anything ending in _fastqc.zip or _fastqc.html which will go into a fastqc_results channel - the script where we are running the fastqc command on our reads variable We can then run our workflow with the following command: nextflow run fastqc.nf --reads testdata/test.20k_reads_1.fastq.gz -with-docker fastqc We are using a docker image I previously had pushed to the Seven Bridges Image repository -- in the next session I will walk through how that is done. By running Nextflow using the with-docker flag we can specify a Docker container to execute this command in. This is beneficial because it means we do not need to have fastqc installed locally on our laptop. We just need to specify a Docker container that has fastqc installed.","title":"c) Processes (inputs, outputs &amp; scripts)"},{"location":"classes/day-4-workflow-development/building-a-nextflow-workflow/#d-channels","text":"Channels are the preferred method of transferring data in Nextflow & can connect two processes or operators. Here we will use the method fromFilePairs to create a channel to load paired-end FASTQ data, rather than just a single FASTQ file. To do this we will replace the code from 1c with the following //fastqc.nf reads = Channel.fromFilePairs(params.reads, size: 2) process fastqc { tag \"$name\" publishDir \"results\", mode: 'copy' container 'pgc-images.sbgenomics.com/deslattesmaysa2/fastqc:v1.0' input: set val(name), file(reads) from reads output: file \"*_fastqc.{zip,html}\" into fastqc_results script: \"\"\" fastqc $reads \"\"\" } The reads variable is now equal to a channel which contains the reads prefix & paired-end FASTQ data. Therefore, the input declaration has also changed to reflect this by declaring the value name . This name can be used as a tag for when the pipeline is run. Also, as we are now declaring two inputs the set keyword has to be used. Finally, we can specify the container name within the processes as a directive. To run the pipeline: nextflow run fastqc.nf --reads \"testdata/test.20k_reads_{1,2}.fastq.gz\" -with-docker pgc-images.sbgenomics.com/adeslat/fastqc:v0.11.9","title":"d) Channels"},{"location":"classes/day-4-workflow-development/building-a-nextflow-workflow/#e-operators","text":"Operators are methods that allow you to manipulate & connect channels. Here we will add a new process multiqc & use the .collect() operator Add the multiqc process after fastqc : //fastqc_multiqc_wf.nf reads = Channel.fromFilePairs(params.reads, size: 2) process fastqc { tag \"$name\" publishDir \"results\", mode: 'copy' container 'pgc-images.sbgenomics.com/deslattesmaysa2/fastqc:v1.0' input: set val(name), file(reads) from reads output: file \"*_fastqc.{zip,html}\" into fastqc_results script: \"\"\" fastqc $reads \"\"\" } process multiqc { publishDir \"results\", mode: 'copy' container 'pgc-images.sbgenomics.com/deslattesmaysa2/multiqc:v1.0' input: file ('fastqc/*') from fastqc_results.collect() output: file \"*multiqc_report.html\" into multiqc_report file \"*_data\" script: \"\"\" multiqc . -m fastqc \"\"\" } Here we have added another process multiqc . We have used the collect operator here so that if fastqc ran for more than two pairs of files multiqc would collect all of the files & run only once. The pipeline can be run with the following: nextflow run fastqc_multiqc_wf.nf --reads \"testdata/test.20k_reads_{1,2}.fastq.gz\" -with-docker pgc-images.sbgenomics.com/deslattesmaysa2/fastqc:v1.0","title":"e) Operators"},{"location":"classes/day-4-workflow-development/building-a-nextflow-workflow/#f-configuration","text":"Configuration, such as parameters, containers & resources eg memory can be set in config files such as nextflow.config . For example our nextflow.config file might look like this: docker.enabled = true params.reads = false process { cpus = 1 memory = \"2.GB\" withName: fastqc { container = \"pgc-images.sbgenomics.com/adeslat/fastqc:v0.11.9\" } withName: multiqc { container = \"pgc-images.sbgenomics.com/adeslat/multiqc:v1.0dev0\" } } which are in fact the images as I stored them in the CAVATICA repository. Here we have enabled docker by default, initialised parameters, set resources & containers. It is best practice to keep these in the config file so that they can more easily be set or removed. Containers & params.reads can then be removed from main.nf . The pipeline can now be run with the following: nextflow run fastqc_multiqc_wf.nf --reads \"testdata/test.20k_reads_{1,2}.fastq.gz\"","title":"f) Configuration"},{"location":"classes/day-4-workflow-development/building-a-nextflow-workflow/#proceed-to-the-next-lesson","text":"Return to the Agenda","title":"Proceed to the next lesson"},{"location":"classes/day-4-workflow-development/day-4-workflow-development/","text":"Agenda for the Day 4: Workflow Development Time (UTC) Programme 11.00 - 11.30 Welcome and Preamble to Building our Workflow 11:10 - 12.00 Building A Nextflow Script 12.00 - 12.10 Short break 12:10 - 12:40 Building A CWL Script 12.40 - 11.50 Shared elements across workflow languages 12.50 - 13:00 Wrap up and overview of next days topics Additional Resources Authenticating on Cavatica Common Workflow Language Common Workflow Langage How-to Nextflow Nextflow Basic pipeline with detail description of the file Nextflow Blast Example Nextflow Community Based Pipelines Nextflow Mixing Programming Languages In The Workflow","title":"Day 4 - Workflow development"},{"location":"classes/day-4-workflow-development/day-4-workflow-development/#agenda-for-the-day-4-workflow-development","text":"Time (UTC) Programme 11.00 - 11.30 Welcome and Preamble to Building our Workflow 11:10 - 12.00 Building A Nextflow Script 12.00 - 12.10 Short break 12:10 - 12:40 Building A CWL Script 12.40 - 11.50 Shared elements across workflow languages 12.50 - 13:00 Wrap up and overview of next days topics","title":"Agenda for the Day 4: Workflow Development"},{"location":"classes/day-4-workflow-development/day-4-workflow-development/#additional-resources","text":"Authenticating on Cavatica Common Workflow Language Common Workflow Langage How-to Nextflow Nextflow Basic pipeline with detail description of the file Nextflow Blast Example Nextflow Community Based Pipelines Nextflow Mixing Programming Languages In The Workflow","title":"Additional Resources"},{"location":"classes/day-4-workflow-development/preamble/","text":"Preamble to building workflows using containers In this session, we will use the two Docker images we built in the previous sections and we will how to put these together into a single workflow using the Nextflow workflow language. Later, we will learn out to put these two Docker images into a single workflow using the CWL or Common Workflow Language. We will: Login to Google shell cloud Build both of our docker images of the processes fastqc and multiqc using the package installers maintained on Anaconda Login to CAVATICA and generate our CAVATICA Authentication Token Tag our Docker images Authenticate with Cavatica within the Google Shell Push our Docker images to our own personal repository on Cavatica Use our own Docker images in the Nextflow and CWL scripts we write. Login to Google shell cloud Please proceed to login to your google shell. If you did not work with us yesterday, please return to the previous lesson to build your fastqc-docker and multiqc-docker images. When it comes up, type: docker images You will see there are no images from yesterday. If you recall, I mentioned that these were ephemeral instances, so the new machine that has just been spun up, though it maintains your file structure, does not have your image anymore. Build your Fastqc Docker Image Lets first build Fastqc Docker Image. If you recall, yesterday, I said there are only 3 steps in building a Docker Image. Build Tag Push Yesterday, we did not push. What is pushing? To make our Docker image available to us, we use a Repository . We are working on CAVATICA and CAVATICA offers to you the option of creating your own repository. Think of the repository just like your GitHub, it is a location where you can keep and use your own Docker Images. To be able to Push our docker images to our repository, we need to Authenticate and we Authenticate with our very own Developer's Token . We will get to that. Please proceed to your fastqc-docker subdirectory. cd fastqc-docker As a best practice, since you now have pushed this into GitHub , you should out of a habit type the following commands: git status Likely there are no changes, but if you were collaborating with someone, they may have made changes, or if you are working on multiple systems, again you should do the following: git pull We do not need to authenticate, but if there were changes, we would proceed as we did yesterday to set up our user.name and user.email . But we do not need to do this at the moment. Now let us build the Docker image Build Fastqc docker build -t fastqc . This tags the image with the tag fastqc, however we actually have to tag it with a tagname that will work on the CAVATICA Registry. Following the directions for pushing images to your own CAVATICA Docker registry with one change the destination is \u2757 pgc-images.sbgenomics.com \u2757 -- the notes erroneously state a different location, I have asked they update the documentation. Lets look at our images docker images It should look something like this: (eos) ad376@cloudshell:~/fastqc-docker$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE fastqc latest 7bcdb3be3afc 6 minutes ago 1.07GB continuumio/miniconda3 latest ce7d119281a1 2 months ago 403MB TAG Fastqc Take note of the IMAGE ID , that is what we will use to tag our image for pushing to CAVATICA. I did a local install of the fastqc to find out the version we have: conda install -c fastqc -y And typed fastqc -v Now I will use that version to change the TAG to more specifically reflect the image I have built. You can see the syntax by typing: docker tag Which tells us (eos) ad376@cloudshell:~/fastqc-docker$ docker tag \"docker tag\" requires exactly 2 arguments. See 'docker tag --help'. Usage: docker tag SOURCE_IMAGE[:TAG] TARGET_IMAGE[:TAG] Note that we have the Docker Image Repository Specific to me is at the location of pgc-images.sbgenomics.com/adeslat. For your own repository -- replace adeslat with your own [CAVATICA USERID] . So now I tag: docker tag 7bcdb3be3afc pgc-images.sbgenomics.com/[YOUR CAVATICA USERID]/fastqc:v0.11.9 Where the first item after the word tag is the IMAGE ID - aka the SOURCE_IMAGE The second is the TARGET_IMAGE PUSH Fastqc Okay, almost there. We have built and we have tagged, now we want to push our image to our Docker Image Repository. To do that we need to authenticate. To authenticate we need a developers token. Login to CAVATICA and Generate Authentication Token Navigate to the Developers Tab Select Authentication Token and Press Generate Authentication Token Have a look at your repository, you will see that it is empty. Now copy your authentication token. Now go back to the Google Shell and login to the repository. docker login pgc-images.sbgenomics.com -u adeslat -p [paste your authentication token here] Now we can push to our repository docker push pgc-images.sbgenomics.com/adeslat/fastqc:v0.11.9 And if all is going well we see: The push refers to repository [pgc-images.sbgenomics.com/adeslat/fastqc] 3f13f484a6c6: Pushed cbc020caf48c: Pushed ab2731ec3f53: Pushed 6fa1f4185aa2: Pushed ad6562704f37: Pushed v0.11.9: digest: sha256:400e075f2ba7c94f6982caa3a43bd90d1857ecde457bf4a26a4ab9b9423d7d85 size: 1373 Now we can inspect our repository on CAVATICA and we see: Now we actually can properly build and tag from the getgo our other image, for multiqc. Build and Tag Multiqc Navigate to the multiqc-docker directory cd ../multiqc-docker And lets build with the tag appropriate for pushing to CAVATICA straight away. docker build -t pgc-images.sbgenomics.com/adeslat/multiqc:v1.0dev0 . Because we are already authenticated, we can simply push. Push Multiqc docker push pgc-images.sbgenomics.com/adeslat/multiqc:v1.0dev0 And now when we inspect our repository, we have: Now we can proceed with our workflow development! Return to Agenda","title":"Preamble to building workflows using containers"},{"location":"classes/day-4-workflow-development/preamble/#preamble-to-building-workflows-using-containers","text":"In this session, we will use the two Docker images we built in the previous sections and we will how to put these together into a single workflow using the Nextflow workflow language. Later, we will learn out to put these two Docker images into a single workflow using the CWL or Common Workflow Language. We will: Login to Google shell cloud Build both of our docker images of the processes fastqc and multiqc using the package installers maintained on Anaconda Login to CAVATICA and generate our CAVATICA Authentication Token Tag our Docker images Authenticate with Cavatica within the Google Shell Push our Docker images to our own personal repository on Cavatica Use our own Docker images in the Nextflow and CWL scripts we write.","title":"Preamble to building workflows using containers"},{"location":"classes/day-4-workflow-development/preamble/#login-to-google-shell-cloud","text":"Please proceed to login to your google shell. If you did not work with us yesterday, please return to the previous lesson to build your fastqc-docker and multiqc-docker images. When it comes up, type: docker images You will see there are no images from yesterday. If you recall, I mentioned that these were ephemeral instances, so the new machine that has just been spun up, though it maintains your file structure, does not have your image anymore.","title":"Login to Google shell cloud"},{"location":"classes/day-4-workflow-development/preamble/#build-your-fastqc-docker-image","text":"Lets first build Fastqc Docker Image. If you recall, yesterday, I said there are only 3 steps in building a Docker Image. Build Tag Push Yesterday, we did not push. What is pushing? To make our Docker image available to us, we use a Repository . We are working on CAVATICA and CAVATICA offers to you the option of creating your own repository. Think of the repository just like your GitHub, it is a location where you can keep and use your own Docker Images. To be able to Push our docker images to our repository, we need to Authenticate and we Authenticate with our very own Developer's Token . We will get to that. Please proceed to your fastqc-docker subdirectory. cd fastqc-docker As a best practice, since you now have pushed this into GitHub , you should out of a habit type the following commands: git status Likely there are no changes, but if you were collaborating with someone, they may have made changes, or if you are working on multiple systems, again you should do the following: git pull We do not need to authenticate, but if there were changes, we would proceed as we did yesterday to set up our user.name and user.email . But we do not need to do this at the moment. Now let us build the Docker image","title":"Build your Fastqc Docker Image"},{"location":"classes/day-4-workflow-development/preamble/#build-fastqc","text":"docker build -t fastqc . This tags the image with the tag fastqc, however we actually have to tag it with a tagname that will work on the CAVATICA Registry. Following the directions for pushing images to your own CAVATICA Docker registry with one change the destination is \u2757 pgc-images.sbgenomics.com \u2757 -- the notes erroneously state a different location, I have asked they update the documentation. Lets look at our images docker images It should look something like this: (eos) ad376@cloudshell:~/fastqc-docker$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE fastqc latest 7bcdb3be3afc 6 minutes ago 1.07GB continuumio/miniconda3 latest ce7d119281a1 2 months ago 403MB","title":"Build Fastqc"},{"location":"classes/day-4-workflow-development/preamble/#tag-fastqc","text":"Take note of the IMAGE ID , that is what we will use to tag our image for pushing to CAVATICA. I did a local install of the fastqc to find out the version we have: conda install -c fastqc -y And typed fastqc -v Now I will use that version to change the TAG to more specifically reflect the image I have built. You can see the syntax by typing: docker tag Which tells us (eos) ad376@cloudshell:~/fastqc-docker$ docker tag \"docker tag\" requires exactly 2 arguments. See 'docker tag --help'. Usage: docker tag SOURCE_IMAGE[:TAG] TARGET_IMAGE[:TAG] Note that we have the Docker Image Repository Specific to me is at the location of pgc-images.sbgenomics.com/adeslat. For your own repository -- replace adeslat with your own [CAVATICA USERID] . So now I tag: docker tag 7bcdb3be3afc pgc-images.sbgenomics.com/[YOUR CAVATICA USERID]/fastqc:v0.11.9 Where the first item after the word tag is the IMAGE ID - aka the SOURCE_IMAGE The second is the TARGET_IMAGE","title":"TAG Fastqc"},{"location":"classes/day-4-workflow-development/preamble/#push-fastqc","text":"Okay, almost there. We have built and we have tagged, now we want to push our image to our Docker Image Repository. To do that we need to authenticate. To authenticate we need a developers token.","title":"PUSH Fastqc"},{"location":"classes/day-4-workflow-development/preamble/#login-to-cavatica-and-generate-authentication-token","text":"Navigate to the Developers Tab Select Authentication Token and Press Generate Authentication Token Have a look at your repository, you will see that it is empty. Now copy your authentication token. Now go back to the Google Shell and login to the repository. docker login pgc-images.sbgenomics.com -u adeslat -p [paste your authentication token here] Now we can push to our repository docker push pgc-images.sbgenomics.com/adeslat/fastqc:v0.11.9 And if all is going well we see: The push refers to repository [pgc-images.sbgenomics.com/adeslat/fastqc] 3f13f484a6c6: Pushed cbc020caf48c: Pushed ab2731ec3f53: Pushed 6fa1f4185aa2: Pushed ad6562704f37: Pushed v0.11.9: digest: sha256:400e075f2ba7c94f6982caa3a43bd90d1857ecde457bf4a26a4ab9b9423d7d85 size: 1373 Now we can inspect our repository on CAVATICA and we see: Now we actually can properly build and tag from the getgo our other image, for multiqc.","title":"Login to CAVATICA and Generate Authentication Token"},{"location":"classes/day-4-workflow-development/preamble/#build-and-tag-multiqc","text":"Navigate to the multiqc-docker directory cd ../multiqc-docker And lets build with the tag appropriate for pushing to CAVATICA straight away. docker build -t pgc-images.sbgenomics.com/adeslat/multiqc:v1.0dev0 . Because we are already authenticated, we can simply push.","title":"Build and Tag Multiqc"},{"location":"classes/day-4-workflow-development/preamble/#push-multiqc","text":"docker push pgc-images.sbgenomics.com/adeslat/multiqc:v1.0dev0 And now when we inspect our repository, we have: Now we can proceed with our workflow development! Return to Agenda","title":"Push Multiqc"},{"location":"classes/day-5-workflow-execution/GitHubActionsForMaintenanceTesting/","text":"GitHub Actions GitHub has built-in capability to execute workflows upon pushes to the repository, exposing these advanced capabilities to the everyday GitHub users. Examples for GitHub Actions include minimal execution of your end-to-end workflow. As an example, have a look at this Long-read-proteogenomics repository and the badges for workflows that it contains. End-to-End Workflow testing There are two workflows executed here within the limits GitHub offers, basically 8GB of memory and small amount of diskspace. name: Testing for Long Reads Proteogenomics with Sqanti # This workflow runs the pipeline with the minimal test dataset to check that it completes without any syntax errors on: push: branches: - dev pull_request: release: types: [published] jobs: test: name: Run workflow tests runs-on: ubuntu-latest env: NXF_VER: ${{ matrix.nxf_ver }} NXF_ANSI_LOG: false strategy: matrix: # Nextflow versions: check pipeline minimum and current latest nxf_ver: ['20.01.0', ''] steps: - name: Check out pipeline code uses: actions/checkout@v2 - name: Install Nextflow run: | wget -qO- get.nextflow.io | bash sudo mv nextflow /usr/local/bin/ - name: Run pipeline with test data run: | nextflow run ${GITHUB_WORKSPACE} --config conf/test_with_sqanti.config This vignette walks the user through the automatic testing workflow from Long Read Proteogenomics - help the researcher in understanding the details of inputs, outputs and scripts that are very straightforward shell scripts helps one understand what is happening here, and better yet, how the researcher can do this with their own work. There are a lot of details here, but one doesn't need to know much to get started with GitHub actions Automatic deployment of actions In the case of the three Dockerfiles reviewed in the previous lesson, each were released once successfully built and then we used GitHub actions to automatically build and publish the images, making them accessible with a degree of confidence that they have been built and therefore there are no syntactical reasons why it should not run. This means that we are in a way, using GitHub actions as a complier for our images. Example of Creating GitHub actions for star-docker container. The STAR docker is the simplest of containers. Why we would do so is that one can imagine scaling to 100s to 1000s of files that need to be aligned with STAR. The smaller the image, the smaller the machine that could be used to execute that image. Food for thought as we continue to scale. Return to the Agenda","title":"GitHubActionsForMaintenanceTesting"},{"location":"classes/day-5-workflow-execution/GitHubActionsForMaintenanceTesting/#github-actions","text":"GitHub has built-in capability to execute workflows upon pushes to the repository, exposing these advanced capabilities to the everyday GitHub users. Examples for GitHub Actions include minimal execution of your end-to-end workflow. As an example, have a look at this Long-read-proteogenomics repository and the badges for workflows that it contains.","title":"GitHub Actions"},{"location":"classes/day-5-workflow-execution/GitHubActionsForMaintenanceTesting/#end-to-end-workflow-testing","text":"There are two workflows executed here within the limits GitHub offers, basically 8GB of memory and small amount of diskspace. name: Testing for Long Reads Proteogenomics with Sqanti # This workflow runs the pipeline with the minimal test dataset to check that it completes without any syntax errors on: push: branches: - dev pull_request: release: types: [published] jobs: test: name: Run workflow tests runs-on: ubuntu-latest env: NXF_VER: ${{ matrix.nxf_ver }} NXF_ANSI_LOG: false strategy: matrix: # Nextflow versions: check pipeline minimum and current latest nxf_ver: ['20.01.0', ''] steps: - name: Check out pipeline code uses: actions/checkout@v2 - name: Install Nextflow run: | wget -qO- get.nextflow.io | bash sudo mv nextflow /usr/local/bin/ - name: Run pipeline with test data run: | nextflow run ${GITHUB_WORKSPACE} --config conf/test_with_sqanti.config This vignette walks the user through the automatic testing workflow from Long Read Proteogenomics - help the researcher in understanding the details of inputs, outputs and scripts that are very straightforward shell scripts helps one understand what is happening here, and better yet, how the researcher can do this with their own work. There are a lot of details here, but one doesn't need to know much to get started with GitHub actions","title":"End-to-End Workflow testing"},{"location":"classes/day-5-workflow-execution/GitHubActionsForMaintenanceTesting/#automatic-deployment-of-actions","text":"In the case of the three Dockerfiles reviewed in the previous lesson, each were released once successfully built and then we used GitHub actions to automatically build and publish the images, making them accessible with a degree of confidence that they have been built and therefore there are no syntactical reasons why it should not run. This means that we are in a way, using GitHub actions as a complier for our images.","title":"Automatic deployment of actions"},{"location":"classes/day-5-workflow-execution/GitHubActionsForMaintenanceTesting/#example-of-creating-github-actions-for-star-docker-container","text":"The STAR docker is the simplest of containers. Why we would do so is that one can imagine scaling to 100s to 1000s of files that need to be aligned with STAR. The smaller the image, the smaller the machine that could be used to execute that image. Food for thought as we continue to scale.","title":"Example of Creating GitHub actions for star-docker container."},{"location":"classes/day-5-workflow-execution/GitHubActionsForMaintenanceTesting/#return-to-the-agenda","text":"","title":"Return to the Agenda"},{"location":"classes/day-5-workflow-execution/WorkingWithAppsOnCAVATICA/","text":"Working with Apps on CAVATICA We now will explore how to use your own and public apps in your project workspace Head Straight to CAVATICA In your browser navigate to \"https://cavatica.sbgenomics.com\" Login here: You are then in an environment where you can see a number of things including public and developer tabs Developer Authentication Token As in all things these days, to push or at times use (if your application is private) applications you have in the repository - you will need to authenticate with your personal authentication token. You should not share your token with others. If you haven't generated your own authentication token, you can see how it is done here with the instructions Authenticating on Cavatica Using Public Apps to your Project You can use a public app without copying the app into your project. After you login to CAVATICA , you can Browse the public apps. At the time of this writing, there are [703] on the CAVATICA platform. You can browse to find the app you are interested in. And you can search , let's search for the Fastqc Analysis application. And you can run this application (note that I have selected the workflow and not just a single application : Once Run is selected, you will be prompted to specify your project: Next, you see that you need to provide files for the application. Let's see how to get files into your project. Copying Files to your Project There are many ways to get files into your project, while you are developing your workflows or doing analysis, it is useful to have test data. As I mentioned, I use Zenodo for testdata, derivative data products, often in the form of matrices, that are typical for input to interactive analysis as we demonstrated with our volcano plot. And also derivative data products such as what was done for the analysis work, The Impact of Biological Sex on Alternative Splicing . In this case, a Nextflow rMATS workflow was run on controlled access data, the GTEx data, and these aggregate matrices were made and released on Zenodo and were input to all the downstream analyses whose notebooks are on GitHub To work with Apps on Cavatica, you need to make your files available to us. It is possible to bring this data in very easily in CAVATICA, and the data can come from many sources. Just a caution, you pay for storage. So working with small test data sets as you learn and develop your workflows, applications and notebooks, is good practice. Also getting in the habit of removing everything at the end of the day and making those steps reproducible. Starting from scratch where it is reasonable will ensure that you have the understanding of how to proceed that you think you have. Much in the same way where i have asked for you to run the class, to share your screen and execute the steps I say will work, is a kind of testing, and a way to ensure what I am saying is true and reproducible. By getting into the habit of daily saving your work on GitHub, and saving those sharable derivative data products on Zenodo, you start to create durable, successful habits that will ensure when you publish that what you say and do are correct and accurate to the best of your ability. Navigate to Files Navigate to Files and select Add Files Select FTP/HTTP In your browser, navigate to the Zenodo site https://doi.org/10.5281/zenodo.7025773 Where our testdata files now reside. Right click and copy link address on test.10k_reads_1.fastq.gz Paste in the window what you copied: https://zenodo.org/record/7025773/files/test.20k_reads_1.fastq.gz?download=1 You will see that unfortunately it includes ?download=1 . Delete this ending so your pasted copy looks like this: https://zenodo.org/record/7025773/files/test.20k_reads_1.fastq.gz Now copy this file in the window and change the second one to have the _2 instead of _1 . Now in the window it should have these two files https://zenodo.org/record/7025773/files/test.20k_reads_1.fastq.gz https://zenodo.org/record/7025773/files/test.20k_reads_2.fastq.gz And your screen should look like this: Press Done and you will see that now your files are there. Running the App Now we have our app in our project, and we have our files -- now we can run the app. Navigate back to our app, select the copy we have made. Select run It says we need files. Now we need to Select our files Now we select run and we see that it is executing. You will see that machines are initializing App Completion Then the analysis was run and we can view the resulting files in the same way that you can view the results of the execution example ran on the Google Shell Cloud -- but we are in a workspace now where we can have a large number of machines running in parallel. There are limits of course, and depending upon the analysis these limits can be discussed -- because the important things is to get the Science done properly and efficiently. Return to the Agenda Additional resources: CAVATICA documentation for the Apps interface: CAVATICA documentation for Docker Basics:","title":"WorkingWithAppsOnCAVATICA"},{"location":"classes/day-5-workflow-execution/WorkingWithAppsOnCAVATICA/#working-with-apps-on-cavatica","text":"We now will explore how to use your own and public apps in your project workspace","title":"Working with Apps on CAVATICA"},{"location":"classes/day-5-workflow-execution/WorkingWithAppsOnCAVATICA/#head-straight-to-cavatica","text":"In your browser navigate to \"https://cavatica.sbgenomics.com\" Login here: You are then in an environment where you can see a number of things including public and developer tabs","title":"Head Straight to CAVATICA"},{"location":"classes/day-5-workflow-execution/WorkingWithAppsOnCAVATICA/#developer-authentication-token","text":"As in all things these days, to push or at times use (if your application is private) applications you have in the repository - you will need to authenticate with your personal authentication token. You should not share your token with others. If you haven't generated your own authentication token, you can see how it is done here with the instructions Authenticating on Cavatica","title":"Developer Authentication Token"},{"location":"classes/day-5-workflow-execution/WorkingWithAppsOnCAVATICA/#using-public-apps-to-your-project","text":"You can use a public app without copying the app into your project. After you login to CAVATICA , you can Browse the public apps. At the time of this writing, there are [703] on the CAVATICA platform. You can browse to find the app you are interested in. And you can search , let's search for the Fastqc Analysis application. And you can run this application (note that I have selected the workflow and not just a single application : Once Run is selected, you will be prompted to specify your project: Next, you see that you need to provide files for the application. Let's see how to get files into your project.","title":"Using Public Apps to your Project"},{"location":"classes/day-5-workflow-execution/WorkingWithAppsOnCAVATICA/#copying-files-to-your-project","text":"There are many ways to get files into your project, while you are developing your workflows or doing analysis, it is useful to have test data. As I mentioned, I use Zenodo for testdata, derivative data products, often in the form of matrices, that are typical for input to interactive analysis as we demonstrated with our volcano plot. And also derivative data products such as what was done for the analysis work, The Impact of Biological Sex on Alternative Splicing . In this case, a Nextflow rMATS workflow was run on controlled access data, the GTEx data, and these aggregate matrices were made and released on Zenodo and were input to all the downstream analyses whose notebooks are on GitHub To work with Apps on Cavatica, you need to make your files available to us. It is possible to bring this data in very easily in CAVATICA, and the data can come from many sources. Just a caution, you pay for storage. So working with small test data sets as you learn and develop your workflows, applications and notebooks, is good practice. Also getting in the habit of removing everything at the end of the day and making those steps reproducible. Starting from scratch where it is reasonable will ensure that you have the understanding of how to proceed that you think you have. Much in the same way where i have asked for you to run the class, to share your screen and execute the steps I say will work, is a kind of testing, and a way to ensure what I am saying is true and reproducible. By getting into the habit of daily saving your work on GitHub, and saving those sharable derivative data products on Zenodo, you start to create durable, successful habits that will ensure when you publish that what you say and do are correct and accurate to the best of your ability.","title":"Copying Files to your Project"},{"location":"classes/day-5-workflow-execution/WorkingWithAppsOnCAVATICA/#navigate-to-files","text":"Navigate to Files and select Add Files Select FTP/HTTP In your browser, navigate to the Zenodo site https://doi.org/10.5281/zenodo.7025773 Where our testdata files now reside. Right click and copy link address on test.10k_reads_1.fastq.gz Paste in the window what you copied: https://zenodo.org/record/7025773/files/test.20k_reads_1.fastq.gz?download=1 You will see that unfortunately it includes ?download=1 . Delete this ending so your pasted copy looks like this: https://zenodo.org/record/7025773/files/test.20k_reads_1.fastq.gz Now copy this file in the window and change the second one to have the _2 instead of _1 . Now in the window it should have these two files https://zenodo.org/record/7025773/files/test.20k_reads_1.fastq.gz https://zenodo.org/record/7025773/files/test.20k_reads_2.fastq.gz And your screen should look like this: Press Done and you will see that now your files are there.","title":"Navigate to Files"},{"location":"classes/day-5-workflow-execution/WorkingWithAppsOnCAVATICA/#running-the-app","text":"Now we have our app in our project, and we have our files -- now we can run the app. Navigate back to our app, select the copy we have made. Select run It says we need files. Now we need to Select our files Now we select run and we see that it is executing. You will see that machines are initializing App Completion Then the analysis was run and we can view the resulting files in the same way that you can view the results of the execution example ran on the Google Shell Cloud -- but we are in a workspace now where we can have a large number of machines running in parallel. There are limits of course, and depending upon the analysis these limits can be discussed -- because the important things is to get the Science done properly and efficiently.","title":"Running the App"},{"location":"classes/day-5-workflow-execution/WorkingWithAppsOnCAVATICA/#return-to-the-agenda","text":"","title":"Return to the Agenda"},{"location":"classes/day-5-workflow-execution/WorkingWithAppsOnCAVATICA/#additional-resources","text":"CAVATICA documentation for the Apps interface: CAVATICA documentation for Docker Basics:","title":"Additional resources:"},{"location":"classes/day-5-workflow-execution/authenticating-on-cavatica/","text":"Authenticating with CAVATICA You can authenticate in two ways, you can use a credential file that the CAVATICA application recognizes or you can do command line login. Both methods use the CAVATICA 's developer's authentication token. If you navigate to CAVATICA to your project , for me it was the elements-of-style project. Then navigate to the CAVATICA Docker registry under the Developer tab. Selecting the one of the containers of interest, fastqc , you can see more detail with the container: Further, you can see, that CAVATICA has conveniently placed the necessary commands to facilitate login , push and pull . Authenticate with command line login Let's copy the login command and paste it in our google shell window . docker login pgc-images.sbgenomics.com -u <USERNAME> -p <YOUR-AUTH-TOKEN> Replace the <USERNAME> with your username and the <YOUR-AUTH-TOKEN> with your authentication token . Authenticate with a CAVATICA credential file. Alternatively you create your credentials in a credential file. To do this at the root directory, you can type the command push ~ to navigate to the root directory. A fun pair of commands for navigation in a shell are pushd and popd . Wikipedia actually has a nice page to explain these two commands for faster navigation . Think of pushing your commands on a stack and popping them off to return where you are. Let's pushd to our root directory pushd ~/ What we have done is stored in memory where we started from and our next destination. We can see this by asking what we have on this kind of stack of directories with another command: dirs What this shows is: (eos) dirs ~ ~/Building-A-CWL-Script This returns our Home directory where we just navigated to and where we came from ~/adeslatt/Building-A-CWL-script . Home or the root directory (Home is the name used for the top of the directory or root because the directory structure is like a tree. And Home looks like ~ . Now make a directory for CAVATICA to find your credentials mkdir .sevenbridges You are creating a subdirectory that is a hidden directory by typing mkdir .sevenbridges , which you can now navigate to. cd .sevenbridges you can use touch to create your credentials file. [cavatica] api_endpoint = https://cavatica-api.sbgenomics.com/v2 auth_token = <paste your AUTHENTICATION TOKEN here> Now to get back to where we started we type popd Return to Agenda","title":"Authenticating on cavatica"},{"location":"classes/day-5-workflow-execution/authenticating-on-cavatica/#authenticating-with-cavatica","text":"You can authenticate in two ways, you can use a credential file that the CAVATICA application recognizes or you can do command line login. Both methods use the CAVATICA 's developer's authentication token. If you navigate to CAVATICA to your project , for me it was the elements-of-style project. Then navigate to the CAVATICA Docker registry under the Developer tab. Selecting the one of the containers of interest, fastqc , you can see more detail with the container: Further, you can see, that CAVATICA has conveniently placed the necessary commands to facilitate login , push and pull .","title":"Authenticating with CAVATICA"},{"location":"classes/day-5-workflow-execution/authenticating-on-cavatica/#authenticate-with-command-line-login","text":"Let's copy the login command and paste it in our google shell window . docker login pgc-images.sbgenomics.com -u <USERNAME> -p <YOUR-AUTH-TOKEN> Replace the <USERNAME> with your username and the <YOUR-AUTH-TOKEN> with your authentication token .","title":"Authenticate with command line login"},{"location":"classes/day-5-workflow-execution/authenticating-on-cavatica/#authenticate-with-a-cavatica-credential-file","text":"Alternatively you create your credentials in a credential file. To do this at the root directory, you can type the command push ~ to navigate to the root directory. A fun pair of commands for navigation in a shell are pushd and popd . Wikipedia actually has a nice page to explain these two commands for faster navigation . Think of pushing your commands on a stack and popping them off to return where you are. Let's pushd to our root directory pushd ~/ What we have done is stored in memory where we started from and our next destination. We can see this by asking what we have on this kind of stack of directories with another command: dirs What this shows is: (eos) dirs ~ ~/Building-A-CWL-Script This returns our Home directory where we just navigated to and where we came from ~/adeslatt/Building-A-CWL-script . Home or the root directory (Home is the name used for the top of the directory or root because the directory structure is like a tree. And Home looks like ~ . Now make a directory for CAVATICA to find your credentials mkdir .sevenbridges You are creating a subdirectory that is a hidden directory by typing mkdir .sevenbridges , which you can now navigate to. cd .sevenbridges you can use touch to create your credentials file. [cavatica] api_endpoint = https://cavatica-api.sbgenomics.com/v2 auth_token = <paste your AUTHENTICATION TOKEN here> Now to get back to where we started we type popd Return to Agenda","title":"Authenticate with a CAVATICA credential file."},{"location":"classes/day-5-workflow-execution/day-5-workflow-execution/","text":"Agenda for the Day 5: Workflow Execution Time (UTC) Programme 11.00 - 11.20 Recap of the week so far 11.20 - 12.00 Working with Apps on the CAVATICA 12.00 - 12.10 Short break 12:30 - 12.45 GitHub Actions to build, test and deposit container images 12.45 - 13:00 Course Survey and Wrap up Working Nextflow Examples on Cloud Resources A small example - picard FilterSamReads A Published Large Example: Zenodo Kids First and INCLUDE Cloud Credit Programs and Office Hours NIH Kids First Cloud Credit Program Overview NIH INCLUDE Cloud Credit Program Overview How to apply for Kids First Cloud Credits How to apply for INCLUDE Cloud Credits Joint Kids First and INCLUDE with Velsera (formerly Seven Bridges) Monthly User Support Office Hours - every third Thursday of the month Opportunity to learn in Colorado Data Science for Diverse Scholars in Down Syndrome Research (DS3) Kids First and INCLUDE Where to Learn More Kids First Data Resource Center INCLUDE Data Coordinating Center","title":"Day 5 - Workflow execution"},{"location":"classes/day-5-workflow-execution/day-5-workflow-execution/#agenda-for-the-day-5-workflow-execution","text":"Time (UTC) Programme 11.00 - 11.20 Recap of the week so far 11.20 - 12.00 Working with Apps on the CAVATICA 12.00 - 12.10 Short break 12:30 - 12.45 GitHub Actions to build, test and deposit container images 12.45 - 13:00 Course Survey and Wrap up","title":"Agenda for the Day 5: Workflow Execution"},{"location":"classes/day-5-workflow-execution/day-5-workflow-execution/#working-nextflow-examples-on-cloud-resources","text":"A small example - picard FilterSamReads A Published Large Example: Zenodo","title":"Working Nextflow Examples on Cloud Resources"},{"location":"classes/day-5-workflow-execution/day-5-workflow-execution/#kids-first-and-include-cloud-credit-programs-and-office-hours","text":"NIH Kids First Cloud Credit Program Overview NIH INCLUDE Cloud Credit Program Overview How to apply for Kids First Cloud Credits How to apply for INCLUDE Cloud Credits Joint Kids First and INCLUDE with Velsera (formerly Seven Bridges) Monthly User Support Office Hours - every third Thursday of the month","title":"Kids First and INCLUDE Cloud Credit Programs and Office Hours"},{"location":"classes/day-5-workflow-execution/day-5-workflow-execution/#opportunity-to-learn-in-colorado","text":"Data Science for Diverse Scholars in Down Syndrome Research (DS3)","title":"Opportunity to learn in Colorado"},{"location":"classes/day-5-workflow-execution/day-5-workflow-execution/#kids-first-and-include-where-to-learn-more","text":"Kids First Data Resource Center INCLUDE Data Coordinating Center","title":"Kids First and INCLUDE Where to Learn More"},{"location":"classes/pre-training/pre-training-creating-kids-first-and-other-accounts/","text":"Kids First, INCLUDE and Cavatica Account Registrations Today in this pre-training workshop, we will spend some time to get everyone registered for a few accounts. By now, we have a GitHub account, an ORCID and hopefully an ERA-Commons ID. This last one will allow us to take advantage of the NIH Researcher Authentication Service. In this portion of the class, we will be registering with Kids First as well as Cavatica and connecting these two accounts together. Beginning with Kids First Account Some of the data within the Kids First Data Resource Center Hub is registered tier access. What that means is that you simply need to register and the data are available to you. Creating a Kids First DRC Account Please create your account within the Kids First Data Resource Portal. Navigate to Kids First DRC First screen you will see is: From here you can navigate to either CREATE YOUR ACCOUNT or PORTAL LOGIN Let's click CREATE YOUR ACCOUNT Select a Connection Service A Connection service is used to manage your identity. Let's go ahead and Select a connection service . We have three choices: We have today created our ORCID and perhaps our ERACommons ID and we have the option of selecting Google. Each have their advantages and disadvantages. I'd like to steer you towards the NIH Researcher Authentication Service. The reason is that this is the service that will know what data sets you are authorized to access. Protecting Human Data - Introducing GA4GH Pediatric data, human data are sensitve data. To protect the use of the data, we have data access committees. Data Access Committees is a world-wide phenomenon. In this genomic era, where medicine and research involves many types of omics data, protecting these data means that one registers their intent and that intent is then documented. While today this is an often manual process, we do imagine a future where there is more automation involved, so the process becomes a bit more smoothed out. In the meantime, the NIH Researcher Authentication Service, is one step closer to the ideal. First there is the management of your identity, next is the management of what datasets you have access. The Global Alliance for Genomics & Health (GA4GH) is a policy-framing and technical standards-setting organization, seeking to enable responsible genomic data sharing within a human rights framework. I encourage you to stay abreast with this organization and the emerging policies they are promoting. Logging in with the NIH Researcher Authentication Service Let's choose the NIH Researcher Authentication Service Join Kids First Data Resource Portal You are now asked to enter basic Information after you have connected with Kids First. Enter the information that describes you best. The example on the website is a fictious Researcher. You are asked to agree to adhere to the NIH Genomic Data User Code of Conduct . Some of those terms are recapitulated on the page. Kids First Portal Dashboard Once these terms are accepted - you will be presented with a Dashboard And we are now ready to connect to Cavatica. Kids First Connect to Cavatica From the Kids First new user dashboard, we see how to connect to Cavatica, we see there are three steps. Creating an INCLUDE Data Hub Account Some of the data within the INCLUDE Data Hub is registered tier access. What that means is that you simply need to register and the data are available to you. Navigate to INCLUDE Data Hub First screen you will see is: From here you can navigate to either Login or Sign up Let's click Sign Up Sign up choices At this time, you can sign up with either Google or ORCID. Either works: Let's choose ORCID, when it becomes available, you will also be able to sign in with your ERACommons ID. We see now the Dashboard and we are now ready to connect to Cavatica. INCLUDE Connect with Cavatica From the INCLUDE dashboard, we see how to connect to Cavatica Using the Connect button on the left, we login to with the Researcher Authorization Service. The data are at this time mediated by a data service, Gen3 Now authenticated, the INCLUDE project to see what studies you are authorized to see. The portal then will allow you to browse these files and push them onto Cavatica. There are other courses that teach you how to do these steps. We are focused on building the literacy here of how to use and build your own tools with Cavatica in as much as possible a platform agnostic manner. Next, we connect using the Connect button on the right to Cavatica. And we are now ready to go to Cavatica See further documentation: Cavatica Account Login Creation Documentation Kids First Getting Started INCLUDE Help Recap of what we have done today Registered with Kids First Data Resource Center Registered with INCLUDE Registered with Cavatica Connected the Kids First DRC with the Cavatica Platform and Researcher Authorization Service Connected the INCLUDE Data Hub with Cavatica Platform and Researcher Authorization Service","title":"Pre training creating kids first and other accounts"},{"location":"classes/pre-training/pre-training-creating-kids-first-and-other-accounts/#kids-first-include-and-cavatica-account-registrations","text":"Today in this pre-training workshop, we will spend some time to get everyone registered for a few accounts. By now, we have a GitHub account, an ORCID and hopefully an ERA-Commons ID. This last one will allow us to take advantage of the NIH Researcher Authentication Service. In this portion of the class, we will be registering with Kids First as well as Cavatica and connecting these two accounts together.","title":"Kids First, INCLUDE  and Cavatica Account Registrations"},{"location":"classes/pre-training/pre-training-creating-kids-first-and-other-accounts/#beginning-with-kids-first-account","text":"Some of the data within the Kids First Data Resource Center Hub is registered tier access. What that means is that you simply need to register and the data are available to you.","title":"Beginning with Kids First Account"},{"location":"classes/pre-training/pre-training-creating-kids-first-and-other-accounts/#creating-a-kids-first-drc-account","text":"Please create your account within the Kids First Data Resource Portal. Navigate to Kids First DRC First screen you will see is: From here you can navigate to either CREATE YOUR ACCOUNT or PORTAL LOGIN Let's click CREATE YOUR ACCOUNT","title":"Creating a Kids First DRC Account"},{"location":"classes/pre-training/pre-training-creating-kids-first-and-other-accounts/#select-a-connection-service","text":"A Connection service is used to manage your identity. Let's go ahead and Select a connection service . We have three choices: We have today created our ORCID and perhaps our ERACommons ID and we have the option of selecting Google. Each have their advantages and disadvantages. I'd like to steer you towards the NIH Researcher Authentication Service. The reason is that this is the service that will know what data sets you are authorized to access.","title":"Select a Connection Service"},{"location":"classes/pre-training/pre-training-creating-kids-first-and-other-accounts/#protecting-human-data-introducing-ga4gh","text":"Pediatric data, human data are sensitve data. To protect the use of the data, we have data access committees. Data Access Committees is a world-wide phenomenon. In this genomic era, where medicine and research involves many types of omics data, protecting these data means that one registers their intent and that intent is then documented. While today this is an often manual process, we do imagine a future where there is more automation involved, so the process becomes a bit more smoothed out. In the meantime, the NIH Researcher Authentication Service, is one step closer to the ideal. First there is the management of your identity, next is the management of what datasets you have access. The Global Alliance for Genomics & Health (GA4GH) is a policy-framing and technical standards-setting organization, seeking to enable responsible genomic data sharing within a human rights framework. I encourage you to stay abreast with this organization and the emerging policies they are promoting.","title":"Protecting Human Data - Introducing GA4GH"},{"location":"classes/pre-training/pre-training-creating-kids-first-and-other-accounts/#logging-in-with-the-nih-researcher-authentication-service","text":"Let's choose the NIH Researcher Authentication Service","title":"Logging in with the NIH Researcher Authentication Service"},{"location":"classes/pre-training/pre-training-creating-kids-first-and-other-accounts/#join-kids-first-data-resource-portal","text":"You are now asked to enter basic Information after you have connected with Kids First. Enter the information that describes you best. The example on the website is a fictious Researcher. You are asked to agree to adhere to the NIH Genomic Data User Code of Conduct . Some of those terms are recapitulated on the page.","title":"Join Kids First Data Resource Portal"},{"location":"classes/pre-training/pre-training-creating-kids-first-and-other-accounts/#kids-first-portal-dashboard","text":"Once these terms are accepted - you will be presented with a Dashboard And we are now ready to connect to Cavatica.","title":"Kids First Portal Dashboard"},{"location":"classes/pre-training/pre-training-creating-kids-first-and-other-accounts/#kids-first-connect-to-cavatica","text":"From the Kids First new user dashboard, we see how to connect to Cavatica, we see there are three steps.","title":"Kids First Connect to Cavatica"},{"location":"classes/pre-training/pre-training-creating-kids-first-and-other-accounts/#creating-an-include-data-hub-account","text":"Some of the data within the INCLUDE Data Hub is registered tier access. What that means is that you simply need to register and the data are available to you. Navigate to INCLUDE Data Hub First screen you will see is: From here you can navigate to either Login or Sign up Let's click Sign Up","title":"Creating an INCLUDE Data Hub Account"},{"location":"classes/pre-training/pre-training-creating-kids-first-and-other-accounts/#sign-up-choices","text":"At this time, you can sign up with either Google or ORCID. Either works: Let's choose ORCID, when it becomes available, you will also be able to sign in with your ERACommons ID. We see now the Dashboard and we are now ready to connect to Cavatica.","title":"Sign up choices"},{"location":"classes/pre-training/pre-training-creating-kids-first-and-other-accounts/#include-connect-with-cavatica","text":"From the INCLUDE dashboard, we see how to connect to Cavatica Using the Connect button on the left, we login to with the Researcher Authorization Service. The data are at this time mediated by a data service, Gen3 Now authenticated, the INCLUDE project to see what studies you are authorized to see. The portal then will allow you to browse these files and push them onto Cavatica. There are other courses that teach you how to do these steps. We are focused on building the literacy here of how to use and build your own tools with Cavatica in as much as possible a platform agnostic manner. Next, we connect using the Connect button on the right to Cavatica. And we are now ready to go to Cavatica See further documentation: Cavatica Account Login Creation Documentation Kids First Getting Started INCLUDE Help","title":"INCLUDE Connect with Cavatica"},{"location":"classes/pre-training/pre-training-creating-kids-first-and-other-accounts/#recap-of-what-we-have-done-today","text":"Registered with Kids First Data Resource Center Registered with INCLUDE Registered with Cavatica Connected the Kids First DRC with the Cavatica Platform and Researcher Authorization Service Connected the INCLUDE Data Hub with Cavatica Platform and Researcher Authorization Service","title":"Recap of what we have done today"},{"location":"classes/pre-training/pre-training/","text":"Agenda for Pre-Training Workshop Time (UTC) Programme 11.00 - 11.10 Welcome Address, Motivation, Why we are here 11.10 - 11.30 Let's Register! 11.30 - 11.50 Kids First, INCLUDE and CAVATICA Registrations 11.50 - 12:00 Wrap up and overview of next days topics Quick Links: GitHub Zenodo ORCID Registration Now! ERA Commons ID INCLUDE Data Hub Kids First Data Resource Portal Cavatica by Velsera (formerly Seven Bridges) Additional Resources: NIH Kids First Cloud Credit Program Overview NIH INCLUDE Cloud Credit Program Overview How to apply for Kids First Cloud Credits How to apply for INCLUDE Cloud Credits Joint Kids First and INCLUDE with Velsera (formerly Seven Bridges) Monthly User Support Office Hours - every third Thursday of the month","title":"Pre-training"},{"location":"classes/pre-training/pre-training/#agenda-for-pre-training-workshop","text":"Time (UTC) Programme 11.00 - 11.10 Welcome Address, Motivation, Why we are here 11.10 - 11.30 Let's Register! 11.30 - 11.50 Kids First, INCLUDE and CAVATICA Registrations 11.50 - 12:00 Wrap up and overview of next days topics","title":"Agenda for Pre-Training Workshop"},{"location":"classes/pre-training/pre-training/#quick-links","text":"GitHub Zenodo ORCID Registration Now! ERA Commons ID INCLUDE Data Hub Kids First Data Resource Portal Cavatica by Velsera (formerly Seven Bridges)","title":"Quick Links:"},{"location":"classes/pre-training/pre-training/#additional-resources","text":"NIH Kids First Cloud Credit Program Overview NIH INCLUDE Cloud Credit Program Overview How to apply for Kids First Cloud Credits How to apply for INCLUDE Cloud Credits Joint Kids First and INCLUDE with Velsera (formerly Seven Bridges) Monthly User Support Office Hours - every third Thursday of the month","title":"Additional Resources:"}]}